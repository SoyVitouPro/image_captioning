{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: សត្វក្របី\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Model Definitions (EncoderCNN and DecoderRNN)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions, states=None):\n",
    "        embeddings = self.embed(captions)\n",
    "        lstm_out, states = self.lstm(embeddings, states)\n",
    "        outputs = self.linear(lstm_out)\n",
    "        return outputs, states\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size=512).to(device)\n",
    "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(word2idx), num_layers=1).to(device)\n",
    "\n",
    "# Load model weights\n",
    "encoder.load_state_dict(torch.load('encoder.pth'))\n",
    "decoder.load_state_dict(torch.load('decoder.pth'))\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Beam Search Decoder\n",
    "def beam_search_decoder(predictions, k):\n",
    "    sequences = [[list(), 1.0]]  # list of (sequence, score)\n",
    "    for row in predictions:\n",
    "        all_candidates = list()\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -np.log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences\n",
    "\n",
    "# Function to generate a caption\n",
    "def generate_caption(image_path, encoder, decoder, idx2word, word2idx, transform, beam_width=3):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = encoder(image)\n",
    "        input_seq = torch.tensor([word2idx['<START>']]).to(device)\n",
    "        sequences = [(input_seq, 0.0, None)]  # (sequence, log_prob, states)\n",
    "\n",
    "        for _ in range(20):  # Max length of the caption\n",
    "            all_candidates = []\n",
    "            for seq, score, states in sequences:\n",
    "                if seq[-1] == word2idx['<END>']:\n",
    "                    all_candidates.append((seq, score, states))\n",
    "                    continue\n",
    "                outputs, states = decoder(features, seq.unsqueeze(0), states)\n",
    "                softmaxed = torch.softmax(outputs[:, -1], dim=1)\n",
    "                top_indices = torch.topk(softmaxed, beam_width)[1].squeeze(0)\n",
    "                \n",
    "                for idx in top_indices:\n",
    "                    next_seq = torch.cat([seq, idx.unsqueeze(0)])\n",
    "                    next_score = score + torch.log(softmaxed[0, idx]).item()\n",
    "                    all_candidates.append((next_seq, next_score, states))\n",
    "\n",
    "            sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
    "\n",
    "        best_seq, best_score, _ = sequences[0]\n",
    "        caption = ' '.join([idx2word[str(idx.item())] for idx in best_seq if idx.item() not in [word2idx['<START>'], word2idx['<END>']]])\n",
    "        return caption\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Image path\n",
    "image_path = '/home/vitoupro/code/image_captioning/data/raw/animals/wolf/0e238d32f4.jpg'\n",
    "\n",
    "caption = generate_caption(image_path, encoder, decoder, idx2word, word2idx, transform)\n",
    "print(\"Generated Caption:\", caption.replace(\" \", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
