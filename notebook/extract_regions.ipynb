{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2817f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing wolf: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.61it/s]\n",
      "Processing deer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.32it/s]\n",
      "Processing rhinoceros: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.86it/s]\n",
      "Processing raccoon: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.99it/s]\n",
      "Processing eagle: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.10it/s]\n",
      "Processing shark: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.35it/s]\n",
      "Processing leopard: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.35it/s]\n",
      "Processing flamingo: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.93it/s]\n",
      "Processing octopus: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.32it/s]\n",
      "Processing lizard: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.81it/s]\n",
      "Processing owl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.80it/s]\n",
      "Processing horse: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.27it/s]\n",
      "Processing bee: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.25it/s]\n",
      "Processing penguin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.40it/s]\n",
      "Processing pigeon: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.72it/s]\n",
      "Processing starfish: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.06it/s]\n",
      "Processing turtle: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.00it/s]\n",
      "Processing bison: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 17.64it/s]\n",
      "Processing bat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.88it/s]\n",
      "Processing lion: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.83it/s]\n",
      "Processing goose: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.49it/s]\n",
      "Processing hornbill: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.78it/s]\n",
      "Processing dragonfly: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.58it/s]\n",
      "Processing seahorse: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.52it/s]\n",
      "Processing hummingbird: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.45it/s]\n",
      "Processing squid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.38it/s]\n",
      "Processing bear: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.55it/s]\n",
      "Processing pig: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.89it/s]\n",
      "Processing cow: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.71it/s]\n",
      "Processing hedgehog: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.44it/s]\n",
      "Processing grasshopper: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.40it/s]\n",
      "Processing zebra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.75it/s]\n",
      "Processing elephant: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.69it/s]\n",
      "Processing possum: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.40it/s]\n",
      "Processing coyote: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.09it/s]\n",
      "Processing butterfly: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.62it/s]\n",
      "Processing swan: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.96it/s]\n",
      "Processing crab: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.24it/s]\n",
      "Processing hamster: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.66it/s]\n",
      "Processing seal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.93it/s]\n",
      "Processing mosquito: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.62it/s]\n",
      "Processing sparrow: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.11it/s]\n",
      "Processing sheep: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.97it/s]\n",
      "Processing badger: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.98it/s]\n",
      "Processing hippopotamus: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.77it/s]\n",
      "Processing okapi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.18it/s]\n",
      "Processing mouse: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.58it/s]\n",
      "Processing pelecaniformes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.74it/s]\n",
      "Processing wombat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.30it/s]\n",
      "Processing lobster: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.66it/s]\n",
      "Processing fox: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.09it/s]\n",
      "Processing goldfish: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.48it/s]\n",
      "Processing reindeer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.66it/s]\n",
      "Processing dog: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.63it/s]\n",
      "Processing antelope: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.72it/s]\n",
      "Processing fly: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.03it/s]\n",
      "Processing beetle: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.74it/s]\n",
      "Processing goat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.23it/s]\n",
      "Processing kangaroo: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.01it/s]\n",
      "Processing tiger: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.27it/s]\n",
      "Processing snake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.68it/s]\n",
      "Processing donkey: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.62it/s]\n",
      "Processing cockroach: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.85it/s]\n",
      "Processing parrot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.75it/s]\n",
      "Processing orangutan: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.64it/s]\n",
      "Processing ox: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.49it/s]\n",
      "Processing duck: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.17it/s]\n",
      "Processing hare: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.31it/s]\n",
      "Processing porcupine: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.36it/s]\n",
      "Processing moth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.97it/s]\n",
      "Processing rat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.33it/s]\n",
      "Processing caterpillar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.70it/s]\n",
      "Processing crow: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.66it/s]\n",
      "Processing squirrel: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.16it/s]\n",
      "Processing jellyfish: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.51it/s]\n",
      "Processing turkey: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.42it/s]\n",
      "Processing dolphin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.52it/s]\n",
      "Processing hyena: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.25it/s]\n",
      "Processing koala: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.63it/s]\n",
      "Processing boar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.23it/s]\n",
      "Processing gorilla: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.50it/s]\n",
      "Processing oyster: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.54it/s]\n",
      "Processing cat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.61it/s]\n",
      "Processing ladybugs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.90it/s]\n",
      "Processing chimpanzee: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.67it/s]\n",
      "Processing sandpiper: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.50it/s]\n",
      "Processing woodpecker: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.23it/s]\n",
      "Processing whale: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 18.97it/s]\n",
      "Processing otter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.28it/s]\n",
      "Processing panda: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:03<00:00, 19.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use pretrained Faster R-CNN\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval().cuda()\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Paths\n",
    "image_dir = \"/home/vitoupro/code/image_captioning/data/raw/animals\"\n",
    "output_dir = \"/home/vitoupro/code/image_captioning/region_features\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all images\n",
    "for cls_folder in os.listdir(image_dir):\n",
    "    full_cls_path = os.path.join(image_dir, cls_folder)\n",
    "    for img_name in tqdm(os.listdir(full_cls_path), desc=f\"Processing {cls_folder}\"):\n",
    "        img_path = os.path.join(full_cls_path, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = transform(image).unsqueeze(0).cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get region proposals (boxes)\n",
    "            detections = model(img_tensor)[0]\n",
    "            boxes = detections[\"boxes\"]\n",
    "            scores = detections[\"scores\"]\n",
    "\n",
    "            keep = scores > 0.5\n",
    "            if keep.sum() == 0:\n",
    "                boxes = boxes[:36]\n",
    "            else:\n",
    "                boxes = boxes[keep][:36]\n",
    "\n",
    "\n",
    "            if boxes.size(0) == 0:\n",
    "                print(f\"[!] Skipping {img_name} due to no confident regions\")\n",
    "                continue\n",
    "\n",
    "            # Get feature map from backbone\n",
    "            features = model.backbone(img_tensor.tensors if hasattr(img_tensor, 'tensors') else img_tensor)[\"0\"]  # [B, 256, H, W]\n",
    "\n",
    "            # Prepare RoIs in format (image_idx, x1, y1, x2, y2)\n",
    "            image_indices = torch.zeros((boxes.shape[0], 1), device=boxes.device)\n",
    "            rois = torch.cat([image_indices, boxes], dim=1)\n",
    "\n",
    "            # RoI Align to extract region features\n",
    "            region_feats = roi_align(features, rois, output_size=(7, 7), spatial_scale=1/4, aligned=True)  # [N, 256, 7, 7]\n",
    "            region_feats = torch.nn.functional.adaptive_avg_pool2d(region_feats, (1, 1))  # [N, 256, 1, 1]\n",
    "            region_feats = region_feats.view(region_feats.size(0), -1)  # [N, 256]\n",
    "            region_feats = region_feats.cpu().numpy()\n",
    "\n",
    "        # Save .npy file\n",
    "        save_name = f\"{cls_folder}_{img_name.replace('.jpg', '')}.npy\"\n",
    "        save_path = os.path.join(output_dir, save_name)\n",
    "        np.save(save_path, region_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4202604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "\n",
    "# ==== Load Vocabulary ====\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "idx2word, word2idx = load_vocabulary('/home/vitoupro/code/image_captioning/data/processed/idx2word.json')\n",
    "\n",
    "# ==== Khmer Encoding ====\n",
    "def encode_khmer_word(word, word2idx):\n",
    "    return [word2idx.get(ch, word2idx['<UNK>']) for ch in word], None\n",
    "\n",
    "def decode_indices(indices, idx2word):\n",
    "    return ''.join([idx2word.get(str(idx), '') for idx in indices]), None\n",
    "\n",
    "# ==== Attention ====\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_out, hidden), dim=2)))\n",
    "        alpha = torch.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "# ==== Decoder ====\n",
    "class BottomUpDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, region_feat_size=256, attention_dim=36):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(region_feat_size, hidden_size, attention_dim)  # region_feat_size = 2048\n",
    "        self.lstm = nn.LSTM(embed_size + region_feat_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(region_feat_size, hidden_size)  # üëà region_feat_size must match actual input\n",
    "        self.init_c = nn.Linear(region_feat_size, hidden_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, region_feats, captions, sampling_probability=1.0):\n",
    "    # region_feats: [B, num_regions, 2048]\n",
    "        embedded = self.embed(captions)  # [B, seq_len, embed_size]\n",
    "          # üî• this should be [B, 2048]\n",
    "        \n",
    "\n",
    "\n",
    "    # Init LSTM hidden state using average of region features\n",
    "        mean_feats = region_feats.mean(dim=1) \n",
    "       \n",
    "        h = self.init_h(mean_feats).unsqueeze(0)  # [1, B, hidden]\n",
    "        c = self.init_c(mean_feats).unsqueeze(0)\n",
    "\n",
    "        outputs = []\n",
    "        inputs = embedded[:, 0, :].unsqueeze(1)  # [B, 1, embed]\n",
    "\n",
    "        for t in range(1, captions.size(1)):\n",
    "            context, _ = self.attn(region_feats, h[-1])  # [B, 2048]\n",
    "            lstm_input = torch.cat((inputs.squeeze(1), context), dim=1).unsqueeze(1)  # [B, 1, embed+2048]\n",
    "            output, (h, c) = self.lstm(lstm_input, (h, c))  # output: [B, 1, hidden]\n",
    "            output = self.linear(output.squeeze(1))  # [B, vocab_size]\n",
    "            outputs.append(output)\n",
    "\n",
    "            teacher_force = torch.rand(1).item() > sampling_probability\n",
    "            top1 = output.argmax(1)\n",
    "            inputs = embedded[:, t, :].unsqueeze(1) if teacher_force else self.embed(top1).unsqueeze(1)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "\n",
    "# ==== Dataset ====\n",
    "class BottomUpCaptionDataset(Dataset):\n",
    "    def __init__(self, img_labels, feature_dir, vocab, max_length=20):\n",
    "        self.img_labels = img_labels\n",
    "        self.feature_dir = feature_dir\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.img_labels.iloc[idx]\n",
    "        image_path = row['image']\n",
    "        class_name = os.path.dirname(image_path)\n",
    "        file_name = os.path.basename(image_path).replace('.jpg', '')\n",
    "        feature_filename = f\"{class_name}_{file_name}.npy\"\n",
    "        feature_path = os.path.join(self.feature_dir, feature_filename)\n",
    "\n",
    "        if not os.path.exists(feature_path):\n",
    "            raise FileNotFoundError(f\"Missing feature: {feature_path}\")\n",
    "        \n",
    "        max_regions = 36\n",
    "        region_feats = np.load(feature_path)  # shape: [N, 256] (N can vary)\n",
    "\n",
    "        if len(region_feats.shape) == 1:\n",
    "            region_feats = region_feats.reshape(1, -1)  # Fix shape if it's (256,)\n",
    "\n",
    "        num_regions = region_feats.shape[0]\n",
    "\n",
    "        if num_regions < max_regions:\n",
    "            pad = np.zeros((max_regions - num_regions, region_feats.shape[1]), dtype=np.float32)\n",
    "            region_feats = np.vstack((region_feats, pad))\n",
    "        else:\n",
    "            region_feats = region_feats[:max_regions]\n",
    "\n",
    "        region_feats = torch.tensor(region_feats).float()\n",
    "\n",
    "\n",
    "        \n",
    "# Should be [36, 2048] or similar\n",
    "\n",
    "        caption = row['caption']\n",
    "        indices, _ = encode_khmer_word(caption, self.vocab)\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return region_feats, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "\n",
    "\n",
    "# ==== Load and Split ====\n",
    "all_df = pd.read_csv('/home/vitoupro/code/image_captioning/data/raw/annotation.txt', delimiter=' ', names=['image', 'caption'])\n",
    "train_df, eval_df = train_test_split(all_df, test_size=0.2, random_state=42)\n",
    "feature_dir = '/home/vitoupro/code/image_captioning/region_features'\n",
    "\n",
    "train_loader = DataLoader(BottomUpCaptionDataset(train_df, feature_dir, word2idx), batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(BottomUpCaptionDataset(eval_df, feature_dir, word2idx), batch_size=32)\n",
    "\n",
    "# ==== Model ====\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BottomUpDecoder(embed_size=256, hidden_size=512, vocab_size=len(word2idx)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ==== Training ====\n",
    "def calculate_wer(gt, pred): return jiwer.wer(gt, pred)\n",
    "def calculate_cer(gt, pred): return jiwer.cer(gt, pred)\n",
    "\n",
    "def decode_for_metrics(tensor): return decode_indices(tensor.tolist(), idx2word)[0]\n",
    "\n",
    "def evaluate(model, loader, epoch):\n",
    "    model.eval()\n",
    "    total_wer, total_cer, count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for feats, captions in loader:\n",
    "            feats, captions = feats.to(device), captions.to(device)\n",
    "            outputs = model(feats, captions[:, :-1])\n",
    "            preds = outputs.argmax(-1)\n",
    "            for i in range(len(captions)):\n",
    "                gt = decode_for_metrics(captions[i])\n",
    "                pred = decode_for_metrics(preds[i])\n",
    "                total_wer += calculate_wer(gt, pred)\n",
    "                total_cer += calculate_cer(gt, pred)\n",
    "                count += 1\n",
    "    print(f\"[EVAL] Epoch {epoch+1}: WER: {total_wer/count:.2f} CER: {total_cer/count:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e1c5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch 1: Loss = 2.9164\n",
      "[EVAL] Epoch 1: WER: 1.00 CER: 0.52\n",
      "[TRAIN] Epoch 2: Loss = 2.3474\n",
      "[EVAL] Epoch 2: WER: 1.00 CER: 0.51\n",
      "[TRAIN] Epoch 3: Loss = 1.9682\n",
      "[EVAL] Epoch 3: WER: 1.00 CER: 0.53\n",
      "[TRAIN] Epoch 4: Loss = 1.6077\n",
      "[EVAL] Epoch 4: WER: 1.00 CER: 0.54\n",
      "[TRAIN] Epoch 5: Loss = 1.3030\n",
      "[EVAL] Epoch 5: WER: 1.00 CER: 0.56\n",
      "[TRAIN] Epoch 6: Loss = 1.1124\n",
      "[EVAL] Epoch 6: WER: 1.00 CER: 0.57\n",
      "[TRAIN] Epoch 7: Loss = 0.8887\n",
      "[EVAL] Epoch 7: WER: 1.00 CER: 0.53\n",
      "[TRAIN] Epoch 8: Loss = 0.7512\n",
      "[EVAL] Epoch 8: WER: 1.00 CER: 0.55\n",
      "[TRAIN] Epoch 9: Loss = 0.7197\n",
      "[EVAL] Epoch 9: WER: 1.00 CER: 0.58\n",
      "[TRAIN] Epoch 10: Loss = 0.5668\n",
      "[EVAL] Epoch 10: WER: 1.00 CER: 0.55\n",
      "[TRAIN] Epoch 11: Loss = 0.5046\n",
      "[EVAL] Epoch 11: WER: 1.00 CER: 0.55\n",
      "[TRAIN] Epoch 12: Loss = 0.4806\n",
      "[EVAL] Epoch 12: WER: 1.00 CER: 0.55\n",
      "[TRAIN] Epoch 13: Loss = 0.3907\n",
      "[EVAL] Epoch 13: WER: 1.00 CER: 0.55\n",
      "[TRAIN] Epoch 14: Loss = 0.3617\n",
      "[EVAL] Epoch 14: WER: 1.00 CER: 0.56\n",
      "[TRAIN] Epoch 15: Loss = 0.2909\n",
      "[EVAL] Epoch 15: WER: 1.00 CER: 0.54\n"
     ]
    }
   ],
   "source": [
    "# ==== Loop ====\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for feats, captions in train_loader:\n",
    "        feats, captions = feats.to(device), captions.to(device)\n",
    "        outputs = model(feats, captions, sampling_probability=max(0.1, 1.0 - epoch * 0.05))\n",
    "        loss = criterion(outputs.view(-1, len(word2idx)), captions[:, 1:].reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[TRAIN] Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")\n",
    "    evaluate(model, eval_loader, epoch)\n",
    "\n",
    "# ==== Save Weights ====\n",
    "torch.save(model.state_dict(), 'bottomup_decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9049084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Saved: /home/vitoupro/code/image_captioning/region_features/00000001_020.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_region_features(image_path, output_path, device='cuda', score_threshold=0.5, top_k=36):\n",
    "    # Load pretrained Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval().to(device)\n",
    "\n",
    "    # Transform for input image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get predictions\n",
    "        detections = model(img_tensor)[0]\n",
    "        boxes = detections[\"boxes\"]\n",
    "        scores = detections[\"scores\"]\n",
    "\n",
    "        # Keep high-score regions\n",
    "        keep = scores > score_threshold\n",
    "        if keep.sum() == 0:\n",
    "            boxes = boxes[:top_k]\n",
    "        else:\n",
    "            boxes = boxes[keep][:top_k]\n",
    "\n",
    "        if boxes.size(0) == 0:\n",
    "            print(f\"[!] Skipping {os.path.basename(image_path)} due to no confident regions\")\n",
    "            return False\n",
    "\n",
    "        # Get backbone features\n",
    "        features = model.backbone(img_tensor)[\"0\"]  # Feature map: [B, 256, H, W]\n",
    "\n",
    "        # Prepare RoIs: (image_idx, x1, y1, x2, y2)\n",
    "        image_indices = torch.zeros((boxes.shape[0], 1), device=boxes.device)\n",
    "        rois = torch.cat([image_indices, boxes], dim=1)\n",
    "\n",
    "        # RoI Align: [N, 256, 7, 7]\n",
    "        region_feats = roi_align(features, rois, output_size=(7, 7), spatial_scale=1/4, aligned=True)\n",
    "\n",
    "        # Average pool: [N, 256]\n",
    "        region_feats = torch.nn.functional.adaptive_avg_pool2d(region_feats, (1, 1))\n",
    "        region_feats = region_feats.view(region_feats.size(0), -1)\n",
    "        region_feats = region_feats.cpu().numpy()\n",
    "\n",
    "    # Save features\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    np.save(output_path, region_feats)\n",
    "    print(f\"[‚úì] Saved: {output_path}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"/home/vitoupro/code/image_captioning/data/00000001_020.jpg\"\n",
    "    output_path = \"/home/vitoupro/code/image_captioning/region_features/00000001_020.jpg\"\n",
    "\n",
    "    extract_region_features(image_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f97934c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ·û†·û∂·ûò·ûü·üí·ûë·üê·ûö\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# --- Define Attention ---\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "\n",
    "# --- Bottom-Up Decoder ---\n",
    "class BottomUpDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, region_feat_size=256, attention_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attn = Attention(region_feat_size, hidden_size, attention_dim)\n",
    "        self.lstm = nn.LSTM(embed_size + region_feat_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(region_feat_size, hidden_size)\n",
    "        self.init_c = nn.Linear(region_feat_size, hidden_size)\n",
    "\n",
    "    def forward(self, region_feats, captions, sampling_probability=1.0):\n",
    "        raise NotImplementedError(\"This is for inference only\")\n",
    "\n",
    "\n",
    "# --- Vocabulary Loader ---\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "\n",
    "# --- Prediction Function ---\n",
    "def predict_caption_bottomup(region_feat_path, decoder, device, idx2word, word2idx, max_length=20):\n",
    "    decoder.eval()\n",
    "\n",
    "    region_feats = np.load(region_feat_path)\n",
    "    if len(region_feats.shape) == 1:\n",
    "        region_feats = region_feats.reshape(1, -1)\n",
    "    if region_feats.shape[0] < 36:\n",
    "        pad = np.zeros((36 - region_feats.shape[0], region_feats.shape[1]), dtype=np.float32)\n",
    "        region_feats = np.vstack((region_feats, pad))\n",
    "    else:\n",
    "        region_feats = region_feats[:36]\n",
    "\n",
    "    region_feats = torch.tensor(region_feats, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    h = decoder.init_h(region_feats.mean(1)).unsqueeze(0)\n",
    "    c = decoder.init_c(region_feats.mean(1)).unsqueeze(0)\n",
    "\n",
    "    input_idx = torch.tensor([word2idx['<START>']], dtype=torch.long).to(device)\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        embedded = decoder.embed(input_idx).unsqueeze(1)\n",
    "        context, _ = decoder.attn(region_feats, h[-1])\n",
    "        lstm_input = torch.cat((embedded.squeeze(1), context), dim=1).unsqueeze(1)\n",
    "        output, (h, c) = decoder.lstm(lstm_input, (h, c))\n",
    "        output = decoder.linear(output.squeeze(1))\n",
    "        predicted_index = output.argmax(-1).item()\n",
    "\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "\n",
    "        predictions.append(idx2word[str(predicted_index)])\n",
    "        input_idx = torch.tensor([predicted_index], dtype=torch.long).to(device)\n",
    "\n",
    "    return ''.join(predictions)\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Config\n",
    "    region_feat_path = '/home/vitoupro/code/image_captioning/region_features/00000001_020.jpg.npy'\n",
    "    decoder_path = '/home/vitoupro/code/image_captioning/notebook/bottomup_decoder.pth'\n",
    "    vocab_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "\n",
    "    # Load vocab\n",
    "    idx2word, word2idx = load_vocabulary(vocab_path)\n",
    "\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Init model\n",
    "    decoder = BottomUpDecoder(\n",
    "        embed_size=256,\n",
    "        hidden_size=512,\n",
    "        vocab_size=len(word2idx),\n",
    "        region_feat_size=256,\n",
    "        attention_dim=36\n",
    "    ).to(device)\n",
    "\n",
    "    # Load weights\n",
    "    decoder.load_state_dict(torch.load(decoder_path, map_location=device))\n",
    "\n",
    "    # Predict\n",
    "    caption = predict_caption_bottomup(region_feat_path, decoder, device, idx2word, word2idx)\n",
    "    print(\"Predicted Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63ec55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
