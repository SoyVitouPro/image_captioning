{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425b19a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding and decoding functions\n",
    "def encode_khmer_word(word, word2idx):\n",
    "    indices = []\n",
    "    for character in word:\n",
    "        index = word2idx.get(character)\n",
    "        if index is None:\n",
    "            return None, f\"Character '{character}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "def decode_indices(indices, idx2word):\n",
    "    characters = []\n",
    "    for index in indices:\n",
    "        character = idx2word.get(str(index))\n",
    "        if character is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        characters.append(character)\n",
    "    return ''.join(characters), None\n",
    "\n",
    "# Model Definitions (EncoderCNN and DecoderRNN)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # for param in resnet.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if 'layer4' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(hidden_size, hidden_size)  # Initialize LSTM hidden state\n",
    "        self.init_c = nn.Linear(hidden_size, hidden_size)  # Initialize LSTM cell state\n",
    "\n",
    "    def forward(self, features, captions, sampling_probability=1.0):\n",
    "        batch_size, seq_len = captions.size()\n",
    "        embeddings = self.embed(captions)\n",
    "    \n",
    "        h = self.init_h(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c = self.init_c(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "    \n",
    "        inputs = embeddings[:, 0].unsqueeze(1)  # Embed <START>\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            lstm_out, (h, c) = self.lstm(inputs, (h, c))\n",
    "            output = self.linear(lstm_out.squeeze(1))\n",
    "            outputs.append(output)\n",
    "\n",
    "        # Decide whether to use teacher forcing or model prediction\n",
    "            teacher_force = torch.rand(1).item() > sampling_probability\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            next_input = captions[:, t] if teacher_force else top1\n",
    "            inputs = self.embed(next_input).unsqueeze(1)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "\n",
    "# Image Captioning Dataset\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_word(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "# # Define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "# ]) \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/raw/annotation.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/data/raw/animals'\n",
    "all_images = pd.read_csv(annotations_file, delimiter=' ', names=['image', 'caption'])\n",
    "\n",
    "# Split dataset\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': train_images, 'caption': train_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "eval_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': eval_images, 'caption': eval_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size=512).to(device)\n",
    "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(word2idx), num_layers=1).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "# optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "def custom_transform(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Return as list of words\n",
    "    return text.split()\n",
    "    \n",
    "\n",
    "def calculate_wer(gt, pred, epoch, file_path='metric.txt'):\n",
    "    \n",
    "    with open(file_path, 'a') as file:  # Open file in append mode\n",
    "        file.write(f\"Epoch {epoch}\\n\")\n",
    "        \n",
    "        file.write(\"===========================\\n\")\n",
    "        match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "        if match_pred:\n",
    "            content_pred = match_pred.group(1)\n",
    "        file.write(f\"pred: {content_pred}\\n\")\n",
    "        match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "        if match_ground_true:\n",
    "            content_ground_true = match_ground_true.group(1)\n",
    "        file.write(f\"true: {content_ground_true}\\n\")\n",
    "        file.write(\"===========================\\n\")\n",
    "    \n",
    "    if not content_ground_true:  # Ensure non-empty\n",
    "        content_ground_true = ['']\n",
    "    if not content_pred:  # Ensure non-empty\n",
    "        content_pred = ['']\n",
    "    wer_score = jiwer.wer(content_ground_true, content_pred)\n",
    "    \n",
    "    return wer_score\n",
    "\n",
    "def calculate_cer(gt, pred):\n",
    "    match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "    if match_pred:\n",
    "        content_pred = match_pred.group(1)\n",
    "    \n",
    "    match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "    if match_ground_true:\n",
    "        content_ground_true = match_ground_true.group(1)\n",
    "\n",
    "    return jiwer.cer(content_ground_true, content_pred)\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, num_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "            \n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "                \n",
    "                wer = calculate_wer(gt_caption, pred_caption, epoch)\n",
    "                cer = calculate_cer(gt_caption, pred_caption)\n",
    "                total_wer += wer\n",
    "                total_cer += cer\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    print(f\"Average WER: {avg_wer:.2f}, Average CER: {avg_cer:.2f}\")\n",
    "    return avg_wer, avg_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8a632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.8530, Sampling Prob: 1.00\n",
      "Average WER: 1.00, Average CER: 1.00\n",
      "Epoch 2: Train Loss: 2.4235, Sampling Prob: 0.95\n",
      "Average WER: 1.00, Average CER: 0.87\n",
      "Epoch 3: Train Loss: 2.2942, Sampling Prob: 0.90\n",
      "Average WER: 1.00, Average CER: 1.06\n",
      "Epoch 4: Train Loss: 2.0832, Sampling Prob: 0.85\n",
      "Average WER: 1.00, Average CER: 1.00\n",
      "Epoch 5: Train Loss: 1.8994, Sampling Prob: 0.80\n",
      "Average WER: 0.86, Average CER: 0.88\n",
      "Epoch 6: Train Loss: 1.2342, Sampling Prob: 0.75\n",
      "Average WER: 0.56, Average CER: 0.77\n",
      "Epoch 7: Train Loss: 0.8641, Sampling Prob: 0.70\n",
      "Average WER: 0.40, Average CER: 0.53\n",
      "Epoch 8: Train Loss: 0.6688, Sampling Prob: 0.65\n",
      "Average WER: 0.37, Average CER: 0.50\n",
      "Epoch 9: Train Loss: 0.5361, Sampling Prob: 0.60\n",
      "Average WER: 0.30, Average CER: 0.34\n",
      "Epoch 10: Train Loss: 0.5329, Sampling Prob: 0.55\n",
      "Average WER: 0.32, Average CER: 0.40\n",
      "Epoch 11: Train Loss: 0.3871, Sampling Prob: 0.50\n",
      "Average WER: 0.30, Average CER: 0.42\n",
      "Epoch 12: Train Loss: 0.3462, Sampling Prob: 0.45\n",
      "Average WER: 0.29, Average CER: 0.36\n",
      "Epoch 13: Train Loss: 0.3048, Sampling Prob: 0.40\n",
      "Average WER: 0.26, Average CER: 0.39\n",
      "Epoch 14: Train Loss: 0.2506, Sampling Prob: 0.35\n",
      "Average WER: 0.24, Average CER: 0.35\n",
      "Epoch 15: Train Loss: 0.2407, Sampling Prob: 0.30\n",
      "Average WER: 0.29, Average CER: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    sampling_prob = max(0.1, 1.0 - epoch * 0.05)\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "        features = encoder(images)\n",
    "\n",
    "        outputs = decoder(features, captions, sampling_probability=sampling_prob)\n",
    "\n",
    "        loss = criterion(outputs.view(-1, len(word2idx)), captions[:, 1:].reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {total_loss/len(train_loader):.4f}, Sampling Prob: {sampling_prob:.2f}')\n",
    "    _, wer = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "\n",
    "    if wer < best_wer:\n",
    "        best_wer = wer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f48233",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wer < best_wer:\n",
    "    best_wer = wer\n",
    "    torch.save(encoder.state_dict(), \"encoder_best.pth\")\n",
    "    torch.save(decoder.state_dict(), \"decoder_best.pth\")\n",
    "    print(\"✅ Saved best model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8eed1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ខ្លាឃ្មុំ\n"
     ]
    }
   ],
   "source": [
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Encode the image\n",
    "    features = encoder(image)\n",
    "\n",
    "    # Start generation with <START> token\n",
    "    input_idx = torch.tensor([word2idx['<START>']], dtype=torch.long).to(device)\n",
    "    predictions = []\n",
    "    h, c = None, None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        input_idx = input_idx.unsqueeze(0)  # (1, 1)\n",
    "\n",
    "        # Initialize hidden state on first step\n",
    "        if h is None and c is None:\n",
    "            h = decoder.init_h(features).unsqueeze(0).repeat(decoder.num_layers, 1, 1)\n",
    "            c = decoder.init_c(features).unsqueeze(0).repeat(decoder.num_layers, 1, 1)\n",
    "\n",
    "        embedded = decoder.embed(input_idx)\n",
    "        output, (h, c) = decoder.lstm(embedded, (h, c))\n",
    "        output = decoder.linear(output.squeeze(1))\n",
    "\n",
    "        predicted_index = output.argmax(-1).item()\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "\n",
    "        predictions.append(idx2word[str(predicted_index)])\n",
    "        input_idx = torch.tensor([predicted_index], dtype=torch.long).to(device)\n",
    "\n",
    "    predicted_caption = ''.join(predictions)  # Khmer: no need for space\n",
    "    return predicted_caption\n",
    "\n",
    "image_path = '/home/vitoupro/code/image_captioning/data/processed/image.png'\n",
    "caption = predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx)\n",
    "print(\"Predicted Caption:\", caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afacae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
