{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fcf849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement khmerwordsegment (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for khmerwordsegment\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install khmerwordsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d2b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total captions: 425\n",
      "âœ… Vocabulary size: 425\n",
      "âœ… Max sequence length: 3\n",
      "\n",
      "ğŸ“Œ Sample Caption: á€á¼á“á–áŸ‚á„á–ááŸŒááŸ’á˜áŸ…á”á½á“á“á·á„á€áŸ†áŸáŸ€áœá–ááŸŒááŸ’á˜áŸ…á˜á½á™\n",
      "ğŸ§© Tokenized: ['á€á¼á“á–áŸ‚á„á–ááŸŒááŸ’á˜áŸ…á”á½á“á“á·á„á€áŸ†áŸáŸ€áœá–ááŸŒááŸ’á˜áŸ…á˜á½á™']\n",
      "ğŸ”¢ Sequence: [1, 30, 2]\n",
      "ğŸ“ Padded: [ 1 30  2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# === Step 1: Provide your file path here ===\n",
    "file_path = \"/home/vitoupro/code/image_captioning/data/raw/rawimglabel.txt\"  # ğŸ” Replace with your actual path\n",
    "\n",
    "# === Step 2: Load captions from the file ===\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "# Clean up: remove blank lines and trim whitespaces\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Tokenize each caption into characters ===\n",
    "# Word-level tokenization\n",
    "tokenized_words = [caption.split() for caption in captions]\n",
    "\n",
    "# === Step 4: Build vocabulary with special tokens ===\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "# Build vocab\n",
    "all_words = [word for caption in tokenized_words for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "\n",
    "\n",
    "word2idx_words = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word_words = {idx: word for word, idx in word2idx_words.items()}\n",
    "\n",
    "# === Step 5: Convert captions to ID sequences ===\n",
    "# Convert to ID sequences\n",
    "def caption_to_ids_words(word_list):\n",
    "    return [word2idx_words['<START>']] + [word2idx_words.get(word, word2idx_words['<UNK>']) for word in word_list] + [word2idx_words['<END>']]\n",
    "\n",
    "sequences_words = [caption_to_ids_words(words) for words in tokenized_words]\n",
    "\n",
    "# === Step 6: Pad sequences ===\n",
    "max_len = max(len(seq) for seq in sequences_words)\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx_words['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "\n",
    "# Padding\n",
    "max_len_words = max(len(seq) for seq in sequences_words)\n",
    "padded_sequences_words = [pad_sequence(seq, max_len_words) for seq in sequences_words]\n",
    "padded_sequences_words = np.array(padded_sequences_words)\n",
    "\n",
    "# === Step 7: Preview results ===\n",
    "print(f\"âœ… Total captions: {len(captions)}\")\n",
    "print(f\"âœ… Vocabulary size: {len(all_words)}\")\n",
    "print(f\"âœ… Max sequence length: {max_len}\")\n",
    "print(\"\\nğŸ“Œ Sample Caption:\", captions[0])\n",
    "print(\"ğŸ§© Tokenized:\", tokenized_words[0])\n",
    "print(\"ğŸ”¢ Sequence:\", sequences_words[0])\n",
    "print(\"ğŸ“ Padded:\", padded_sequences_words[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa711cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocabulary saved to word2idx_words.json and idx2word_words.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save word2idx_wordleveltest5k and idx2word_wordleveltest5k to a file\n",
    "with open(\"word2idx_words.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(word2idx_words, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"idx2word_words.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(idx2word_words, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… Vocabulary saved to word2idx_words.json and idx2word_words.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a3e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished!\n",
      "âœ… Total captions: 425\n",
      "âœ… Vocabulary size (including special tokens): 481\n",
      "âœ… Max sequence length: 28\n",
      "âœ… Saved word2idx_wordlevel.json and idx2word_wordlevel.json into /home/vitoupro/code/image_captioning/data/processed/\n",
      "\n",
      "ğŸ“Œ Example caption: á€á¼á“á–áŸ‚á„á–ááŸŒááŸ’á˜áŸ…á”á½á“á“á·á„á€áŸ†áŸáŸ€áœá–ááŸŒááŸ’á˜áŸ…á˜á½á™\n",
      "ğŸ§© Tokenized: ['á€á¼á“', 'á–áŸ‚á„', 'á–ááŸŒááŸ’á˜áŸ…', 'á”á½á“', 'á“á·á„', 'á€áŸ†áŸáŸ€áœ', 'á–ááŸŒááŸ’á˜áŸ…', 'á˜á½á™']\n",
      "ğŸ”¢ Sequence: [1, 26, 333, 310, 274, 242, 38, 310, 353, 2]\n",
      "ğŸ“ Padded: [  1  26 333 310 274 242  38 310 353   2   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/rawimglabel.txt'  # ğŸ” Update if needed\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # ğŸ” Your Khmer dictionary\n",
    "\n",
    "# === Step 2: Load Captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Word Segmentation\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "tokenized_captions = [segmenter.segment(caption) for caption in captions]\n",
    "\n",
    "# === Step 4: Build Vocabulary\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Sort vocab by frequency (optional), or just alphabetically\n",
    "vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "\n",
    "word2idx_wordleveltest5k = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word_wordleveltest5k = {idx: word for word, idx in word2idx_wordleveltest5k.items()}\n",
    "\n",
    "# === Step 5: Convert Captions to ID Sequences\n",
    "def caption_to_ids(word_list):\n",
    "    return [word2idx_wordleveltest5k['<START>']] + [word2idx_wordleveltest5k.get(word, word2idx_wordleveltest5k['<UNK>']) for word in word_list] + [word2idx_wordleveltest5k['<END>']]\n",
    "\n",
    "sequences = [caption_to_ids(words) for words in tokenized_captions]\n",
    "\n",
    "# === Step 6: Padding\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx_wordleveltest5k['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "padded_sequences = [pad_sequence(seq, max_len) for seq in sequences]\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "# === Step 7: Save Vocabulary\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… Finished!\")\n",
    "print(f\"âœ… Total captions: {len(captions)}\")\n",
    "print(f\"âœ… Vocabulary size (including special tokens): {len(word2idx_wordleveltest5k)}\")\n",
    "print(f\"âœ… Max sequence length: {max_len}\")\n",
    "print(f\"âœ… Saved word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json into {save_dir}\")\n",
    "\n",
    "# === Step 8: Preview\n",
    "print(\"\\nğŸ“Œ Example caption:\", captions[0])\n",
    "print(\"ğŸ§© Tokenized:\", tokenized_captions[0])\n",
    "print(\"ğŸ”¢ Sequence:\", sequences[0])\n",
    "print(\"ğŸ“ Padded:\", padded_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4331ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocabulary saved to word2idx_wordlevel.json and idx2word_wordlevel.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save word2idx_wordleveltest5k and idx2word_wordleveltest5k to a file\n",
    "with open(\"word2idx_wordleveltest5k.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(word2idx_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"idx2word_level.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(idx2word_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… Vocabulary saved to word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608a6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'  # your current label\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # your Khmer dictionary\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel.txt'  # â¬…ï¸ NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Initialize segmenter\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "# === Step 4: Process and segment\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)  # split only once (image and caption)\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    image_name, caption = parts\n",
    "    segmented_words = segmenter.segment(caption)\n",
    "    segmented_caption = ' '.join(segmented_words)  # join back with space\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 5: Save new segmented label file\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"âœ… Word-segmented captions saved to {output_label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d880854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_gpt.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'  # your current label\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/gptdic.txt'  # your Khmer dictionary\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_gpt.txt'  # â¬…ï¸ NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Initialize segmenter\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "# === Step 4: Process and segment\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)  # split only once (image and caption)\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    image_name, caption = parts\n",
    "    segmented_words = segmenter.segment(caption)\n",
    "    segmented_caption = ' '.join(segmented_words)  # join back with space\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 5: Save new segmented label file\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"âœ… Word-segmented captions saved to {output_label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Rough dictionary generated with 22409 words!\n"
     ]
    }
   ],
   "source": [
    "#dic kloun eng\n",
    "# Build rough dictionary from captions themselves\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'\n",
    "output_dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic_auto_generated.txt'\n",
    "\n",
    "captions = []\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# Naive split: treat every continuous 2, 3, 4, 5, 6 character groups as possible words\n",
    "generated_words = set()\n",
    "\n",
    "for caption in captions:\n",
    "    for size in range(1, 8):  # try 2 to 6 character words\n",
    "        for i in range(len(caption) - size + 1):\n",
    "            substring = caption[i:i+size]\n",
    "            generated_words.add(substring)\n",
    "\n",
    "# Save the rough dictionary\n",
    "with open(output_dict_path, 'w', encoding='utf-8') as f:\n",
    "    for word in sorted(generated_words):\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"âœ… Rough dictionary generated with {len(generated_words)} words!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612d1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total lines: 425\n",
      "\n",
      "[Line 1] á€á¼á“á–áŸ‚á„á–ááŸŒááŸ’á˜áŸ…á”á½á“á“á·á„á€áŸ†áŸáŸ€áœá–ááŸŒááŸ’á˜áŸ…á˜á½á™\n",
      "[Line 2] á†áŸ’á€áŸ‚á¢á„áŸ’á‚á»á™á›á¾áŸá¶á¡á»á„\n",
      "[Line 3] á˜á¶á“á˜á½á€á“áŸ…á›á¾áœáŸ‰á¶á›á¸á–ááŸŒááŸ’á˜áŸ…\n",
      "[Line 4] áá¼á–ááŸŒáŸá˜á½á™á“áŸ…á‡á·áá€áŸ…á¢á¸á“á·á„á˜á¶á“áá¼á–ááŸŒáŸá˜á½á™á‘áŸ€áá“áŸ…á›á¾á€áŸ…á¢á¸\n",
      "[Line 5] á€áŸ†áŸáŸ€áœá“áŸ…á›á¾áá»á…á„áŸ’á€áŸ’ášá¶á“á”á¶á™\n",
      "[Line 6] á€á“áŸ’ááŸ’ášá€á–ááŸŒáŸ\n",
      "[Line 7] á˜á¶á“á€á¶áŸ†á”á·ááŸáŸ’ášá½á…á“á·á„á•áŸ’á›áŸ‚áŸáŸ’áá”áºášá¸á“áŸ…á€áŸ’á“á»á„á€á“áŸ’ááŸ’ášá€\n",
      "[Line 8] á†áŸ’á€áŸ‚á–á¶á€áŸ‹á˜á½á€á¢á„áŸ’á‚á»á™á‡á·ááœáŸ‰á¶á›á¸á–ááŸŒá›á¿á„\n",
      "[Line 9] á€áŸ…á¢á¸á–á¸ášá“áŸ…á‡á·áá‚áŸ’á“á¶á“á·á„á˜á¶á“ááŸ’á“á¾á™á“áŸ…á›á¾á€áŸ…á¢á¸á‘á¶áŸ†á„á–á¸áš\n",
      "[Line 10] á”áŸ’ášá¢á”áŸ‹á”á¶á™á–ááŸŒá•áŸ’á€á¶áˆá¼á€\n"
     ]
    }
   ],
   "source": [
    "# Check captions properly\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/rawimglabel.txt'\n",
    "\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"âœ… Total lines: {len(lines)}\\n\")\n",
    "\n",
    "for i, line in enumerate(lines[:10]):  # show first 10 lines\n",
    "    print(f\"[Line {i+1}] {line.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c1d8c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved cleaned word2idx_level.json and idx2word_level.json!\n",
      "âœ… Vocabulary size: 433\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load segmented captions\n",
    "segmented_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel.txt'\n",
    "\n",
    "captions = []\n",
    "with open(segmented_label_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# Tokenize into words\n",
    "tokenized_captions = [caption.split() for caption in captions]\n",
    "\n",
    "# Build vocabulary\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "\n",
    "# ğŸ›  Correct filtering: keep only words with length > 1\n",
    "filtered_words = [word for word in set(all_words) if len(word) > 1]\n",
    "\n",
    "# ğŸ›  Build vocab from filtered words\n",
    "vocab_words = special_tokens + sorted(filtered_words)\n",
    "\n",
    "# Create word2idx and idx2word\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Save\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_level.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_level.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… Saved cleaned word2idx_level.json and idx2word_level.json!\")\n",
    "print(f\"âœ… Vocabulary size: {len(word2idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "803fd1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished fixing segmentation! Saved new file to: /home/vitoupro/code/image_captioning/data/processed/fixed_word_segmented_imglabel.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ğŸ›  Step 1: Define function to fix Khmer word segmentation\n",
    "def fix_khmer_segmentation(text):\n",
    "    words = text.strip().split()\n",
    "\n",
    "    # Khmer unicode range\n",
    "    khmer_range = re.compile(r'[\\u1780-\\u17FF]+')\n",
    "\n",
    "    fixed_words = []\n",
    "    for word in words:\n",
    "        if not khmer_range.match(word):\n",
    "            # Not a full Khmer word => likely broken vowel/diacritic => merge with previous\n",
    "            if fixed_words:\n",
    "                fixed_words[-1] += word\n",
    "            else:\n",
    "                fixed_words.append(word)\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    "    return ' '.join(fixed_words)\n",
    "\n",
    "# ğŸ›  Step 2: Apply it to fix the whole caption file\n",
    "input_file = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel.txt'\n",
    "output_file = '/home/vitoupro/code/image_captioning/data/processed/fixed_word_segmented_imglabel.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "fixed_lines = []\n",
    "for line in lines:\n",
    "    parts = line.split(' ', 1)\n",
    "    if len(parts) == 2:\n",
    "        img_name, caption = parts\n",
    "        fixed_caption = fix_khmer_segmentation(caption)\n",
    "        fixed_lines.append(f\"{img_name} {fixed_caption}\")\n",
    "\n",
    "# ğŸ›  Step 3: Save the fixed captions\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for line in fixed_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"âœ… Finished fixing segmentation! Saved new file to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7884a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished!\n",
      "âœ… Total captions: 998\n",
      "âœ… Vocabulary size (including special tokens): 2262\n",
      "âœ… Max sequence length: 185\n",
      "âœ… Saved word2idx_wordleveltest.json and idx2word_wordleveltest.json into /home/vitoupro/code/image_captioning/data/processed/\n",
      "\n",
      "ğŸ“Œ Example caption: image_1.jpg á€áŸ’á˜áŸá„á”áŸ’ášá»áŸá–á¸ášá“á¶á€áŸ‹áŠáŸ‚á›á˜á¶á“áŸá€áŸ‹ášá‰áŸ‰áŸášá‰áŸ‰áŸƒá€áŸ†á–á»á„á˜á¾á›á‘áŸ…áŠáŸƒášá”áŸáŸ‹á–á½á€á‚áŸ áááŸˆá–á½á€á‚áŸá€áŸ†á–á»á„á¢á„áŸ’á‚á»á™á›áŸá„á“áŸ…á€áŸ’á“á»á„áœá¶á›áŸ”\n",
      "ğŸ§© Tokenized: ['i', 'm', 'a', 'g', 'e', '_', '1', '.', 'j', 'p', 'g', ' ', 'á€áŸ’á˜áŸá„á”áŸ’ášá»áŸ', 'á–á¸áš', 'á“á¶á€áŸ‹', 'áŠáŸ‚á›á˜á¶á“', 'áŸá€', 'áŸ‹', 'ášá‰áŸ‰áŸášá‰áŸ‰áŸƒ', 'á€áŸ†á–á»á„', 'á˜á¾á›á‘áŸ…', 'áŠáŸƒ', 'ášá”áŸáŸ‹', 'á–á½á€á‚áŸ', ' ', 'áááŸˆ', 'á–á½á€á‚áŸ', 'á€áŸ†á–á»á„', 'á¢á„áŸ’á‚á»á™á›áŸá„', 'á“áŸ…á€áŸ’á“á»á„', 'áœ', 'á¶', 'á›', 'áŸ”']\n",
      "ğŸ”¢ Sequence: [1, 47, 51, 39, 45, 43, 38, 8, 6, 48, 54, 45, 4, 264, 1394, 1038, 737, 1841, 2251, 1637, 226, 1570, 738, 1659, 1411, 4, 303, 1411, 226, 2097, 1071, 1803, 2230, 1739, 2257, 2]\n",
      "ğŸ“ Padded: [   1   47   51   39   45   43   38    8    6   48   54   45    4  264\n",
      " 1394 1038  737 1841 2251 1637  226 1570  738 1659 1411    4  303 1411\n",
      "  226 2097 1071 1803 2230 1739 2257    2    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset.txt'  # ğŸ” Update if needed\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # ğŸ” Your Khmer dictionary\n",
    "\n",
    "# === Step 2: Load Captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Word Segmentation\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "tokenized_captions = [segmenter.segment(caption) for caption in captions]\n",
    "\n",
    "# === Step 4: Build Vocabulary\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Sort vocab by frequency (optional), or just alphabetically\n",
    "vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "\n",
    "word2idx_wordleveltest5k = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word_wordleveltest5k = {idx: word for word, idx in word2idx_wordleveltest5k.items()}\n",
    "\n",
    "# === Step 5: Convert Captions to ID Sequences\n",
    "def caption_to_ids(word_list):\n",
    "    return [word2idx_wordleveltest5k['<START>']] + [word2idx_wordleveltest5k.get(word, word2idx_wordleveltest5k['<UNK>']) for word in word_list] + [word2idx_wordleveltest5k['<END>']]\n",
    "\n",
    "sequences = [caption_to_ids(words) for words in tokenized_captions]\n",
    "\n",
    "# === Step 6: Padding\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx_wordleveltest5k['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "padded_sequences = [pad_sequence(seq, max_len) for seq in sequences]\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "# === Step 7: Save Vocabulary\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… Finished!\")\n",
    "print(f\"âœ… Total captions: {len(captions)}\")\n",
    "print(f\"âœ… Vocabulary size (including special tokens): {len(word2idx_wordleveltest5k)}\")\n",
    "print(f\"âœ… Max sequence length: {max_len}\")\n",
    "print(f\"âœ… Saved word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json into {save_dir}\")\n",
    "\n",
    "# === Step 8: Preview\n",
    "print(\"\\nğŸ“Œ Example caption:\", captions[0])\n",
    "print(\"ğŸ§© Tokenized:\", tokenized_captions[0])\n",
    "print(\"ğŸ”¢ Sequence:\", sequences[0])\n",
    "print(\"ğŸ“ Padded:\", padded_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874b9152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset.txt'  # your current label\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # your Khmer dictionary\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest.txt'  # â¬…ï¸ NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Initialize segmenter\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "# === Step 4: Process and segment\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)  # split only once (image and caption)\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    image_name, caption = parts\n",
    "    segmented_words = segmenter.segment(caption)\n",
    "    segmented_caption = ' '.join(segmented_words)  # join back with space\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 5: Save new segmented label file\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"âœ… Word-segmented captions saved to {output_label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790c9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished!\n",
      "âœ… Total captions: 4989\n",
      "âœ… Vocabulary size (including special tokens): 4227\n",
      "âœ… Max sequence length: 171\n",
      "âœ… Saved word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json into /home/vitoupro/code/image_captioning/data/processed/\n",
      "\n",
      "ğŸ“Œ Example image: image_1.jpg\n",
      "ğŸ“Œ Example caption: á”á»ášáŸáœáŸá™á€áŸ’á˜áŸá„á–á¸ášá“á¶á€áŸ‹áŠáŸ‚á›á˜á¶á“áŸá€áŸ‹ááŸ’á˜á¶áŸáŸ‹á¢áŸ€á“á˜á¾á›á‘áŸ…áŠáŸƒášá”áŸáŸ‹á–á½á€á‚áŸá“áŸ…á–áŸá›áŠá¾ášá›áŸá„á“áŸ…á‘á¸á’áŸ’á›á¶\n",
      "ğŸ§© Tokenized: ['á”á»ášáŸ', 'áœáŸá™á€áŸ’á˜áŸá„', 'á–á¸áš', 'á“á¶á€áŸ‹', 'áŠáŸ‚á›á˜á¶á“', 'áŸá€', 'áŸ‹', 'ááŸ’á˜á¶áŸ', 'áŸ‹', 'á¢áŸ€á“', 'á˜á¾á›á‘áŸ…', 'áŠáŸƒ', 'ášá”áŸáŸ‹', 'á–á½á€á‚áŸ', 'á“áŸ…á–áŸá›', 'áŠá¾ášá›áŸá„', 'á“áŸ…', 'á‘á¸á’áŸ’á›á¶']\n",
      "ğŸ”¢ Sequence: [1, 2132, 3400, 2535, 1891, 1340, 3402, 4219, 628, 4219, 4053, 2847, 1341, 3025, 2562, 1971, 1320, 1947, 1672, 2]\n",
      "ğŸ“ Padded: [   1 2132 3400 2535 1891 1340 3402 4219  628 4219 4053 2847 1341 3025\n",
      " 2562 1971 1320 1947 1672    2    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "#for 5k\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset5k.txt'  # ğŸ” Your label file\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # ğŸ” Your Khmer dictionary\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'  # ğŸ” Where to save output\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Step 2: Load Captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_lines = f.readlines()\n",
    "\n",
    "# === Step 3: Split out image names and captions\n",
    "image_names = []\n",
    "captions = []\n",
    "\n",
    "for line in raw_lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    parts = line.split(' ', 1)  # split into image name and caption\n",
    "    if len(parts) == 2:\n",
    "        image_name, caption = parts\n",
    "        image_names.append(image_name)\n",
    "        captions.append(caption)\n",
    "    else:\n",
    "        print(f\"âš ï¸ Skipping malformed line: {line}\")\n",
    "\n",
    "# === Step 4: Word Segmentation (on captions only)\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "tokenized_captions = [segmenter.segment(caption) for caption in captions]\n",
    "\n",
    "# === Step 5: Build Vocabulary\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Sort vocab by frequency (optional) or alphabetically\n",
    "vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "\n",
    "word2idx_wordleveltest5k = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word_wordleveltest5k = {idx: word for word, idx in word2idx_wordleveltest5k.items()}\n",
    "\n",
    "# === Step 6: Convert Captions to ID Sequences\n",
    "def caption_to_ids(word_list):\n",
    "    return [word2idx_wordleveltest5k['<START>']] + [word2idx_wordleveltest5k.get(word, word2idx_wordleveltest5k['<UNK>']) for word in word_list] + [word2idx_wordleveltest5k['<END>']]\n",
    "\n",
    "sequences = [caption_to_ids(words) for words in tokenized_captions]\n",
    "\n",
    "# === Step 7: Padding\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx_wordleveltest5k['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "padded_sequences = [pad_sequence(seq, max_len) for seq in sequences]\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "# === Step 8: Save Vocabulary\n",
    "with open(os.path.join(save_dir, 'word2idx_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… Finished!\")\n",
    "print(f\"âœ… Total captions: {len(captions)}\")\n",
    "print(f\"âœ… Vocabulary size (including special tokens): {len(word2idx_wordleveltest5k)}\")\n",
    "print(f\"âœ… Max sequence length: {max_len}\")\n",
    "print(f\"âœ… Saved word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json into {save_dir}\")\n",
    "\n",
    "# === Step 9: Preview\n",
    "print(\"\\nğŸ“Œ Example image:\", image_names[0])\n",
    "print(\"ğŸ“Œ Example caption:\", captions[0])\n",
    "print(\"ğŸ§© Tokenized:\", tokenized_captions[0])\n",
    "print(\"ğŸ”¢ Sequence:\", sequences[0])\n",
    "print(\"ğŸ“ Padded:\", padded_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f848f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k.txt\n",
      "âœ… Total lines processed: 4989\n"
     ]
    }
   ],
   "source": [
    "#for 5k\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset5k.txt'  # your current label\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # your Khmer dictionary\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k.txt'  # â¬…ï¸ NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Initialize segmenter\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "# === Step 4: Process and segment\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)  # split only once (image_name and caption)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"âš ï¸ Warning: Skipping malformed line: {line}\")\n",
    "        continue\n",
    "    \n",
    "    image_name, caption = parts\n",
    "    segmented_words = segmenter.segment(caption)\n",
    "    segmented_caption = ' '.join(segmented_words)  # join back with space\n",
    "    segmented_line = f\"{image_name} {segmented_caption}\"\n",
    "    segmented_lines.append(segmented_line)\n",
    "\n",
    "# === Step 5: Save new segmented label file\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"âœ… Word-segmented captions saved to {output_label_path}\")\n",
    "print(f\"âœ… Total lines processed: {len(segmented_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e26406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting khmercut\n",
      "  Using cached khmercut-0.1.0.tar.gz (5.9 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-crfsuite (from khmercut)\n",
      "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: khmercut\n",
      "  Building wheel for khmercut (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for khmercut: filename=khmercut-0.1.0-py3-none-any.whl size=5872135 sha256=fab439dc37439034490153ebd2859a5d8c5122573cf226b167384902d473ace0\n",
      "  Stored in directory: /home/vitoupro/.cache/pip/wheels/11/56/60/586253cc7205c610ea0644cff5e1e0c1ce57798756c2f1ab9e\n",
      "Successfully built khmercut\n",
      "Installing collected packages: python-crfsuite, khmercut\n",
      "Successfully installed khmercut-0.1.0 python-crfsuite-0.9.11\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install khmercut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623237ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k_khmercut.txt\n",
      "âœ… Total lines processed: 4989\n"
     ]
    }
   ],
   "source": [
    "from khmercut import tokenize\n",
    "import os\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset5k.txt'  # raw label\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k_khmercut.txt'  # â¬…ï¸ NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Segment each caption using khmercut.tokenize\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"âš ï¸ Skipping malformed line: {line}\")\n",
    "        continue\n",
    "\n",
    "    image_name, caption = parts\n",
    "    segmented_words = tokenize(caption)  # âœ… Use khmercut's tokenizer here\n",
    "    segmented_caption = ' '.join(segmented_words)\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 4: Save segmented result\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"âœ… Word-segmented captions saved to {output_label_path}\")\n",
    "print(f\"âœ… Total lines processed: {len(segmented_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2176bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocabulary saved to:\n",
      "â†’ word2idx_level_khmercut.json\n",
      "â†’ idx2word_level_khmercut.json\n",
      "ğŸŸ© Total unique words (excluding special tokens): 4066\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === Step 1: Load segmented caption file\n",
    "segmented_file_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k_khmercut.txt'\n",
    "\n",
    "captions = []\n",
    "with open(segmented_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# === Step 2: Tokenize into word list\n",
    "tokenized_captions = [caption.split() for caption in captions]\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "\n",
    "# === Step 3: Create vocab with special tokens\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "vocab_words = special_tokens + sorted(set(all_words))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# === Step 4: Save vocab files\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_level_khmercut.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_level_khmercut.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… Vocabulary saved to:\")\n",
    "print(\"â†’ word2idx_level_khmercut.json\")\n",
    "print(\"â†’ idx2word_level_khmercut.json\")\n",
    "print(f\"ğŸŸ© Total unique words (excluding special tokens): {len(vocab_words) - len(special_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afa30ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt\n",
      "âœ… Total lines processed: 425\n"
     ]
    }
   ],
   "source": [
    "from khmercut import tokenize\n",
    "import os\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'  # raw label\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt'  # â¬…ï¸ NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Segment each caption using khmercut.tokenize\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"âš ï¸ Skipping malformed line: {line}\")\n",
    "        continue\n",
    "\n",
    "    image_name, caption = parts\n",
    "    segmented_words = tokenize(caption)  # âœ… Use khmercut's tokenizer here\n",
    "    segmented_caption = ' '.join(segmented_words)\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 4: Save segmented result\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"âœ… Word-segmented captions saved to {output_label_path}\")\n",
    "print(f\"âœ… Total lines processed: {len(segmented_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d94bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocabulary saved to:\n",
      "â†’ word2idx_level_khmercut.json\n",
      "â†’ idx2word_level_khmercut.json\n",
      "ğŸŸ© Total unique words (excluding special tokens): 371\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === Step 1: Load segmented caption file\n",
    "segmented_file_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt'\n",
    "\n",
    "captions = []\n",
    "with open(segmented_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# === Step 2: Tokenize into word list\n",
    "tokenized_captions = [caption.split() for caption in captions]\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "\n",
    "# === Step 3: Create vocab with special tokens\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "vocab_words = special_tokens + sorted(set(all_words))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# === Step 4: Save vocab files\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_level_khmercut_420.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_level_khmercut_420.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… Vocabulary saved to:\")\n",
    "print(\"â†’ word2idx_level_khmercut.json\")\n",
    "print(\"â†’ idx2word_level_khmercut.json\")\n",
    "print(f\"ğŸŸ© Total unique words (excluding special tokens): {len(vocab_words) - len(special_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf409b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
