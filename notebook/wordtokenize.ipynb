{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fcf849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement khmerwordsegment (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for khmerwordsegment\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install khmerwordsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d2b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total captions: 425\n",
      "‚úÖ Vocabulary size: 425\n",
      "‚úÖ Max sequence length: 3\n",
      "\n",
      "üìå Sample Caption: ·ûÄ·ûº·ûì·ûñ·üÇ·ûÑ·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûî·ûΩ·ûì·ûì·û∑·ûÑ·ûÄ·üÜ·ûü·üÄ·ûú·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûò·ûΩ·ûô\n",
      "üß© Tokenized: ['·ûÄ·ûº·ûì·ûñ·üÇ·ûÑ·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûî·ûΩ·ûì·ûì·û∑·ûÑ·ûÄ·üÜ·ûü·üÄ·ûú·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûò·ûΩ·ûô']\n",
      "üî¢ Sequence: [1, 30, 2]\n",
      "üìè Padded: [ 1 30  2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# === Step 1: Provide your file path here ===\n",
    "file_path = \"/home/vitoupro/code/image_captioning/data/raw/rawimglabel.txt\"  # üîÅ Replace with your actual path\n",
    "\n",
    "# === Step 2: Load captions from the file ===\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "# Clean up: remove blank lines and trim whitespaces\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Tokenize each caption into characters ===\n",
    "# Word-level tokenization\n",
    "tokenized_words = [caption.split() for caption in captions]\n",
    "\n",
    "# === Step 4: Build vocabulary with special tokens ===\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "# Build vocab\n",
    "all_words = [word for caption in tokenized_words for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "\n",
    "\n",
    "word2idx_words = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word_words = {idx: word for word, idx in word2idx_words.items()}\n",
    "\n",
    "# === Step 5: Convert captions to ID sequences ===\n",
    "# Convert to ID sequences\n",
    "def caption_to_ids_words(word_list):\n",
    "    return [word2idx_words['<START>']] + [word2idx_words.get(word, word2idx_words['<UNK>']) for word in word_list] + [word2idx_words['<END>']]\n",
    "\n",
    "sequences_words = [caption_to_ids_words(words) for words in tokenized_words]\n",
    "\n",
    "# === Step 6: Pad sequences ===\n",
    "max_len = max(len(seq) for seq in sequences_words)\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx_words['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "\n",
    "# Padding\n",
    "max_len_words = max(len(seq) for seq in sequences_words)\n",
    "padded_sequences_words = [pad_sequence(seq, max_len_words) for seq in sequences_words]\n",
    "padded_sequences_words = np.array(padded_sequences_words)\n",
    "\n",
    "# === Step 7: Preview results ===\n",
    "print(f\"‚úÖ Total captions: {len(captions)}\")\n",
    "print(f\"‚úÖ Vocabulary size: {len(all_words)}\")\n",
    "print(f\"‚úÖ Max sequence length: {max_len}\")\n",
    "print(\"\\nüìå Sample Caption:\", captions[0])\n",
    "print(\"üß© Tokenized:\", tokenized_words[0])\n",
    "print(\"üî¢ Sequence:\", sequences_words[0])\n",
    "print(\"üìè Padded:\", padded_sequences_words[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa711cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocabulary saved to word2idx_words.json and idx2word_words.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save word2idx_wordleveltest5k and idx2word_wordleveltest5k to a file\n",
    "with open(\"word2idx_words.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(word2idx_words, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"idx2word_words.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(idx2word_words, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Vocabulary saved to word2idx_words.json and idx2word_words.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a3e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished!\n",
      "‚úÖ Total captions: 425\n",
      "‚úÖ Vocabulary size (including special tokens): 481\n",
      "‚úÖ Max sequence length: 28\n",
      "‚úÖ Saved word2idx_wordlevel.json and idx2word_wordlevel.json into /home/vitoupro/code/image_captioning/data/processed/\n",
      "\n",
      "üìå Example caption: ·ûÄ·ûº·ûì·ûñ·üÇ·ûÑ·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûî·ûΩ·ûì·ûì·û∑·ûÑ·ûÄ·üÜ·ûü·üÄ·ûú·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûò·ûΩ·ûô\n",
      "üß© Tokenized: ['·ûÄ·ûº·ûì', '·ûñ·üÇ·ûÑ', '·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ', '·ûî·ûΩ·ûì', '·ûì·û∑·ûÑ', '·ûÄ·üÜ·ûü·üÄ·ûú', '·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ', '·ûò·ûΩ·ûô']\n",
      "üî¢ Sequence: [1, 26, 333, 310, 274, 242, 38, 310, 353, 2]\n",
      "üìè Padded: [  1  26 333 310 274 242  38 310 353   2   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/rawimglabel.txt'  # üîÅ Update if needed\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # üîÅ Your Khmer dictionary\n",
    "\n",
    "# === Step 2: Load Captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Word Segmentation\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "tokenized_captions = [segmenter.segment(caption) for caption in captions]\n",
    "\n",
    "# === Step 4: Build Vocabulary\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Sort vocab by frequency (optional), or just alphabetically\n",
    "vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "\n",
    "word2idx_wordleveltest5k = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word_wordleveltest5k = {idx: word for word, idx in word2idx_wordleveltest5k.items()}\n",
    "\n",
    "# === Step 5: Convert Captions to ID Sequences\n",
    "def caption_to_ids(word_list):\n",
    "    return [word2idx_wordleveltest5k['<START>']] + [word2idx_wordleveltest5k.get(word, word2idx_wordleveltest5k['<UNK>']) for word in word_list] + [word2idx_wordleveltest5k['<END>']]\n",
    "\n",
    "sequences = [caption_to_ids(words) for words in tokenized_captions]\n",
    "\n",
    "# === Step 6: Padding\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx_wordleveltest5k['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "padded_sequences = [pad_sequence(seq, max_len) for seq in sequences]\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "# === Step 7: Save Vocabulary\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Finished!\")\n",
    "print(f\"‚úÖ Total captions: {len(captions)}\")\n",
    "print(f\"‚úÖ Vocabulary size (including special tokens): {len(word2idx_wordleveltest5k)}\")\n",
    "print(f\"‚úÖ Max sequence length: {max_len}\")\n",
    "print(f\"‚úÖ Saved word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json into {save_dir}\")\n",
    "\n",
    "# === Step 8: Preview\n",
    "print(\"\\nüìå Example caption:\", captions[0])\n",
    "print(\"üß© Tokenized:\", tokenized_captions[0])\n",
    "print(\"üî¢ Sequence:\", sequences[0])\n",
    "print(\"üìè Padded:\", padded_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4331ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocabulary saved to word2idx_wordlevel.json and idx2word_wordlevel.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save word2idx_wordleveltest5k and idx2word_wordleveltest5k to a file\n",
    "with open(\"word2idx_wordleveltest5k.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(word2idx_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"idx2word_level.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(idx2word_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Vocabulary saved to word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608a6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'  # your current label\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # your Khmer dictionary\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel.txt'  # ‚¨ÖÔ∏è NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Initialize segmenter\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "# === Step 4: Process and segment\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)  # split only once (image and caption)\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    image_name, caption = parts\n",
    "    segmented_words = segmenter.segment(caption)\n",
    "    segmented_caption = ' '.join(segmented_words)  # join back with space\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 5: Save new segmented label file\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Word-segmented captions saved to {output_label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d880854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_gpt.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'  # your current label\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/gptdic.txt'  # your Khmer dictionary\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_gpt.txt'  # ‚¨ÖÔ∏è NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Initialize segmenter\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "# === Step 4: Process and segment\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)  # split only once (image and caption)\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    image_name, caption = parts\n",
    "    segmented_words = segmenter.segment(caption)\n",
    "    segmented_caption = ' '.join(segmented_words)  # join back with space\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 5: Save new segmented label file\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Word-segmented captions saved to {output_label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rough dictionary generated with 22409 words!\n"
     ]
    }
   ],
   "source": [
    "#dic kloun eng\n",
    "# Build rough dictionary from captions themselves\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'\n",
    "output_dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic_auto_generated.txt'\n",
    "\n",
    "captions = []\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# Naive split: treat every continuous 2, 3, 4, 5, 6 character groups as possible words\n",
    "generated_words = set()\n",
    "\n",
    "for caption in captions:\n",
    "    for size in range(1, 8):  # try 2 to 6 character words\n",
    "        for i in range(len(caption) - size + 1):\n",
    "            substring = caption[i:i+size]\n",
    "            generated_words.add(substring)\n",
    "\n",
    "# Save the rough dictionary\n",
    "with open(output_dict_path, 'w', encoding='utf-8') as f:\n",
    "    for word in sorted(generated_words):\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Rough dictionary generated with {len(generated_words)} words!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612d1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total lines: 425\n",
      "\n",
      "[Line 1] ·ûÄ·ûº·ûì·ûñ·üÇ·ûÑ·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûî·ûΩ·ûì·ûì·û∑·ûÑ·ûÄ·üÜ·ûü·üÄ·ûú·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûò·ûΩ·ûô\n",
      "[Line 2] ·ûÜ·üí·ûÄ·üÇ·û¢·ûÑ·üí·ûÇ·ûª·ûô·ûõ·ûæ·ûü·û∂·û°·ûª·ûÑ\n",
      "[Line 3] ·ûò·û∂·ûì·ûò·ûΩ·ûÄ·ûì·üÖ·ûõ·ûæ·ûú·üâ·û∂·ûõ·û∏·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ\n",
      "[Line 4] ·ûê·ûº·ûñ·ûé·üå·ûü·ûò·ûΩ·ûô·ûì·üÖ·ûá·û∑·ûè·ûÄ·üÖ·û¢·û∏·ûì·û∑·ûÑ·ûò·û∂·ûì·ûê·ûº·ûñ·ûé·üå·ûü·ûò·ûΩ·ûô·ûë·üÄ·ûè·ûì·üÖ·ûõ·ûæ·ûÄ·üÖ·û¢·û∏\n",
      "[Line 5] ·ûÄ·üÜ·ûü·üÄ·ûú·ûì·üÖ·ûõ·ûæ·ûè·ûª·ûÖ·ûÑ·üí·ûÄ·üí·ûö·û∂·ûì·ûî·û∂·ûô\n",
      "[Line 6] ·ûÄ·ûì·üí·ûè·üí·ûö·ûÄ·ûñ·ûé·üå·ûü\n",
      "[Line 7] ·ûò·û∂·ûì·ûÄ·û∂·üÜ·ûî·û∑·ûè·ûü·üí·ûö·ûΩ·ûÖ·ûì·û∑·ûÑ·ûï·üí·ûõ·üÇ·ûü·üí·ûè·ûî·û∫·ûö·û∏·ûì·üÖ·ûÄ·üí·ûì·ûª·ûÑ·ûÄ·ûì·üí·ûè·üí·ûö·ûÄ\n",
      "[Line 8] ·ûÜ·üí·ûÄ·üÇ·ûñ·û∂·ûÄ·üã·ûò·ûΩ·ûÄ·û¢·ûÑ·üí·ûÇ·ûª·ûô·ûá·û∑·ûè·ûú·üâ·û∂·ûõ·û∏·ûñ·ûé·üå·ûõ·ûø·ûÑ\n",
      "[Line 9] ·ûÄ·üÖ·û¢·û∏·ûñ·û∏·ûö·ûì·üÖ·ûá·û∑·ûè·ûÇ·üí·ûì·û∂·ûì·û∑·ûÑ·ûò·û∂·ûì·ûÅ·üí·ûì·ûæ·ûô·ûì·üÖ·ûõ·ûæ·ûÄ·üÖ·û¢·û∏·ûë·û∂·üÜ·ûÑ·ûñ·û∏·ûö\n",
      "[Line 10] ·ûî·üí·ûö·û¢·ûî·üã·ûî·û∂·ûô·ûñ·ûé·üå·ûï·üí·ûÄ·û∂·ûà·ûº·ûÄ\n"
     ]
    }
   ],
   "source": [
    "# Check captions properly\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/rawimglabel.txt'\n",
    "\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"‚úÖ Total lines: {len(lines)}\\n\")\n",
    "\n",
    "for i, line in enumerate(lines[:10]):  # show first 10 lines\n",
    "    print(f\"[Line {i+1}] {line.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c1d8c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved cleaned word2idx_level.json and idx2word_level.json!\n",
      "‚úÖ Vocabulary size: 433\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load segmented captions\n",
    "segmented_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel.txt'\n",
    "\n",
    "captions = []\n",
    "with open(segmented_label_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# Tokenize into words\n",
    "tokenized_captions = [caption.split() for caption in captions]\n",
    "\n",
    "# Build vocabulary\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "\n",
    "# üõ† Correct filtering: keep only words with length > 1\n",
    "filtered_words = [word for word in set(all_words) if len(word) > 1]\n",
    "\n",
    "# üõ† Build vocab from filtered words\n",
    "vocab_words = special_tokens + sorted(filtered_words)\n",
    "\n",
    "# Create word2idx and idx2word\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Save\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_level.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_level.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Saved cleaned word2idx_level.json and idx2word_level.json!\")\n",
    "print(f\"‚úÖ Vocabulary size: {len(word2idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "803fd1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished fixing segmentation! Saved new file to: /home/vitoupro/code/image_captioning/data/processed/fixed_word_segmented_imglabel.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# üõ† Step 1: Define function to fix Khmer word segmentation\n",
    "def fix_khmer_segmentation(text):\n",
    "    words = text.strip().split()\n",
    "\n",
    "    # Khmer unicode range\n",
    "    khmer_range = re.compile(r'[\\u1780-\\u17FF]+')\n",
    "\n",
    "    fixed_words = []\n",
    "    for word in words:\n",
    "        if not khmer_range.match(word):\n",
    "            # Not a full Khmer word => likely broken vowel/diacritic => merge with previous\n",
    "            if fixed_words:\n",
    "                fixed_words[-1] += word\n",
    "            else:\n",
    "                fixed_words.append(word)\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    "    return ' '.join(fixed_words)\n",
    "\n",
    "# üõ† Step 2: Apply it to fix the whole caption file\n",
    "input_file = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel.txt'\n",
    "output_file = '/home/vitoupro/code/image_captioning/data/processed/fixed_word_segmented_imglabel.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "fixed_lines = []\n",
    "for line in lines:\n",
    "    parts = line.split(' ', 1)\n",
    "    if len(parts) == 2:\n",
    "        img_name, caption = parts\n",
    "        fixed_caption = fix_khmer_segmentation(caption)\n",
    "        fixed_lines.append(f\"{img_name} {fixed_caption}\")\n",
    "\n",
    "# üõ† Step 3: Save the fixed captions\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for line in fixed_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Finished fixing segmentation! Saved new file to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7884a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished!\n",
      "‚úÖ Total captions: 998\n",
      "‚úÖ Vocabulary size (including special tokens): 2262\n",
      "‚úÖ Max sequence length: 185\n",
      "‚úÖ Saved word2idx_wordleveltest.json and idx2word_wordleveltest.json into /home/vitoupro/code/image_captioning/data/processed/\n",
      "\n",
      "üìå Example caption: image_1.jpg ·ûÄ·üí·ûò·üÅ·ûÑ·ûî·üí·ûö·ûª·ûü·ûñ·û∏·ûö·ûì·û∂·ûÄ·üã·ûä·üÇ·ûõ·ûò·û∂·ûì·ûü·ûÄ·üã·ûö·ûâ·üâ·üÅ·ûö·ûâ·üâ·üÉ·ûÄ·üÜ·ûñ·ûª·ûÑ·ûò·ûæ·ûõ·ûë·üÖ·ûä·üÉ·ûö·ûî·ûü·üã·ûñ·ûΩ·ûÄ·ûÇ·üÅ ·ûÅ·ûé·üà·ûñ·ûΩ·ûÄ·ûÇ·üÅ·ûÄ·üÜ·ûñ·ûª·ûÑ·û¢·ûÑ·üí·ûÇ·ûª·ûô·ûõ·üÅ·ûÑ·ûì·üÖ·ûÄ·üí·ûì·ûª·ûÑ·ûú·û∂·ûõ·üî\n",
      "üß© Tokenized: ['i', 'm', 'a', 'g', 'e', '_', '1', '.', 'j', 'p', 'g', ' ', '·ûÄ·üí·ûò·üÅ·ûÑ·ûî·üí·ûö·ûª·ûü', '·ûñ·û∏·ûö', '·ûì·û∂·ûÄ·üã', '·ûä·üÇ·ûõ·ûò·û∂·ûì', '·ûü·ûÄ', '·üã', '·ûö·ûâ·üâ·üÅ·ûö·ûâ·üâ·üÉ', '·ûÄ·üÜ·ûñ·ûª·ûÑ', '·ûò·ûæ·ûõ·ûë·üÖ', '·ûä·üÉ', '·ûö·ûî·ûü·üã', '·ûñ·ûΩ·ûÄ·ûÇ·üÅ', ' ', '·ûÅ·ûé·üà', '·ûñ·ûΩ·ûÄ·ûÇ·üÅ', '·ûÄ·üÜ·ûñ·ûª·ûÑ', '·û¢·ûÑ·üí·ûÇ·ûª·ûô·ûõ·üÅ·ûÑ', '·ûì·üÖ·ûÄ·üí·ûì·ûª·ûÑ', '·ûú', '·û∂', '·ûõ', '·üî']\n",
      "üî¢ Sequence: [1, 47, 51, 39, 45, 43, 38, 8, 6, 48, 54, 45, 4, 264, 1394, 1038, 737, 1841, 2251, 1637, 226, 1570, 738, 1659, 1411, 4, 303, 1411, 226, 2097, 1071, 1803, 2230, 1739, 2257, 2]\n",
      "üìè Padded: [   1   47   51   39   45   43   38    8    6   48   54   45    4  264\n",
      " 1394 1038  737 1841 2251 1637  226 1570  738 1659 1411    4  303 1411\n",
      "  226 2097 1071 1803 2230 1739 2257    2    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset.txt'  # üîÅ Update if needed\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # üîÅ Your Khmer dictionary\n",
    "\n",
    "# === Step 2: Load Captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Word Segmentation\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "tokenized_captions = [segmenter.segment(caption) for caption in captions]\n",
    "\n",
    "# === Step 4: Build Vocabulary\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Sort vocab by frequency (optional), or just alphabetically\n",
    "vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "\n",
    "word2idx_wordleveltest5k = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word_wordleveltest5k = {idx: word for word, idx in word2idx_wordleveltest5k.items()}\n",
    "\n",
    "# === Step 5: Convert Captions to ID Sequences\n",
    "def caption_to_ids(word_list):\n",
    "    return [word2idx_wordleveltest5k['<START>']] + [word2idx_wordleveltest5k.get(word, word2idx_wordleveltest5k['<UNK>']) for word in word_list] + [word2idx_wordleveltest5k['<END>']]\n",
    "\n",
    "sequences = [caption_to_ids(words) for words in tokenized_captions]\n",
    "\n",
    "# === Step 6: Padding\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx_wordleveltest5k['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "padded_sequences = [pad_sequence(seq, max_len) for seq in sequences]\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "# === Step 7: Save Vocabulary\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Finished!\")\n",
    "print(f\"‚úÖ Total captions: {len(captions)}\")\n",
    "print(f\"‚úÖ Vocabulary size (including special tokens): {len(word2idx_wordleveltest5k)}\")\n",
    "print(f\"‚úÖ Max sequence length: {max_len}\")\n",
    "print(f\"‚úÖ Saved word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json into {save_dir}\")\n",
    "\n",
    "# === Step 8: Preview\n",
    "print(\"\\nüìå Example caption:\", captions[0])\n",
    "print(\"üß© Tokenized:\", tokenized_captions[0])\n",
    "print(\"üî¢ Sequence:\", sequences[0])\n",
    "print(\"üìè Padded:\", padded_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874b9152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset.txt'  # your current label\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # your Khmer dictionary\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest.txt'  # ‚¨ÖÔ∏è NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Initialize segmenter\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "# === Step 4: Process and segment\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)  # split only once (image and caption)\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    image_name, caption = parts\n",
    "    segmented_words = segmenter.segment(caption)\n",
    "    segmented_caption = ' '.join(segmented_words)  # join back with space\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 5: Save new segmented label file\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Word-segmented captions saved to {output_label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790c9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished!\n",
      "‚úÖ Total captions: 4989\n",
      "‚úÖ Vocabulary size (including special tokens): 4227\n",
      "‚úÖ Max sequence length: 171\n",
      "‚úÖ Saved word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json into /home/vitoupro/code/image_captioning/data/processed/\n",
      "\n",
      "üìå Example image: image_1.jpg\n",
      "üìå Example caption: ·ûî·ûª·ûö·ûü·ûú·üê·ûô·ûÄ·üí·ûò·üÅ·ûÑ·ûñ·û∏·ûö·ûì·û∂·ûÄ·üã·ûä·üÇ·ûõ·ûò·û∂·ûì·ûü·ûÄ·üã·ûÅ·üí·ûò·û∂·ûü·üã·û¢·üÄ·ûì·ûò·ûæ·ûõ·ûë·üÖ·ûä·üÉ·ûö·ûî·ûü·üã·ûñ·ûΩ·ûÄ·ûÇ·üÅ·ûì·üÖ·ûñ·üÅ·ûõ·ûä·ûæ·ûö·ûõ·üÅ·ûÑ·ûì·üÖ·ûë·û∏·ûí·üí·ûõ·û∂\n",
      "üß© Tokenized: ['·ûî·ûª·ûö·ûü', '·ûú·üê·ûô·ûÄ·üí·ûò·üÅ·ûÑ', '·ûñ·û∏·ûö', '·ûì·û∂·ûÄ·üã', '·ûä·üÇ·ûõ·ûò·û∂·ûì', '·ûü·ûÄ', '·üã', '·ûÅ·üí·ûò·û∂·ûü', '·üã', '·û¢·üÄ·ûì', '·ûò·ûæ·ûõ·ûë·üÖ', '·ûä·üÉ', '·ûö·ûî·ûü·üã', '·ûñ·ûΩ·ûÄ·ûÇ·üÅ', '·ûì·üÖ·ûñ·üÅ·ûõ', '·ûä·ûæ·ûö·ûõ·üÅ·ûÑ', '·ûì·üÖ', '·ûë·û∏·ûí·üí·ûõ·û∂']\n",
      "üî¢ Sequence: [1, 2132, 3400, 2535, 1891, 1340, 3402, 4219, 628, 4219, 4053, 2847, 1341, 3025, 2562, 1971, 1320, 1947, 1672, 2]\n",
      "üìè Padded: [   1 2132 3400 2535 1891 1340 3402 4219  628 4219 4053 2847 1341 3025\n",
      " 2562 1971 1320 1947 1672    2    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "#for 5k\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset5k.txt'  # üîÅ Your label file\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # üîÅ Your Khmer dictionary\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'  # üîÅ Where to save output\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Step 2: Load Captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_lines = f.readlines()\n",
    "\n",
    "# === Step 3: Split out image names and captions\n",
    "image_names = []\n",
    "captions = []\n",
    "\n",
    "for line in raw_lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    parts = line.split(' ', 1)  # split into image name and caption\n",
    "    if len(parts) == 2:\n",
    "        image_name, caption = parts\n",
    "        image_names.append(image_name)\n",
    "        captions.append(caption)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed line: {line}\")\n",
    "\n",
    "# === Step 4: Word Segmentation (on captions only)\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "tokenized_captions = [segmenter.segment(caption) for caption in captions]\n",
    "\n",
    "# === Step 5: Build Vocabulary\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Sort vocab by frequency (optional) or alphabetically\n",
    "vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "\n",
    "word2idx_wordleveltest5k = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word_wordleveltest5k = {idx: word for word, idx in word2idx_wordleveltest5k.items()}\n",
    "\n",
    "# === Step 6: Convert Captions to ID Sequences\n",
    "def caption_to_ids(word_list):\n",
    "    return [word2idx_wordleveltest5k['<START>']] + [word2idx_wordleveltest5k.get(word, word2idx_wordleveltest5k['<UNK>']) for word in word_list] + [word2idx_wordleveltest5k['<END>']]\n",
    "\n",
    "sequences = [caption_to_ids(words) for words in tokenized_captions]\n",
    "\n",
    "# === Step 7: Padding\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx_wordleveltest5k['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "padded_sequences = [pad_sequence(seq, max_len) for seq in sequences]\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "# === Step 8: Save Vocabulary\n",
    "with open(os.path.join(save_dir, 'word2idx_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_wordleveltest5k.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word_wordleveltest5k, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Finished!\")\n",
    "print(f\"‚úÖ Total captions: {len(captions)}\")\n",
    "print(f\"‚úÖ Vocabulary size (including special tokens): {len(word2idx_wordleveltest5k)}\")\n",
    "print(f\"‚úÖ Max sequence length: {max_len}\")\n",
    "print(f\"‚úÖ Saved word2idx_wordleveltest5k.json and idx2word_wordleveltest5k.json into {save_dir}\")\n",
    "\n",
    "# === Step 9: Preview\n",
    "print(\"\\nüìå Example image:\", image_names[0])\n",
    "print(\"üìå Example caption:\", captions[0])\n",
    "print(\"üß© Tokenized:\", tokenized_captions[0])\n",
    "print(\"üî¢ Sequence:\", sequences[0])\n",
    "print(\"üìè Padded:\", padded_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f848f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k.txt\n",
      "‚úÖ Total lines processed: 4989\n"
     ]
    }
   ],
   "source": [
    "#for 5k\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Khmer Word Segmenter\n",
    "class KhmerWordSegmenter:\n",
    "    def __init__(self, dict_path):\n",
    "        self.word_dict = set()\n",
    "        self.max_word_len = 0\n",
    "        self.load_dictionary(dict_path)\n",
    "\n",
    "    def load_dictionary(self, dict_path):\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    if len(word) > self.max_word_len:\n",
    "                        self.max_word_len = len(word)\n",
    "\n",
    "    def segment(self, text):\n",
    "        result = []\n",
    "        index = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while index < text_length:\n",
    "            matched = False\n",
    "            for size in range(self.max_word_len, 0, -1):\n",
    "                if index + size > text_length:\n",
    "                    continue\n",
    "                candidate = text[index:index+size]\n",
    "                if candidate in self.word_dict:\n",
    "                    result.append(candidate)\n",
    "                    index += size\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                result.append(text[index])\n",
    "                index += 1\n",
    "        return result\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset5k.txt'  # your current label\n",
    "dict_path = '/home/vitoupro/code/image_captioning/data/raw/dic.txt'  # your Khmer dictionary\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k.txt'  # ‚¨ÖÔ∏è NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Initialize segmenter\n",
    "segmenter = KhmerWordSegmenter(dict_path)\n",
    "\n",
    "# === Step 4: Process and segment\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)  # split only once (image_name and caption)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"‚ö†Ô∏è Warning: Skipping malformed line: {line}\")\n",
    "        continue\n",
    "    \n",
    "    image_name, caption = parts\n",
    "    segmented_words = segmenter.segment(caption)\n",
    "    segmented_caption = ' '.join(segmented_words)  # join back with space\n",
    "    segmented_line = f\"{image_name} {segmented_caption}\"\n",
    "    segmented_lines.append(segmented_line)\n",
    "\n",
    "# === Step 5: Save new segmented label file\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Word-segmented captions saved to {output_label_path}\")\n",
    "print(f\"‚úÖ Total lines processed: {len(segmented_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e26406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting khmercut\n",
      "  Using cached khmercut-0.1.0.tar.gz (5.9 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-crfsuite (from khmercut)\n",
      "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: khmercut\n",
      "  Building wheel for khmercut (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for khmercut: filename=khmercut-0.1.0-py3-none-any.whl size=5872135 sha256=fab439dc37439034490153ebd2859a5d8c5122573cf226b167384902d473ace0\n",
      "  Stored in directory: /home/vitoupro/.cache/pip/wheels/11/56/60/586253cc7205c610ea0644cff5e1e0c1ce57798756c2f1ab9e\n",
      "Successfully built khmercut\n",
      "Installing collected packages: python-crfsuite, khmercut\n",
      "Successfully installed khmercut-0.1.0 python-crfsuite-0.9.11\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install khmercut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623237ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k_khmercut.txt\n",
      "‚úÖ Total lines processed: 4989\n"
     ]
    }
   ],
   "source": [
    "from khmercut import tokenize\n",
    "import os\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/translated_dataset5k.txt'  # raw label\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k_khmercut.txt'  # ‚¨ÖÔ∏è NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Segment each caption using khmercut.tokenize\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed line: {line}\")\n",
    "        continue\n",
    "\n",
    "    image_name, caption = parts\n",
    "    segmented_words = tokenize(caption)  # ‚úÖ Use khmercut's tokenizer here\n",
    "    segmented_caption = ' '.join(segmented_words)\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 4: Save segmented result\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Word-segmented captions saved to {output_label_path}\")\n",
    "print(f\"‚úÖ Total lines processed: {len(segmented_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2176bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocabulary saved to:\n",
      "‚Üí word2idx_level_khmercut.json\n",
      "‚Üí idx2word_level_khmercut.json\n",
      "üü© Total unique words (excluding special tokens): 4066\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === Step 1: Load segmented caption file\n",
    "segmented_file_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest5k_khmercut.txt'\n",
    "\n",
    "captions = []\n",
    "with open(segmented_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# === Step 2: Tokenize into word list\n",
    "tokenized_captions = [caption.split() for caption in captions]\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "\n",
    "# === Step 3: Create vocab with special tokens\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "vocab_words = special_tokens + sorted(set(all_words))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# === Step 4: Save vocab files\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_level_khmercut.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_level_khmercut.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Vocabulary saved to:\")\n",
    "print(\"‚Üí word2idx_level_khmercut.json\")\n",
    "print(\"‚Üí idx2word_level_khmercut.json\")\n",
    "print(f\"üü© Total unique words (excluding special tokens): {len(vocab_words) - len(special_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afa30ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt\n",
      "‚úÖ Total lines processed: 425\n"
     ]
    }
   ],
   "source": [
    "from khmercut import tokenize\n",
    "import os\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'  # raw label\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt'  # ‚¨ÖÔ∏è NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Segment each caption using khmercut.tokenize\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed line: {line}\")\n",
    "        continue\n",
    "\n",
    "    image_name, caption = parts\n",
    "    segmented_words = tokenize(caption)  # ‚úÖ Use khmercut's tokenizer here\n",
    "    segmented_caption = ' '.join(segmented_words)\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 4: Save segmented result\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Word-segmented captions saved to {output_label_path}\")\n",
    "print(f\"‚úÖ Total lines processed: {len(segmented_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d94bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocabulary saved to:\n",
      "‚Üí word2idx_level_khmercut.json\n",
      "‚Üí idx2word_level_khmercut.json\n",
      "üü© Total unique words (excluding special tokens): 371\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === Step 1: Load segmented caption file\n",
    "segmented_file_path = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt'\n",
    "\n",
    "captions = []\n",
    "with open(segmented_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# === Step 2: Tokenize into word list\n",
    "tokenized_captions = [caption.split() for caption in captions]\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "\n",
    "# === Step 3: Create vocab with special tokens\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "vocab_words = special_tokens + sorted(set(all_words))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# === Step 4: Save vocab files\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_level_khmercut_420.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_level_khmercut_420.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Vocabulary saved to:\")\n",
    "print(\"‚Üí word2idx_level_khmercut.json\")\n",
    "print(\"‚Üí idx2word_level_khmercut.json\")\n",
    "print(f\"üü© Total unique words (excluding special tokens): {len(vocab_words) - len(special_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf409b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
