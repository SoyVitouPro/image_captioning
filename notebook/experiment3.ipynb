{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding and decoding functions\n",
    "def encode_khmer_word(word, word2idx):\n",
    "    indices = []\n",
    "    for character in word:\n",
    "        index = word2idx.get(character)\n",
    "        if index is None:\n",
    "            return None, f\"Character '{character}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "def decode_indices(indices, idx2word):\n",
    "    characters = []\n",
    "    for index in indices:\n",
    "        character = idx2word.get(str(index))\n",
    "        if character is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        characters.append(character)\n",
    "    return ''.join(characters), None\n",
    "\n",
    "# Model Definitions (EncoderCNN and DecoderRNN)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(hidden_size, hidden_size)  # Initialize LSTM hidden state\n",
    "        self.init_c = nn.Linear(hidden_size, hidden_size)  # Initialize LSTM cell state\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)\n",
    "        h0 = self.init_h(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c0 = self.init_c(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        lstm_out, _ = self.lstm(embeddings, (h0, c0))\n",
    "        outputs = self.linear(lstm_out)\n",
    "        return outputs\n",
    "\n",
    "# Image Captioning Dataset\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_word(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "]) \n",
    "\n",
    "# Load dataset\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/raw/annotation.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/data/raw/animals'\n",
    "all_images = pd.read_csv(annotations_file, delimiter=' ', names=['image', 'caption'])\n",
    "\n",
    "# Split dataset\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': train_images, 'caption': train_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "eval_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': eval_images, 'caption': eval_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size=512).to(device)\n",
    "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(word2idx), num_layers=1).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "def custom_transform(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Return as list of words\n",
    "    return text.split()\n",
    "    \n",
    "\n",
    "def calculate_wer(gt, pred, epoch, file_path='metric.txt'):\n",
    "    \n",
    "    with open(file_path, 'a') as file:  # Open file in append mode\n",
    "        file.write(f\"Epoch {epoch}\\n\")\n",
    "        \n",
    "        file.write(\"===========================\\n\")\n",
    "        match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "        if match_pred:\n",
    "            content_pred = match_pred.group(1)\n",
    "        file.write(f\"pred: {content_pred}\\n\")\n",
    "        match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "        if match_ground_true:\n",
    "            content_ground_true = match_ground_true.group(1)\n",
    "        file.write(f\"true: {content_ground_true}\\n\")\n",
    "        file.write(\"===========================\\n\")\n",
    "    \n",
    "    if not content_ground_true:  # Ensure non-empty\n",
    "        content_ground_true = ['']\n",
    "    if not content_pred:  # Ensure non-empty\n",
    "        content_pred = ['']\n",
    "    wer_score = jiwer.wer(content_ground_true, content_pred)\n",
    "    \n",
    "    return wer_score\n",
    "\n",
    "def calculate_cer(gt, pred):\n",
    "    match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "    if match_pred:\n",
    "        content_pred = match_pred.group(1)\n",
    "    \n",
    "    match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "    if match_ground_true:\n",
    "        content_ground_true = match_ground_true.group(1)\n",
    "\n",
    "    return jiwer.cer(content_ground_true, content_pred)\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, num_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "            \n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "                \n",
    "                wer = calculate_wer(gt_caption, pred_caption, epoch)\n",
    "                cer = calculate_cer(gt_caption, pred_caption)\n",
    "                total_wer += wer\n",
    "                total_cer += cer\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    print(f\"Average WER: {avg_wer:.2f}, Average CER: {avg_cer:.2f}\")\n",
    "    return avg_wer, avg_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.7804002950831157\n",
      "Average WER: 0.97, Average CER: 0.25\n",
      "Epoch 2: Train Loss: 0.5050743563873011\n",
      "Average WER: 0.83, Average CER: 0.20\n",
      "Epoch 3: Train Loss: 0.38889224136747963\n",
      "Average WER: 0.57, Average CER: 0.15\n",
      "Epoch 4: Train Loss: 0.26699337290554515\n",
      "Average WER: 0.32, Average CER: 0.09\n",
      "Epoch 5: Train Loss: 0.1642110865654015\n",
      "Average WER: 0.22, Average CER: 0.06\n",
      "Epoch 6: Train Loss: 0.08937570180107908\n",
      "Average WER: 0.16, Average CER: 0.04\n",
      "Epoch 7: Train Loss: 0.055989182258887986\n",
      "Average WER: 0.12, Average CER: 0.03\n",
      "Epoch 8: Train Loss: 0.048371379013832025\n",
      "Average WER: 0.11, Average CER: 0.03\n",
      "Epoch 9: Train Loss: 0.03615249984148072\n",
      "Average WER: 0.10, Average CER: 0.03\n",
      "Epoch 10: Train Loss: 0.03486498088644045\n",
      "Average WER: 0.10, Average CER: 0.03\n",
      "Epoch 11: Train Loss: 0.02760678636500748\n",
      "Average WER: 0.12, Average CER: 0.03\n",
      "Epoch 12: Train Loss: 0.024037751562257365\n",
      "Average WER: 0.07, Average CER: 0.02\n",
      "Epoch 13: Train Loss: 0.018246573611821342\n",
      "Average WER: 0.10, Average CER: 0.03\n",
      "Epoch 14: Train Loss: 0.020065130018515558\n",
      "Average WER: 0.13, Average CER: 0.03\n",
      "Epoch 15: Train Loss: 0.012908858150561772\n",
      "Average WER: 0.07, Average CER: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions[:, :-1])\n",
    "        loss = criterion(outputs.view(-1, len(word2idx)), captions[:, 1:].reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {total_loss/len(train_loader)}')\n",
    "    _, wer = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "\n",
    "    if wer < best_wer:\n",
    "        best_wer = wer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ឆ្កែ<END>\n"
     ]
    }
   ],
   "source": [
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Load and transform the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and transfer to device\n",
    "    \n",
    "    # Generate features from the image using the encoder\n",
    "    features = encoder(image)\n",
    "    \n",
    "    # Start the sequence with the <START> token\n",
    "    predicted_indices = [word2idx['<START>']]\n",
    "    predictions = []\n",
    "    \n",
    "    # Initial input to the LSTM is the <START> token\n",
    "    input_idx = torch.tensor([predicted_indices[-1]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Initialize the LSTM state\n",
    "    h, c = None, None\n",
    "    \n",
    "    # Generate words until the <END> token is predicted or the max length is reached\n",
    "    for _ in range(20):  # Assuming max length of 20 for safety\n",
    "        input_idx = input_idx.unsqueeze(0)  # Add batch dimension for single time-step prediction\n",
    "        if h is None and c is None:\n",
    "            # Generate initial hidden states from features\n",
    "            h = decoder.init_h(features).unsqueeze(0).repeat(decoder.num_layers, 1, 1)\n",
    "            c = decoder.init_c(features).unsqueeze(0).repeat(decoder.num_layers, 1, 1)\n",
    "        \n",
    "        outputs, (h, c) = decoder.lstm(decoder.embed(input_idx), (h, c))\n",
    "        outputs = decoder.linear(outputs.squeeze(1))\n",
    "        \n",
    "        # Get the predicted word index\n",
    "        predicted_index = outputs.argmax(-1).item()\n",
    "        predicted_indices.append(predicted_index)\n",
    "        predictions.append(idx2word[str(predicted_index)])  # Decode to word\n",
    "        \n",
    "        # Prepare the next input\n",
    "        input_idx = torch.tensor([predicted_index], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Stop if the <END> token is predicted\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "    \n",
    "    predicted_caption = ' '.join(predictions)  # Join the predicted words\n",
    "    \n",
    "    return predicted_caption\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/vitoupro/code/image_captioning/data/raw/animals/dog/0be3797d3d.jpg'\n",
    "predicted_caption = predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx)\n",
    "print(\"Predicted Caption:\", predicted_caption.replace(\" \", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoder and decoder models\n",
    "torch.save(encoder.state_dict(), 'encoder_v4.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder_v4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'captioning_model_3z.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
