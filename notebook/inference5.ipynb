{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acdd7dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ឆ្កែ\n"
     ]
    }
   ],
   "source": [
    "# inference4.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Model Definitions (ensure these definitions are exactly as they were during training)\n",
    "# Attention Module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "# EncoderCNN with spatial features\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if 'layer4' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((14, 14))\n",
    "        self.embed = nn.Linear(2048, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = self.avgpool(features)\n",
    "        features = features.view(features.size(0), 2048, -1).permute(0, 2, 1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# Decoder with Attention\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, attention_dim=256, num_layers=1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(embed_size, hidden_size, attention_dim)\n",
    "        self.lstm = nn.LSTM(embed_size + embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, encoder_out, captions, sampling_probability=1.0):\n",
    "        batch_size, seq_len = captions.size()\n",
    "        embedded = self.embedding(captions)\n",
    "        h = self.init_h(encoder_out.mean(1)).unsqueeze(0)\n",
    "        c = self.init_c(encoder_out.mean(1)).unsqueeze(0)\n",
    "\n",
    "        inputs = embedded[:, 0, :].unsqueeze(1)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            context, _ = self.attention(encoder_out, h[-1])\n",
    "            lstm_input = torch.cat((inputs.squeeze(1), context), dim=1).unsqueeze(1)\n",
    "            output, (h, c) = self.lstm(lstm_input, (h, c))\n",
    "            output = self.linear(output.squeeze(1))\n",
    "            outputs.append(output)\n",
    "\n",
    "            teacher_force = torch.rand(1).item() > sampling_probability\n",
    "            top1 = output.argmax(1)\n",
    "            inputs = embedded[:, t, :].unsqueeze(1) if teacher_force else self.embedding(top1).unsqueeze(1)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "    \n",
    "\n",
    "# Load vocabulary\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size=256).to(device)\n",
    "decoder = DecoderWithAttention(embed_size=256, hidden_size=512, vocab_size=len(word2idx), num_layers=1).to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "encoder.load_state_dict(torch.load('/home/vitoupro/code/image_captioning/notebook/encoderattdec.pth'))\n",
    "decoder.load_state_dict(torch.load('/home/vitoupro/code/image_captioning/notebook/decoderattdec.pth'))\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Prediction function\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    encoder_out = encoder(image)\n",
    "    h = decoder.init_h(encoder_out.mean(1)).unsqueeze(0)\n",
    "    c = decoder.init_c(encoder_out.mean(1)).unsqueeze(0)\n",
    "\n",
    "    input_idx = torch.tensor([word2idx['<START>']], dtype=torch.long).to(device)\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        embedded = decoder.embedding(input_idx).unsqueeze(1)\n",
    "        context, _ = decoder.attention(encoder_out, h[-1])\n",
    "        lstm_input = torch.cat((embedded.squeeze(1), context), dim=1).unsqueeze(1)\n",
    "\n",
    "        output, (h, c) = decoder.lstm(lstm_input, (h, c))\n",
    "        output = decoder.linear(output.squeeze(1))\n",
    "        predicted_index = output.argmax(-1).item()\n",
    "\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "\n",
    "        predictions.append(idx2word[str(predicted_index)])\n",
    "        input_idx = torch.tensor([predicted_index], dtype=torch.long).to(device)\n",
    "\n",
    "    return ''.join(predictions)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/vitoupro/code/image_captioning/data/2.png'\n",
    "predicted_caption = predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx)\n",
    "print(\"Predicted Caption:\", predicted_caption.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35351557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
