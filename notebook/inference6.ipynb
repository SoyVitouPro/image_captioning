{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470cd2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: សត្វតោ\n"
     ]
    }
   ],
   "source": [
    "# inference4.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Model Definitions (ensure these definitions are exactly as they were during training)\n",
    "# Attention Module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if 'layer4' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # output shape: (B, 2048, 1, 1)\n",
    "        self.linear = nn.Linear(2048, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)             # (B, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # (B, 2048) — safe reshape\n",
    "        features = self.linear(features)                # (B, embed_size)\n",
    "        return features.unsqueeze(1)                    # (B, 1, embed_size)\n",
    "           # (B, 1, embed_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads=8, num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = self.embedding(captions)  # (B, T, E)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "\n",
    "        tgt = tgt.permute(1, 0, 2)      # (T, B, E)\n",
    "        memory = features.permute(1, 0, 2)  # (1, B, E)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)  # (T, B, vocab_size)\n",
    "        return out.permute(1, 0, 2)  # (B, T, vocab_size)\n",
    "    \n",
    "\n",
    "# Load vocabulary\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size=256).to(device)\n",
    "decoder = TransformerDecoder(    vocab_size=len(word2idx),\n",
    "    embed_size=256,\n",
    "    num_heads=8,\n",
    "    num_layers=3).to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "encoder.load_state_dict(torch.load('/home/vitoupro/code/image_captioning/notebook/encodertransf.pth'))\n",
    "decoder.load_state_dict(torch.load('/home/vitoupro/code/image_captioning/notebook/decodertransf.pth'))\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Prediction function\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    memory = encoder(image)  # (1, 1, embed_size)\n",
    "    input_indices = [word2idx['<START>']]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        tgt = torch.tensor([input_indices], dtype=torch.long).to(device)\n",
    "        tgt_embed = decoder.embedding(tgt)\n",
    "        tgt_embed = decoder.pos_encoding(tgt_embed)\n",
    "        tgt_embed = tgt_embed.permute(1, 0, 2)  # (T, 1, E)\n",
    "        memory = memory.permute(1, 0, 2)        # (1, 1, E)\n",
    "\n",
    "        tgt_mask = decoder.generate_square_subsequent_mask(tgt_embed.size(0)).to(device)\n",
    "        output = decoder.transformer_decoder(tgt_embed, memory, tgt_mask=tgt_mask)\n",
    "        output = decoder.fc_out(output[-1])  # (1, vocab_size)\n",
    "\n",
    "        predicted_index = output.argmax(-1).item()\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "\n",
    "        input_indices.append(predicted_index)\n",
    "\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in input_indices[1:]]  # skip <START>\n",
    "    return ''.join(predicted_tokens)\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/vitoupro/code/image_captioning/data/2.png'\n",
    "predicted_caption = predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx)\n",
    "print(\"Predicted Caption:\", predicted_caption.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a5bb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
