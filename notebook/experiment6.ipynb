{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1beac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding and decoding functions\n",
    "def encode_khmer_word(word, word2idx):\n",
    "    indices = []\n",
    "    for character in word:\n",
    "        index = word2idx.get(character)\n",
    "        if index is None:\n",
    "            return None, f\"Character '{character}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "def decode_indices(indices, idx2word):\n",
    "    characters = []\n",
    "    for index in indices:\n",
    "        character = idx2word.get(str(index))\n",
    "        if character is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        characters.append(character)\n",
    "    return ''.join(characters), None\n",
    "\n",
    "# Attention Module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if 'layer4' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # output shape: (B, 2048, 1, 1)\n",
    "        self.linear = nn.Linear(2048, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)             # (B, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # (B, 2048) — safe reshape\n",
    "        features = self.linear(features)                # (B, embed_size)\n",
    "        return features.unsqueeze(1)                    # (B, 1, embed_size)\n",
    "           # (B, 1, embed_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads=8, num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = self.embedding(captions)  # (B, T, E)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "\n",
    "        tgt = tgt.permute(1, 0, 2)      # (T, B, E)\n",
    "        memory = features.permute(1, 0, 2)  # (1, B, E)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)  # (T, B, vocab_size)\n",
    "        return out.permute(1, 0, 2)  # (B, T, vocab_size)\n",
    "\n",
    "\n",
    "# The rest of the code remains the same...\n",
    "# You can now instantiate DecoderRNN with attention in your training setup.\n",
    "# Image Captioning Dataset\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_word(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "# # Define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "# ]) \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/raw/annotation.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/data/raw/animals'\n",
    "all_images = pd.read_csv(annotations_file, delimiter=' ', names=['image', 'caption'])\n",
    "\n",
    "# Split dataset\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': train_images, 'caption': train_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "eval_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': eval_images, 'caption': eval_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Unified embed size\n",
    "embed_size = 256\n",
    "\n",
    "encoder = EncoderCNN(embed_size=embed_size).to(device)\n",
    "decoder = TransformerDecoder(\n",
    "    vocab_size=len(word2idx),\n",
    "    embed_size=embed_size,\n",
    "    num_heads=8,\n",
    "    num_layers=3\n",
    ").to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())  # include all trainable encoder parts\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "def custom_transform(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Return as list of words\n",
    "    return text.split()\n",
    "    \n",
    "\n",
    "def calculate_wer(gt, pred, epoch, file_path='metric.txt'):\n",
    "    # Default empty strings to avoid UnboundLocalError\n",
    "    content_pred = \"\"\n",
    "    content_ground_true = \"\"\n",
    "\n",
    "    # Extract prediction (remove after <END>)\n",
    "    match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "    if match_pred:\n",
    "        content_pred = match_pred.group(1)\n",
    "\n",
    "    # Extract ground truth (between <START> and <END>)\n",
    "    match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "    if match_ground_true:\n",
    "        content_ground_true = match_ground_true.group(1)\n",
    "\n",
    "    # Write to log file\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(f\"Epoch {epoch}\\n\")\n",
    "        file.write(\"===========================\\n\")\n",
    "        file.write(f\"pred: {content_pred}\\n\")\n",
    "        file.write(f\"true: {content_ground_true}\\n\")\n",
    "        file.write(\"===========================\\n\")\n",
    "\n",
    "    # Ensure non-empty values for WER\n",
    "    content_pred = content_pred.strip() or \"\"\n",
    "    content_ground_true = content_ground_true.strip() or \"\"\n",
    "\n",
    "    wer_score = jiwer.wer(content_ground_true, content_pred)\n",
    "    return wer_score\n",
    "\n",
    "\n",
    "\n",
    "def calculate_cer(gt, pred):\n",
    "    content_pred = \"\"\n",
    "    content_ground_true = \"\"\n",
    "\n",
    "    match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "    if match_pred:\n",
    "        content_pred = match_pred.group(1)\n",
    "\n",
    "    match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "    if match_ground_true:\n",
    "        content_ground_true = match_ground_true.group(1)\n",
    "\n",
    "    content_pred = content_pred.strip() or \"\"\n",
    "    content_ground_true = content_ground_true.strip() or \"\"\n",
    "\n",
    "    return jiwer.cer(content_ground_true, content_pred)\n",
    "\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, num_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "            \n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "                \n",
    "                wer = calculate_wer(gt_caption, pred_caption, epoch)\n",
    "                cer = calculate_cer(gt_caption, pred_caption)\n",
    "                total_wer += wer\n",
    "                total_cer += cer\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    print(f\"Average WER: {avg_wer:.2f}, Average CER: {avg_cer:.2f}\")\n",
    "    return avg_wer, avg_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e0f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.7655\n",
      "Average WER: 0.88, Average CER: 0.32\n",
      "Epoch 2: Train Loss: 0.4657\n",
      "Average WER: 0.66, Average CER: 0.18\n",
      "Epoch 3: Train Loss: 0.3439\n",
      "Average WER: 0.50, Average CER: 0.13\n",
      "Epoch 4: Train Loss: 0.2834\n",
      "Average WER: 0.50, Average CER: 0.13\n",
      "Epoch 5: Train Loss: 0.2044\n",
      "Average WER: 0.43, Average CER: 0.13\n",
      "Epoch 6: Train Loss: 0.2094\n",
      "Average WER: 0.38, Average CER: 0.10\n",
      "Epoch 7: Train Loss: 0.1578\n",
      "Average WER: 0.36, Average CER: 0.09\n",
      "Epoch 8: Train Loss: 0.1738\n",
      "Average WER: 0.43, Average CER: 0.12\n",
      "Epoch 9: Train Loss: 0.1470\n",
      "Average WER: 0.31, Average CER: 0.09\n",
      "Epoch 10: Train Loss: 0.1461\n",
      "Average WER: 0.32, Average CER: 0.09\n",
      "Epoch 11: Train Loss: 0.1327\n",
      "Average WER: 0.31, Average CER: 0.08\n",
      "Epoch 12: Train Loss: 0.1176\n",
      "Average WER: 0.26, Average CER: 0.07\n",
      "Epoch 13: Train Loss: 0.1074\n",
      "Average WER: 0.30, Average CER: 0.07\n",
      "Epoch 14: Train Loss: 0.1060\n",
      "Average WER: 0.40, Average CER: 0.12\n",
      "Epoch 15: Train Loss: 0.1088\n",
      "Average WER: 0.29, Average CER: 0.08\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "\n",
    "train_losses, wer_scores, cer_scores = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        features = encoder(images)                           # (B, 1, embed_size)\n",
    "        outputs = decoder(features, captions[:, :-1])  # ✅ just 2 arguments: (image_features, input_captions)\n",
    "       # (B, T, vocab_size)\n",
    "        loss = criterion(outputs.reshape(-1, len(word2idx)), captions[:, 1:].reshape(-1))\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_wer, avg_cer = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "    wer_scores.append(avg_wer)\n",
    "    cer_scores.append(avg_cer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43650c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: សេះ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode image: (1, 1, embed_size)\n",
    "    encoder_out = encoder(image)\n",
    "\n",
    "    input_indices = [word2idx['<START>']]\n",
    "    for _ in range(max_length):\n",
    "        tgt = torch.tensor([input_indices], dtype=torch.long).to(device)  # shape: (1, T)\n",
    "        tgt_embed = decoder.embedding(tgt)\n",
    "        tgt_embed = decoder.pos_encoding(tgt_embed)  # (1, T, E)\n",
    "        tgt_embed = tgt_embed.permute(1, 0, 2)        # (T, 1, E)\n",
    "        memory = encoder_out.permute(1, 0, 2)         # (1, 1, E)\n",
    "        tgt_mask = decoder.generate_square_subsequent_mask(tgt_embed.size(0)).to(device)\n",
    "\n",
    "        output = decoder.transformer_decoder(tgt_embed, memory, tgt_mask=tgt_mask)\n",
    "        output = decoder.fc_out(output[-1])  # (1, vocab_size) → last time step\n",
    "\n",
    "        predicted_index = output.argmax(-1).item()\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "\n",
    "        input_indices.append(predicted_index)\n",
    "\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in input_indices[1:]]  # skip <START>\n",
    "    predicted_caption = ''.join(predicted_tokens)\n",
    "    return predicted_caption\n",
    "\n",
    "\n",
    "image_path = '/home/vitoupro/code/image_captioning/data/image.png'\n",
    "caption = predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx)\n",
    "print(\"Predicted Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871ea54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved best model!\n"
     ]
    }
   ],
   "source": [
    "if avg_wer < best_wer:\n",
    "    best_wer = avg_wer\n",
    "    torch.save(encoder.state_dict(), \"encodertransf.pth\")\n",
    "    torch.save(decoder.state_dict(), \"decodertransf.pth\")\n",
    "    print(\"✅ Saved best model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41336d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'captioning_model_transf.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd542a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
