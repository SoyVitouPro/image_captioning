{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef27f349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word_wordleveltest.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding a list of words (word-level)\n",
    "def encode_khmer_sentence(sentence, word2idx):\n",
    "    words = sentence.strip().split()  # 🛠 Split into words first\n",
    "    indices = []\n",
    "    for word in words:\n",
    "        index = word2idx.get(word)\n",
    "        if index is None:\n",
    "            return None, f\"Word '{word}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "\n",
    "# Decoding a list of indices (word-level)\n",
    "def decode_indices(indices, idx2word):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        word = idx2word.get(str(index))\n",
    "        if word is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        words.append(word)\n",
    "    return ' '.join(words), None  # 🛠 Join decoded words with a space\n",
    "\n",
    "\n",
    "# Attention Module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if 'layer4' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # output shape: (B, 2048, 1, 1)\n",
    "        self.linear = nn.Linear(2048, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)             # (B, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # (B, 2048) — safe reshape\n",
    "        features = self.linear(features)                # (B, embed_size)\n",
    "        return features.unsqueeze(1)                    # (B, 1, embed_size)\n",
    "           # (B, 1, embed_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads=8, num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = self.embedding(captions)  # (B, T, E)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "\n",
    "        tgt = tgt.permute(1, 0, 2)      # (T, B, E)\n",
    "        memory = features.permute(1, 0, 2)  # (1, B, E)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)  # (T, B, vocab_size)\n",
    "        return out.permute(1, 0, 2)  # (B, T, vocab_size)\n",
    "\n",
    "\n",
    "# The rest of the code remains the same...\n",
    "# You can now instantiate DecoderRNN with attention in your training setup.\n",
    "# Image Captioning Dataset\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "    # Split caption into words first\n",
    "        indices, error = encode_khmer_sentence(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "\n",
    "# # Define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "# ]) \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/notebook/dataset_images'\n",
    "# Load dataset manually (correct split)\n",
    "img_dir = '/home/vitoupro/code/image_captioning/notebook/dataset_images'\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabeltest.txt'\n",
    "\n",
    "image_names = []\n",
    "captions = []\n",
    "\n",
    "with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)  # Split once only\n",
    "        if len(parts) == 2:\n",
    "            image_name, caption = parts\n",
    "            image_names.append(image_name)\n",
    "            captions.append(caption)\n",
    "\n",
    "all_images = pd.DataFrame({'image': image_names, 'caption': captions})\n",
    "\n",
    "# Split dataset\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': train_images, 'caption': train_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "eval_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': eval_images, 'caption': eval_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Unified embed size\n",
    "# embed_size = 256\n",
    "embed_size = 512\n",
    "\n",
    "encoder = EncoderCNN(embed_size=embed_size).to(device)\n",
    "decoder = TransformerDecoder(\n",
    "    vocab_size=len(word2idx),\n",
    "    embed_size=embed_size,\n",
    "    num_heads=8,\n",
    "    num_layers=3\n",
    ").to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())  # include all trainable encoder parts\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "def custom_transform(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Return as list of words\n",
    "    return text.split()\n",
    "    \n",
    "\n",
    "def calculate_wer(gt, pred, epoch, file_path='metric.txt'):\n",
    "    match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "    match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "\n",
    "    content_pred = match_pred.group(1).strip() if match_pred else \"\"\n",
    "    content_ground_true = match_ground_true.group(1).strip() if match_ground_true else \"\"\n",
    "\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(f\"Epoch {epoch}\\n\")\n",
    "        file.write(\"===========================\\n\")\n",
    "        file.write(f\"pred: {content_pred}\\n\")\n",
    "        file.write(f\"true: {content_ground_true}\\n\")\n",
    "        file.write(\"===========================\\n\")\n",
    "\n",
    "    # Avoid jiwer crash by ensuring non-empty reference\n",
    "    if not content_ground_true:\n",
    "        return 1.0  # Max error if reference is missing\n",
    "    return jiwer.wer(content_ground_true, content_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_cer(gt, pred):\n",
    "    match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "    match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "\n",
    "    content_pred = match_pred.group(1).strip() if match_pred else \"\"\n",
    "    content_ground_true = match_ground_true.group(1).strip() if match_ground_true else \"\"\n",
    "\n",
    "    # New: if either is empty, just SKIP and return None\n",
    "    if not content_pred or not content_ground_true:\n",
    "        return None\n",
    "\n",
    "    return jiwer.cer(content_ground_true, content_pred)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, num_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "            \n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "    \n",
    "                wer = calculate_wer(gt_caption, pred_caption, epoch)\n",
    "                cer = calculate_cer(gt_caption, pred_caption)\n",
    "    \n",
    "                if wer is None or cer is None:\n",
    "                    continue  # Skip if WER or CER couldn't be computed\n",
    "    \n",
    "                total_wer += wer\n",
    "                total_cer += cer\n",
    "                num_samples += 1\n",
    "\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    print(f\"Average WER: {avg_wer:.2f}, Average CER: {avg_cer:.2f}\")\n",
    "    return avg_wer, avg_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46cb302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 6.3132\n",
      "Average WER: 0.00, Average CER: 0.00\n",
      "Epoch 2: Train Loss: 5.8406\n",
      "Average WER: 0.00, Average CER: 0.00\n",
      "Epoch 3: Train Loss: 5.5356\n",
      "Average WER: 0.84, Average CER: 0.71\n",
      "Epoch 4: Train Loss: 5.1335\n",
      "Average WER: 0.80, Average CER: 0.68\n",
      "Epoch 5: Train Loss: 4.9188\n",
      "Average WER: 0.79, Average CER: 0.68\n",
      "Epoch 6: Train Loss: 4.8737\n",
      "Average WER: 0.80, Average CER: 0.70\n",
      "Epoch 7: Train Loss: 4.6214\n",
      "Average WER: 0.78, Average CER: 0.67\n",
      "Epoch 8: Train Loss: 3.8202\n",
      "Average WER: 0.76, Average CER: 0.66\n",
      "Epoch 9: Train Loss: 3.9226\n",
      "Average WER: 0.76, Average CER: 0.67\n",
      "Epoch 10: Train Loss: 3.7218\n",
      "Average WER: 0.78, Average CER: 0.68\n",
      "Epoch 11: Train Loss: 3.8881\n",
      "Average WER: 0.81, Average CER: 0.71\n",
      "Epoch 12: Train Loss: 2.7112\n",
      "Average WER: 0.78, Average CER: 0.68\n",
      "Epoch 13: Train Loss: 4.4994\n",
      "Average WER: 0.81, Average CER: 0.71\n",
      "Epoch 14: Train Loss: 4.2744\n",
      "Average WER: 0.78, Average CER: 0.70\n",
      "Epoch 15: Train Loss: 3.9202\n",
      "Average WER: 0.80, Average CER: 0.70\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "teacher_forcing_ratio = 0.9  # Start with 90% teacher forcing\n",
    "\n",
    "train_losses, wer_scores, cer_scores = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        \n",
    "        input_tokens = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        if torch.rand(1).item() > teacher_forcing_ratio:\n",
    "            # Scheduled sampling: use own prediction as input\n",
    "            outputs = []\n",
    "            batch_size = images.size(0)\n",
    "            inputs = torch.full((batch_size, 1), word2idx['<START>'], dtype=torch.long, device=device)\n",
    "\n",
    "            for _ in range(input_tokens.size(1)):\n",
    "                output = decoder(features, inputs)\n",
    "                last_output = output[:, -1, :]  # last timestep\n",
    "                predicted = last_output.argmax(-1).unsqueeze(1)\n",
    "                inputs = torch.cat((inputs, predicted), dim=1)\n",
    "                outputs.append(last_output)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1).squeeze(2)  # (B, T, vocab)\n",
    "        else:\n",
    "            # Teacher forcing: normal\n",
    "            outputs = decoder(features, input_tokens)\n",
    "\n",
    "        batch_size, seq_len, vocab_size = outputs.size()\n",
    "        loss = criterion(\n",
    "            outputs.reshape(batch_size * seq_len, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_wer, avg_cer = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "    wer_scores.append(avg_wer)\n",
    "    cer_scores.append(avg_cer)\n",
    "\n",
    "    # Reduce teacher forcing slowly\n",
    "    teacher_forcing_ratio = max(0.5, teacher_forcing_ratio * 0.95)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56bb28b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ឆ្កែខ្មៅនិងកំពុងរត់គ្នាដោយគ្នានៅក្នុងវាលស្មៅដែលពោរពេញទៅដោយ។។មួយ។។\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_caption_beam_search(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20, beam_size=3):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode\n",
    "    encoder_out = encoder(image)  # (1, 1, embed)\n",
    "\n",
    "    sequences = [[list([word2idx['<START>']]), 0.0]]  # (sequence, score)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            tgt = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "            outputs = decoder(encoder_out, tgt)\n",
    "            outputs = outputs[:, -1, :]  # last step\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "            topk_probs, topk_indices = probs.topk(beam_size)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                candidate = seq + [topk_indices[0][i].item()]\n",
    "                candidate_score = score - torch.log(topk_probs[0][i]).item()\n",
    "                all_candidates.append((candidate, candidate_score))\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:beam_size]\n",
    "\n",
    "    best_seq = sequences[0][0]\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in best_seq[1:] if idx != word2idx['<END>']]  # skip <START>\n",
    "    predicted_caption = ''.join(predicted_tokens)\n",
    "    return predicted_caption\n",
    "  \n",
    "  \n",
    "test_image_path = '/home/vitoupro/code/image_captioning/data/image.png'  # your image\n",
    "\n",
    "predicted_caption = predict_caption_beam_search(\n",
    "    image_path=test_image_path,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    idx2word=idx2word,\n",
    "    word2idx=word2idx,\n",
    "    max_length=20,\n",
    "    beam_size=3  # you can test beam_size=3 or 5\n",
    ")\n",
    "\n",
    "print(\"Predicted Caption:\", predicted_caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586944c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
