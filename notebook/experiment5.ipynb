{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e46392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Image Captioning with Attention-Enhanced LSTM\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding and decoding functions\n",
    "def encode_khmer_word(word, word2idx):\n",
    "    indices = []\n",
    "    for character in word:\n",
    "        index = word2idx.get(character)\n",
    "        if index is None:\n",
    "            return None, f\"Character '{character}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "def decode_indices(indices, idx2word):\n",
    "    characters = []\n",
    "    for index in indices:\n",
    "        character = idx2word.get(str(index))\n",
    "        if character is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        characters.append(character)\n",
    "    return ''.join(characters), None\n",
    "\n",
    "# Attention Module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "# EncoderCNN with spatial features\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if 'layer4' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((14, 14))\n",
    "        self.embed = nn.Linear(2048, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = self.avgpool(features)\n",
    "        features = features.view(features.size(0), 2048, -1).permute(0, 2, 1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# Decoder with Attention\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, attention_dim=256, num_layers=1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(embed_size, hidden_size, attention_dim)\n",
    "        self.lstm = nn.LSTM(embed_size + embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, encoder_out, captions, sampling_probability=1.0):\n",
    "        batch_size, seq_len = captions.size()\n",
    "        embedded = self.embedding(captions)\n",
    "        h = self.init_h(encoder_out.mean(1)).unsqueeze(0)\n",
    "        c = self.init_c(encoder_out.mean(1)).unsqueeze(0)\n",
    "\n",
    "        inputs = embedded[:, 0, :].unsqueeze(1)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            context, _ = self.attention(encoder_out, h[-1])\n",
    "            lstm_input = torch.cat((inputs.squeeze(1), context), dim=1).unsqueeze(1)\n",
    "            output, (h, c) = self.lstm(lstm_input, (h, c))\n",
    "            output = self.linear(output.squeeze(1))\n",
    "            outputs.append(output)\n",
    "\n",
    "            teacher_force = torch.rand(1).item() > sampling_probability\n",
    "            top1 = output.argmax(1)\n",
    "            inputs = embedded[:, t, :].unsqueeze(1) if teacher_force else self.embedding(top1).unsqueeze(1)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# The rest of the code remains the same...\n",
    "# You can now instantiate DecoderRNN with attention in your training setup.\n",
    "# Image Captioning Dataset\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_word(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "# # Define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "# ]) \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/raw/annotation.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/data/raw/animals'\n",
    "all_images = pd.read_csv(annotations_file, delimiter=' ', names=['image', 'caption'])\n",
    "\n",
    "# Split dataset\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': train_images, 'caption': train_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "eval_dataset = ImageCaptionDataset(\n",
    "    img_labels=pd.DataFrame({'image': eval_images, 'caption': eval_captions}),\n",
    "    img_dir=img_dir,\n",
    "    vocab=word2idx,\n",
    "    transform=transform,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Unified embed size\n",
    "embed_size = 256\n",
    "\n",
    "encoder = EncoderCNN(embed_size=embed_size).to(device)\n",
    "decoder = DecoderWithAttention(\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=512,\n",
    "    vocab_size=len(word2idx),\n",
    "    num_layers=1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "def custom_transform(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Return as list of words\n",
    "    return text.split()\n",
    "    \n",
    "\n",
    "def calculate_wer(gt, pred, epoch, file_path='metric.txt'):\n",
    "    # Default empty strings to avoid UnboundLocalError\n",
    "    content_pred = \"\"\n",
    "    content_ground_true = \"\"\n",
    "\n",
    "    # Extract prediction (remove after <END>)\n",
    "    match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "    if match_pred:\n",
    "        content_pred = match_pred.group(1)\n",
    "\n",
    "    # Extract ground truth (between <START> and <END>)\n",
    "    match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "    if match_ground_true:\n",
    "        content_ground_true = match_ground_true.group(1)\n",
    "\n",
    "    # Write to log file\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(f\"Epoch {epoch}\\n\")\n",
    "        file.write(\"===========================\\n\")\n",
    "        file.write(f\"pred: {content_pred}\\n\")\n",
    "        file.write(f\"true: {content_ground_true}\\n\")\n",
    "        file.write(\"===========================\\n\")\n",
    "\n",
    "    # Ensure non-empty values for WER\n",
    "    content_pred = content_pred.strip() or \"\"\n",
    "    content_ground_true = content_ground_true.strip() or \"\"\n",
    "\n",
    "    wer_score = jiwer.wer(content_ground_true, content_pred)\n",
    "    return wer_score\n",
    "\n",
    "\n",
    "\n",
    "def calculate_cer(gt, pred):\n",
    "    content_pred = \"\"\n",
    "    content_ground_true = \"\"\n",
    "\n",
    "    match_pred = re.search(r\"^(.*?)<END>\", pred)\n",
    "    if match_pred:\n",
    "        content_pred = match_pred.group(1)\n",
    "\n",
    "    match_ground_true = re.search(r\"<START>(.*?)<END>\", gt)\n",
    "    if match_ground_true:\n",
    "        content_ground_true = match_ground_true.group(1)\n",
    "\n",
    "    content_pred = content_pred.strip() or \"\"\n",
    "    content_ground_true = content_ground_true.strip() or \"\"\n",
    "\n",
    "    return jiwer.cer(content_ground_true, content_pred)\n",
    "\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, num_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "            \n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "                \n",
    "                wer = calculate_wer(gt_caption, pred_caption, epoch)\n",
    "                cer = calculate_cer(gt_caption, pred_caption)\n",
    "                total_wer += wer\n",
    "                total_cer += cer\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    print(f\"Average WER: {avg_wer:.2f}, Average CER: {avg_cer:.2f}\")\n",
    "    return avg_wer, avg_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273f7a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.9380, Sampling Prob: 1.00\n",
      "Average WER: 1.00, Average CER: 0.87\n",
      "Epoch 2: Train Loss: 2.4834, Sampling Prob: 0.95\n",
      "Average WER: 1.00, Average CER: 0.77\n",
      "Epoch 3: Train Loss: 1.9386, Sampling Prob: 0.90\n",
      "Average WER: 0.86, Average CER: 0.51\n",
      "Epoch 4: Train Loss: 1.1467, Sampling Prob: 0.85\n",
      "Average WER: 0.45, Average CER: 0.38\n",
      "Epoch 5: Train Loss: 0.7784, Sampling Prob: 0.80\n",
      "Average WER: 0.29, Average CER: 0.31\n",
      "Epoch 6: Train Loss: 0.5544, Sampling Prob: 0.75\n",
      "Average WER: 0.22, Average CER: 0.26\n",
      "Epoch 7: Train Loss: 0.4697, Sampling Prob: 0.70\n",
      "Average WER: 0.26, Average CER: 0.32\n",
      "Epoch 8: Train Loss: 0.3930, Sampling Prob: 0.65\n",
      "Average WER: 0.27, Average CER: 0.28\n",
      "Epoch 9: Train Loss: 0.3468, Sampling Prob: 0.60\n",
      "Average WER: 0.24, Average CER: 0.28\n",
      "Epoch 10: Train Loss: 0.3358, Sampling Prob: 0.55\n",
      "Average WER: 0.27, Average CER: 0.30\n",
      "Epoch 11: Train Loss: 0.3185, Sampling Prob: 0.50\n",
      "Average WER: 0.21, Average CER: 0.25\n",
      "Epoch 12: Train Loss: 0.2318, Sampling Prob: 0.45\n",
      "Average WER: 0.26, Average CER: 0.29\n",
      "Epoch 13: Train Loss: 0.2005, Sampling Prob: 0.40\n",
      "Average WER: 0.22, Average CER: 0.32\n",
      "Epoch 14: Train Loss: 0.2106, Sampling Prob: 0.35\n",
      "Average WER: 0.22, Average CER: 0.34\n",
      "Epoch 15: Train Loss: 0.1460, Sampling Prob: 0.30\n",
      "Average WER: 0.20, Average CER: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with attention\n",
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "\n",
    "train_losses, wer_scores, cer_scores = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    sampling_prob = max(0.1, 1.0 - epoch * 0.05)\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, sampling_probability=sampling_prob)\n",
    "\n",
    "        loss = criterion(outputs.view(-1, len(word2idx)), captions[:, 1:].reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {avg_loss:.4f}, Sampling Prob: {sampling_prob:.2f}')\n",
    "\n",
    "    avg_wer, avg_cer = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "    wer_scores.append(avg_wer)\n",
    "    cer_scores.append(avg_cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11e47a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ហាមស្ទ័រ\n"
     ]
    }
   ],
   "source": [
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "\n",
    "    # Extract spatial features from encoder: (1, 196, embed_size)\n",
    "    encoder_out = encoder(image)\n",
    "\n",
    "    # Initialize LSTM hidden and cell state from mean-pooled encoder output\n",
    "    h = decoder.init_h(encoder_out.mean(1)).unsqueeze(0)\n",
    "    c = decoder.init_c(encoder_out.mean(1)).unsqueeze(0)\n",
    "\n",
    "    input_idx = torch.tensor([word2idx['<START>']], dtype=torch.long).to(device)\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        embedded = decoder.embedding(input_idx).unsqueeze(1)  # (1, 1, embed_size)\n",
    "\n",
    "        # Apply attention to get context vector\n",
    "        context, _ = decoder.attention(encoder_out, h[-1])  # context: (1, embed_size)\n",
    "\n",
    "        # Concatenate embedded word and context\n",
    "        lstm_input = torch.cat((embedded.squeeze(1), context), dim=1).unsqueeze(1)  # (1, 1, 2*embed_size)\n",
    "\n",
    "        # LSTM forward\n",
    "        output, (h, c) = decoder.lstm(lstm_input, (h, c))\n",
    "        output = decoder.linear(output.squeeze(1))  # (1, vocab_size)\n",
    "\n",
    "        # Predict next token\n",
    "        predicted_index = output.argmax(-1).item()\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "\n",
    "        predictions.append(idx2word[str(predicted_index)])\n",
    "        input_idx = torch.tensor([predicted_index], dtype=torch.long).to(device)\n",
    "\n",
    "    predicted_caption = ''.join(predictions)  # Khmer: character-based\n",
    "    return predicted_caption\n",
    "\n",
    "image_path = '/home/vitoupro/code/image_captioning/data/00000001_020.jpg'\n",
    "caption = predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx)\n",
    "print(\"Predicted Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c4c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "if avg_wer < best_wer:\n",
    "    best_wer = avg_wer\n",
    "    torch.save(encoder.state_dict(), \"encoderattdec.pth\")\n",
    "    torch.save(decoder.state_dict(), \"decoderattdec.pth\")\n",
    "    print(\"✅ Saved best model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdefebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'captioning_model_attdec.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06c3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
