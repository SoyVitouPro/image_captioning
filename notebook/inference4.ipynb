{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366f76b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ដំរី<END>\n"
     ]
    }
   ],
   "source": [
    "# inference4.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Model Definitions (ensure these definitions are exactly as they were during training)\n",
    "# Model Definitions (EncoderCNN and DecoderRNN)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # for param in resnet.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if 'layer4' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(hidden_size, hidden_size)  # Initialize LSTM hidden state\n",
    "        self.init_c = nn.Linear(hidden_size, hidden_size)  # Initialize LSTM cell state\n",
    "\n",
    "    def forward(self, features, captions, sampling_probability=1.0):\n",
    "        batch_size, seq_len = captions.size()\n",
    "        embeddings = self.embed(captions)\n",
    "    \n",
    "        h = self.init_h(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c = self.init_c(features).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "    \n",
    "        inputs = embeddings[:, 0].unsqueeze(1)  # Embed <START>\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            lstm_out, (h, c) = self.lstm(inputs, (h, c))\n",
    "            output = self.linear(lstm_out.squeeze(1))\n",
    "            outputs.append(output)\n",
    "\n",
    "        # Decide whether to use teacher forcing or model prediction\n",
    "            teacher_force = torch.rand(1).item() > sampling_probability\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            next_input = captions[:, t] if teacher_force else top1\n",
    "            inputs = self.embed(next_input).unsqueeze(1)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "    \n",
    "\n",
    "# Load vocabulary\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size=512).to(device)\n",
    "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(word2idx), num_layers=1).to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "encoder.load_state_dict(torch.load('/home/vitoupro/code/image_captioning/notebook/encoderssp.pth'))\n",
    "decoder.load_state_dict(torch.load('/home/vitoupro/code/image_captioning/notebook/decoderssp.pth'))\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Prediction function\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Load and transform the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and transfer to device\n",
    "    \n",
    "    # Generate features from the image using the encoder\n",
    "    features = encoder(image)\n",
    "    \n",
    "    # Start the sequence with the <START> token\n",
    "    predicted_indices = [word2idx['<START>']]\n",
    "    predictions = []\n",
    "    \n",
    "    # Initial input to the LSTM is the <START> token\n",
    "    input_idx = torch.tensor([predicted_indices[-1]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Initialize the LSTM state\n",
    "    h, c = None, None\n",
    "    \n",
    "    # Generate words until the <END> token is predicted or the max length is reached\n",
    "    for _ in range(20):  # Assuming max length of 20 for safety\n",
    "        input_idx = input_idx.unsqueeze(0)  # Add batch dimension for single time-step prediction\n",
    "        if h is None and c is None:\n",
    "            # Generate initial hidden states from features\n",
    "            h = decoder.init_h(features).unsqueeze(0).repeat(decoder.num_layers, 1, 1)\n",
    "            c = decoder.init_c(features).unsqueeze(0).repeat(decoder.num_layers, 1, 1)\n",
    "        \n",
    "        outputs, (h, c) = decoder.lstm(decoder.embed(input_idx), (h, c))\n",
    "        outputs = decoder.linear(outputs.squeeze(1))\n",
    "        \n",
    "        # Get the predicted word index\n",
    "        predicted_index = outputs.argmax(-1).item()\n",
    "        predicted_indices.append(predicted_index)\n",
    "        predictions.append(idx2word[str(predicted_index)])  # Decode to word\n",
    "        \n",
    "        # Prepare the next input\n",
    "        input_idx = torch.tensor([predicted_index], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Stop if the <END> token is predicted\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "    \n",
    "    predicted_caption = ' '.join(predictions)  # Join the predicted words\n",
    "    \n",
    "    return predicted_caption\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/vitoupro/code/image_captioning/data/2.png'\n",
    "predicted_caption = predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx)\n",
    "print(\"Predicted Caption:\", predicted_caption.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3f8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
