{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ce051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5493333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word-segmented captions saved to /home/vitoupro/code/image_captioning/data/processed/blip_captions_khmer_sorted_segment.txt\n",
      "‚úÖ Total lines processed: 4724\n"
     ]
    }
   ],
   "source": [
    "from khmercut import tokenize\n",
    "import os\n",
    "\n",
    "# === Step 1: Paths\n",
    "captions_file_path = '/home/vitoupro/code/image_captioning/notebook/blip_captions_khmer_sorted.txt'  # raw label\n",
    "output_label_path = '/home/vitoupro/code/image_captioning/data/processed/blip_captions_khmer_sorted_segment.txt'  # ‚¨ÖÔ∏è NEW segmented label\n",
    "\n",
    "# === Step 2: Load raw captions\n",
    "with open(captions_file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_captions = f.readlines()\n",
    "\n",
    "captions = [line.strip() for line in raw_captions if line.strip()]\n",
    "\n",
    "# === Step 3: Segment each caption using khmercut.tokenize\n",
    "segmented_lines = []\n",
    "for line in captions:\n",
    "    parts = line.strip().split(' ', 1)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed line: {line}\")\n",
    "        continue\n",
    "\n",
    "    image_name, caption = parts\n",
    "    segmented_words = tokenize(caption)  # ‚úÖ Use khmercut's tokenizer here\n",
    "    segmented_caption = ' '.join(segmented_words)\n",
    "    segmented_lines.append(f\"{image_name} {segmented_caption}\")\n",
    "\n",
    "# === Step 4: Save segmented result\n",
    "os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
    "\n",
    "with open(output_label_path, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Word-segmented captions saved to {output_label_path}\")\n",
    "print(f\"‚úÖ Total lines processed: {len(segmented_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a9d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocabulary saved to:\n",
      "‚Üí word2idx_blip_caption_khmer_sorted.json\n",
      "‚Üí idx2word_blip_caption_khmer_sorted.json\n",
      "üü© Total unique words (excluding special tokens): 1718\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === Step 1: Load segmented caption file\n",
    "segmented_file_path = '/home/vitoupro/code/image_captioning/data/processed/blip_captions_khmer_sorted_segment.txt'\n",
    "\n",
    "captions = []\n",
    "with open(segmented_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# === Step 2: Tokenize into word list\n",
    "tokenized_captions = [caption.split() for caption in captions]\n",
    "all_words = [word for caption in tokenized_captions for word in caption]\n",
    "\n",
    "# === Step 3: Create vocab with special tokens\n",
    "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
    "vocab_words = special_tokens + sorted(set(all_words))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# === Step 4: Save vocab files\n",
    "save_dir = '/home/vitoupro/code/image_captioning/data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'word2idx_blip_caption_khmer_sorted.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(word2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(save_dir, 'idx2word_blip_caption_khmer_sorted.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2word, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Vocabulary saved to:\")\n",
    "print(\"‚Üí word2idx_blip_caption_khmer_sorted.json\")\n",
    "print(\"‚Üí idx2word_blip_caption_khmer_sorted.json\")\n",
    "print(f\"üü© Total unique words (excluding special tokens): {len(vocab_words) - len(special_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b6fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from timm import create_model\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word_blip_caption_khmer_sorted.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding a list of words (word-level)\n",
    "def encode_khmer_sentence(sentence, word2idx):\n",
    "    words = sentence.strip().split()\n",
    "    indices = []\n",
    "    for word in words:\n",
    "        index = word2idx.get(word)\n",
    "        if index is None:\n",
    "            return None, f\"Word '{word}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "# Decoding a list of indices (word-level)\n",
    "def decode_indices(indices, idx2word):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        word = idx2word.get(str(index))\n",
    "        if word is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        words.append(word)\n",
    "    return ' '.join(words), None\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "\n",
    "# Data-efficient Image Transformer (DeiT) Encoder\n",
    "class DeiTEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(DeiTEncoder, self).__init__()\n",
    "        import timm\n",
    "        self.deit = timm.create_model('deit_tiny_patch16_224', pretrained=True)\n",
    "        self.deit.head = nn.Identity()  # Remove classification head\n",
    "        self.linear = nn.Linear(self.deit.num_features, embed_size)  # Project to embed_size\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.deit.forward_features(images)  # (B, seq_len, 192)\n",
    "        cls_token = features[:, 0, :]  # Take only [CLS] token: (B, 192)\n",
    "        out = self.linear(cls_token)  # Project to (B, embed_size)\n",
    "        return out.unsqueeze(1)  # (B, 1, embed_size)\n",
    "       \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads=8, num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = self.embedding(captions)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        memory = features.permute(1, 0, 2)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out.permute(1, 0, 2)\n",
    "\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_sentence(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/processed/blip_captions_khmer_sorted_segment.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/notebook/downloaded_images'\n",
    "image_names, captions = [], []\n",
    "\n",
    "with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            image_name, caption = parts\n",
    "            image_names.append(image_name)\n",
    "            captions.append(caption)\n",
    "\n",
    "all_images = pd.DataFrame({'image': image_names, 'caption': captions})\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(pd.DataFrame({'image': train_images, 'caption': train_captions}), img_dir, word2idx, transform, 20)\n",
    "eval_dataset = ImageCaptionDataset(pd.DataFrame({'image': eval_images, 'caption': eval_captions}), img_dir, word2idx, transform, 20)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embed_size = 256\n",
    "encoder = DeiTEncoder(embed_size=embed_size).to(device)\n",
    "decoder = TransformerDecoder(len(word2idx), embed_size, num_heads=8, num_layers=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "smoothing = SmoothingFunction().method1\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, total_bleu1, total_bleu2, total_bleu4, total_rougeL = 0, 0, 0, 0, 0, 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "\n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "\n",
    "                ref_match = re.search(r\"<START>(.*?)<END>\", gt_caption)\n",
    "                pred_match = re.search(r\"^(.*?)<END>\", pred_caption)\n",
    "                reference = ref_match.group(1).strip() if ref_match else \"\"\n",
    "                prediction = pred_match.group(1).strip() if pred_match else \"\"\n",
    "                if not reference or not prediction:\n",
    "                    continue\n",
    "\n",
    "                ref_tokens = reference.split()\n",
    "                pred_tokens = prediction.split()\n",
    "\n",
    "                total_wer += jiwer.wer(reference, prediction)\n",
    "                total_cer += jiwer.cer(reference, prediction)\n",
    "                total_bleu1 += sentence_bleu([ref_tokens], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu2 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu4 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "                total_rougeL += rouge.score(reference, prediction)['rougeL'].fmeasure\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu1 = total_bleu1 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu2 = total_bleu2 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu4 = total_bleu4 / num_samples if num_samples > 0 else 0\n",
    "    avg_rougeL = total_rougeL / num_samples if num_samples > 0 else 0\n",
    "\n",
    "    print(f\"WER: {avg_wer:.2f}, CER: {avg_cer:.2f}, BLEU-1: {avg_bleu1:.2f}, BLEU-2: {avg_bleu2:.2f}, BLEU-4: {avg_bleu4:.2f}, ROUGE-L: {avg_rougeL:.2f}\")\n",
    "    return avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc9e4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3.9495\n",
      "WER: 0.72, CER: 0.64, BLEU-1: 0.29, BLEU-2: 0.18, BLEU-4: 0.08, ROUGE-L: 0.00\n",
      "Epoch 2: Train Loss: 3.3265\n",
      "WER: 0.68, CER: 0.60, BLEU-1: 0.33, BLEU-2: 0.20, BLEU-4: 0.11, ROUGE-L: 0.00\n",
      "Epoch 3: Train Loss: 3.2123\n",
      "WER: 0.68, CER: 0.60, BLEU-1: 0.34, BLEU-2: 0.22, BLEU-4: 0.10, ROUGE-L: 0.01\n",
      "Epoch 4: Train Loss: 3.3382\n",
      "WER: 0.66, CER: 0.59, BLEU-1: 0.35, BLEU-2: 0.23, BLEU-4: 0.11, ROUGE-L: 0.01\n",
      "Epoch 5: Train Loss: 3.3967\n",
      "WER: 0.65, CER: 0.58, BLEU-1: 0.37, BLEU-2: 0.24, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 6: Train Loss: 3.6058\n",
      "WER: 0.68, CER: 0.60, BLEU-1: 0.34, BLEU-2: 0.21, BLEU-4: 0.11, ROUGE-L: 0.01\n",
      "Epoch 7: Train Loss: 3.5711\n",
      "WER: 0.64, CER: 0.57, BLEU-1: 0.37, BLEU-2: 0.25, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 8: Train Loss: 3.4291\n",
      "WER: 0.65, CER: 0.58, BLEU-1: 0.36, BLEU-2: 0.24, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 9: Train Loss: 3.6365\n",
      "WER: 0.65, CER: 0.59, BLEU-1: 0.38, BLEU-2: 0.25, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 10: Train Loss: 3.7296\n",
      "WER: 0.65, CER: 0.58, BLEU-1: 0.39, BLEU-2: 0.25, BLEU-4: 0.13, ROUGE-L: 0.01\n",
      "Epoch 11: Train Loss: 3.6375\n",
      "WER: 0.64, CER: 0.57, BLEU-1: 0.38, BLEU-2: 0.25, BLEU-4: 0.13, ROUGE-L: 0.01\n",
      "Epoch 12: Train Loss: 3.9348\n",
      "WER: 0.65, CER: 0.57, BLEU-1: 0.37, BLEU-2: 0.24, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 13: Train Loss: 3.8597\n",
      "WER: 0.66, CER: 0.59, BLEU-1: 0.37, BLEU-2: 0.25, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 14: Train Loss: 3.8129\n",
      "WER: 0.64, CER: 0.56, BLEU-1: 0.38, BLEU-2: 0.25, BLEU-4: 0.13, ROUGE-L: 0.01\n",
      "Epoch 15: Train Loss: 3.8060\n",
      "WER: 0.66, CER: 0.60, BLEU-1: 0.35, BLEU-2: 0.21, BLEU-4: 0.11, ROUGE-L: 0.01\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "teacher_forcing_ratio = 0.9  # Start with 90% teacher forcing\n",
    "\n",
    "train_losses, wer_scores, cer_scores = [], [], []\n",
    "bleu1_scores, bleu2_scores, bleu4_scores, rougeL_scores = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        \n",
    "        input_tokens = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        if torch.rand(1).item() > teacher_forcing_ratio:\n",
    "            # Scheduled sampling: use own prediction as input\n",
    "            outputs = []\n",
    "            batch_size = images.size(0)\n",
    "            inputs = torch.full((batch_size, 1), word2idx['<START>'], dtype=torch.long, device=device)\n",
    "\n",
    "            for _ in range(input_tokens.size(1)):\n",
    "                output = decoder(features, inputs)\n",
    "                last_output = output[:, -1, :]  # last timestep\n",
    "                predicted = last_output.argmax(-1).unsqueeze(1)\n",
    "                inputs = torch.cat((inputs, predicted), dim=1)\n",
    "                outputs.append(last_output)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1).squeeze(2)  # (B, T, vocab)\n",
    "        else:\n",
    "            # Teacher forcing: normal\n",
    "            outputs = decoder(features, input_tokens)\n",
    "\n",
    "        batch_size, seq_len, vocab_size = outputs.size()\n",
    "        loss = criterion(\n",
    "            outputs.reshape(batch_size * seq_len, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "\n",
    "    wer_scores.append(avg_wer)\n",
    "    cer_scores.append(avg_cer)\n",
    "\n",
    "    # Reduce teacher forcing slowly\n",
    "    teacher_forcing_ratio = max(0.3, teacher_forcing_ratio * 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93625493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing a prediction after training ===\n",
      "\n",
      "Predicted Caption: ·ûï·üí·ûõ·ûº·ûú ·ûò·üí·ûì·û∂·ûÄ·üã ·ûä·üÇ·ûõ ·ûò·û∂·ûì ·ûì·üÖ·ûõ·ûæ ·ûì·üÖ·ûõ·ûæ\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # shape (1, 3, H, W)\n",
    "\n",
    "    # Encode image\n",
    "    encoder_out = encoder(image)  # (1, 1, embed_size)\n",
    "\n",
    "    # Start generating\n",
    "    generated_indices = [word2idx['<START>']]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        input_tensor = torch.tensor([generated_indices], dtype=torch.long).to(device)  # (1, T)\n",
    "        \n",
    "        # Predict next token\n",
    "        output = decoder(encoder_out, input_tensor)  # (1, T, vocab_size)\n",
    "        next_token_logits = output[:, -1, :]          # (1, vocab_size)\n",
    "        predicted_index = next_token_logits.argmax(dim=-1).item()\n",
    "\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "        \n",
    "        generated_indices.append(predicted_index)\n",
    "\n",
    "    # Decode indices to words\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in generated_indices[1:]]  # Skip <START>\n",
    "\n",
    "    predicted_caption = ' '.join(predicted_tokens)  # üî• join words with space\n",
    "    return predicted_caption\n",
    "\n",
    "print(\"\\n=== Testing a prediction after training ===\\n\")\n",
    "\n",
    "test_image_path = '/home/vitoupro/code/image_captioning/data/image.png'  # your image\n",
    "\n",
    "predicted_caption = predict_caption(\n",
    "    image_path=test_image_path,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    idx2word=idx2word,\n",
    "    word2idx=word2idx,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "print(\"Predicted Caption:\", predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05da3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from timm import create_model\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word_blip_caption_khmer_sorted.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding a list of words (word-level)\n",
    "def encode_khmer_sentence(sentence, word2idx):\n",
    "    words = sentence.strip().split()\n",
    "    indices = []\n",
    "    for word in words:\n",
    "        index = word2idx.get(word)\n",
    "        if index is None:\n",
    "            return None, f\"Word '{word}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "# Decoding a list of indices (word-level)\n",
    "def decode_indices(indices, idx2word):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        word = idx2word.get(str(index))\n",
    "        if word is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        words.append(word)\n",
    "    return ' '.join(words), None\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "\n",
    "# Data-efficient Image Transformer (DeiT) Encoder\n",
    "class DeiTEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(DeiTEncoder, self).__init__()\n",
    "        import timm\n",
    "        self.deit = timm.create_model('deit_small_patch16_224', pretrained=True)\n",
    "        self.deit.head = nn.Identity()  # Remove classification head\n",
    "        self.linear = nn.Linear(self.deit.num_features, embed_size)  # Project to embed_size\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.deit.forward_features(images)  # (B, seq_len, 192)\n",
    "        cls_token = features[:, 0, :]  # Take only [CLS] token: (B, 192)\n",
    "        out = self.linear(cls_token)  # Project to (B, embed_size)\n",
    "        return out.unsqueeze(1)  # (B, 1, embed_size)\n",
    "       \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads=8, num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = self.embedding(captions)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        memory = features.permute(1, 0, 2)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out.permute(1, 0, 2)\n",
    "\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_sentence(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/processed/blip_captions_khmer_sorted_segment.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/notebook/downloaded_images'\n",
    "image_names, captions = [], []\n",
    "\n",
    "with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            image_name, caption = parts\n",
    "            image_names.append(image_name)\n",
    "            captions.append(caption)\n",
    "\n",
    "all_images = pd.DataFrame({'image': image_names, 'caption': captions})\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(pd.DataFrame({'image': train_images, 'caption': train_captions}), img_dir, word2idx, transform, 20)\n",
    "eval_dataset = ImageCaptionDataset(pd.DataFrame({'image': eval_images, 'caption': eval_captions}), img_dir, word2idx, transform, 20)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embed_size = 256\n",
    "encoder = DeiTEncoder(embed_size=embed_size).to(device)\n",
    "decoder = TransformerDecoder(len(word2idx), embed_size, num_heads=8, num_layers=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "smoothing = SmoothingFunction().method1\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, total_bleu1, total_bleu2, total_bleu4, total_rougeL = 0, 0, 0, 0, 0, 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "\n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "\n",
    "                ref_match = re.search(r\"<START>(.*?)<END>\", gt_caption)\n",
    "                pred_match = re.search(r\"^(.*?)<END>\", pred_caption)\n",
    "                reference = ref_match.group(1).strip() if ref_match else \"\"\n",
    "                prediction = pred_match.group(1).strip() if pred_match else \"\"\n",
    "                if not reference or not prediction:\n",
    "                    continue\n",
    "\n",
    "                ref_tokens = reference.split()\n",
    "                pred_tokens = prediction.split()\n",
    "\n",
    "                total_wer += jiwer.wer(reference, prediction)\n",
    "                total_cer += jiwer.cer(reference, prediction)\n",
    "                total_bleu1 += sentence_bleu([ref_tokens], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu2 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu4 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "                total_rougeL += rouge.score(reference, prediction)['rougeL'].fmeasure\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu1 = total_bleu1 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu2 = total_bleu2 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu4 = total_bleu4 / num_samples if num_samples > 0 else 0\n",
    "    avg_rougeL = total_rougeL / num_samples if num_samples > 0 else 0\n",
    "\n",
    "    print(f\"WER: {avg_wer:.2f}, CER: {avg_cer:.2f}, BLEU-1: {avg_bleu1:.2f}, BLEU-2: {avg_bleu2:.2f}, BLEU-4: {avg_bleu4:.2f}, ROUGE-L: {avg_rougeL:.2f}\")\n",
    "    return avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2decda0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dncoder Type: <class '__main__.DeiTEncoder'>\n",
      "‚úÖ Number of Parameters in Encoder: 21764224\n",
      "‚úÖ Number of Trainable Parameters: 21764224\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Dncoder Type:\", type(encoder))\n",
    "print(\"‚úÖ Number of Parameters in Encoder:\", sum(p.numel() for p in encoder.parameters()))\n",
    "print(\"‚úÖ Number of Trainable Parameters:\", sum(p.numel() for p in encoder.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598363aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Decoder Type: <class '__main__.TransformerDecoder'>\n",
      "‚úÖ Number of Parameters in Decoder: 3255738\n",
      "‚úÖ Number of Trainable Parameters: 3255738\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Decoder Type:\", type(decoder))\n",
    "print(\"‚úÖ Number of Parameters in Decoder:\", sum(p.numel() for p in decoder.parameters()))\n",
    "print(\"‚úÖ Number of Trainable Parameters:\", sum(p.numel() for p in decoder.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08fbf065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 4.0593\n",
      "WER: 0.73, CER: 0.65, BLEU-1: 0.28, BLEU-2: 0.17, BLEU-4: 0.09, ROUGE-L: 0.00\n",
      "Epoch 2: Train Loss: 3.3484\n",
      "WER: 0.70, CER: 0.62, BLEU-1: 0.31, BLEU-2: 0.19, BLEU-4: 0.09, ROUGE-L: 0.00\n",
      "Epoch 3: Train Loss: 3.3634\n",
      "WER: 0.68, CER: 0.61, BLEU-1: 0.31, BLEU-2: 0.20, BLEU-4: 0.10, ROUGE-L: 0.01\n",
      "Epoch 4: Train Loss: 3.1843\n",
      "WER: 0.68, CER: 0.61, BLEU-1: 0.33, BLEU-2: 0.21, BLEU-4: 0.10, ROUGE-L: 0.01\n",
      "Epoch 5: Train Loss: 3.2915\n",
      "WER: 0.65, CER: 0.58, BLEU-1: 0.37, BLEU-2: 0.24, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 6: Train Loss: 3.4639\n",
      "WER: 0.74, CER: 0.65, BLEU-1: 0.27, BLEU-2: 0.16, BLEU-4: 0.08, ROUGE-L: 0.01\n",
      "Epoch 7: Train Loss: 3.5978\n",
      "WER: 0.66, CER: 0.59, BLEU-1: 0.37, BLEU-2: 0.24, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 8: Train Loss: 3.5317\n",
      "WER: 0.67, CER: 0.61, BLEU-1: 0.35, BLEU-2: 0.22, BLEU-4: 0.11, ROUGE-L: 0.01\n",
      "Epoch 9: Train Loss: 3.6541\n",
      "WER: 0.65, CER: 0.58, BLEU-1: 0.37, BLEU-2: 0.25, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 10: Train Loss: 3.7834\n",
      "WER: 0.67, CER: 0.60, BLEU-1: 0.35, BLEU-2: 0.23, BLEU-4: 0.11, ROUGE-L: 0.01\n",
      "Epoch 11: Train Loss: 3.8687\n",
      "WER: 0.64, CER: 0.58, BLEU-1: 0.38, BLEU-2: 0.26, BLEU-4: 0.13, ROUGE-L: 0.01\n",
      "Epoch 12: Train Loss: 3.7871\n",
      "WER: 0.67, CER: 0.60, BLEU-1: 0.36, BLEU-2: 0.24, BLEU-4: 0.12, ROUGE-L: 0.01\n",
      "Epoch 13: Train Loss: 3.8260\n",
      "WER: 0.64, CER: 0.57, BLEU-1: 0.40, BLEU-2: 0.27, BLEU-4: 0.14, ROUGE-L: 0.01\n",
      "Epoch 14: Train Loss: 3.7405\n",
      "WER: 0.65, CER: 0.58, BLEU-1: 0.37, BLEU-2: 0.25, BLEU-4: 0.13, ROUGE-L: 0.01\n",
      "Epoch 15: Train Loss: 3.7887\n",
      "WER: 0.68, CER: 0.60, BLEU-1: 0.37, BLEU-2: 0.24, BLEU-4: 0.12, ROUGE-L: 0.01\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "teacher_forcing_ratio = 0.9  # Start with 90% teacher forcing\n",
    "\n",
    "train_losses, wer_scores, cer_scores = [], [], []\n",
    "bleu1_scores, bleu2_scores, bleu4_scores, rougeL_scores = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        \n",
    "        input_tokens = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        if torch.rand(1).item() > teacher_forcing_ratio:\n",
    "            # Scheduled sampling: use own prediction as input\n",
    "            outputs = []\n",
    "            batch_size = images.size(0)\n",
    "            inputs = torch.full((batch_size, 1), word2idx['<START>'], dtype=torch.long, device=device)\n",
    "\n",
    "            for _ in range(input_tokens.size(1)):\n",
    "                output = decoder(features, inputs)\n",
    "                last_output = output[:, -1, :]  # last timestep\n",
    "                predicted = last_output.argmax(-1).unsqueeze(1)\n",
    "                inputs = torch.cat((inputs, predicted), dim=1)\n",
    "                outputs.append(last_output)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1).squeeze(2)  # (B, T, vocab)\n",
    "        else:\n",
    "            # Teacher forcing: normal\n",
    "            outputs = decoder(features, input_tokens)\n",
    "\n",
    "        batch_size, seq_len, vocab_size = outputs.size()\n",
    "        loss = criterion(\n",
    "            outputs.reshape(batch_size * seq_len, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "\n",
    "    wer_scores.append(avg_wer)\n",
    "    cer_scores.append(avg_cer)\n",
    "\n",
    "    # Reduce teacher forcing slowly\n",
    "    teacher_forcing_ratio = max(0.3, teacher_forcing_ratio * 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ada9460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved best model!\n"
     ]
    }
   ],
   "source": [
    "if avg_wer < best_wer:\n",
    "    best_wer = avg_wer\n",
    "    torch.save(encoder.state_dict(), \"encoderDeiT.pth\")\n",
    "    torch.save(decoder.state_dict(), \"decodertransf.pth\")\n",
    "    print(\"‚úÖ Saved best model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d14f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'captioning_model_transf.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "767a592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing a prediction after training ===\n",
      "\n",
      "Predicted Caption: ·ûü·üí·ûè·üí·ûö·û∏ ·ûò·üí·ûì·û∂·ûÄ·üã ·ûò·û∂·ûì ·ûò·û∂·ûì ·ûñ·ûé·üå\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # shape (1, 3, H, W)\n",
    "\n",
    "    # Encode image\n",
    "    encoder_out = encoder(image)  # (1, 1, embed_size)\n",
    "\n",
    "    # Start generating\n",
    "    generated_indices = [word2idx['<START>']]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        input_tensor = torch.tensor([generated_indices], dtype=torch.long).to(device)  # (1, T)\n",
    "        \n",
    "        # Predict next token\n",
    "        output = decoder(encoder_out, input_tensor)  # (1, T, vocab_size)\n",
    "        next_token_logits = output[:, -1, :]          # (1, vocab_size)\n",
    "        predicted_index = next_token_logits.argmax(dim=-1).item()\n",
    "\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "        \n",
    "        generated_indices.append(predicted_index)\n",
    "\n",
    "    # Decode indices to words\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in generated_indices[1:]]  # Skip <START>\n",
    "\n",
    "    predicted_caption = ' '.join(predicted_tokens)  # üî• join words with space\n",
    "    return predicted_caption\n",
    "\n",
    "print(\"\\n=== Testing a prediction after training ===\\n\")\n",
    "\n",
    "test_image_path = '/home/vitoupro/code/image_captioning/data/10.png'  # your image\n",
    "\n",
    "predicted_caption = predict_caption(\n",
    "    image_path=test_image_path,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    idx2word=idx2word,\n",
    "    word2idx=word2idx,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "print(\"Predicted Caption:\", predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "926e9772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Predicted Caption: ·ûü·üí·ûè·üí·ûö·û∏ ·ûò·üí·ûì·û∂·ûÄ·üã ·ûä·üÇ·ûõ ·ûò·û∂·ûì ·ûñ·ûé·üå\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# === Load trained model ===\n",
    "encoder.load_state_dict(torch.load('/home/vitoupro/code/image_captioning/notebook/encoderDeiT.pth'))  # Replace with actual path\n",
    "decoder.load_state_dict(torch.load('/home/vitoupro/code/image_captioning/notebook/decodertransf.pth'))  # Replace with actual path\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# === Preprocessing ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# === Caption Generation ===\n",
    "@torch.no_grad()\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "\n",
    "    features = encoder(image)  # (1, 1, embed_size)\n",
    "    generated_ids = [word2idx[\"<START>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        input_seq = torch.tensor([generated_ids], dtype=torch.long).to(device)\n",
    "        outputs = decoder(features, input_seq)  # (1, T, vocab_size)\n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        predicted_id = next_token_logits.argmax(-1).item()\n",
    "\n",
    "        if predicted_id == word2idx[\"<END>\"]:\n",
    "            break\n",
    "        generated_ids.append(predicted_id)\n",
    "\n",
    "    tokens = [idx2word[str(i)] for i in generated_ids[1:]]  # skip <START>\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# === Run Inference ===\n",
    "test_image = \"/home/vitoupro/code/image_captioning/data/10.png\"  # Replace with your test image\n",
    "caption = predict_caption(test_image, encoder, decoder, transform, device, idx2word, word2idx)\n",
    "print(\"üñºÔ∏è Predicted Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a0e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
