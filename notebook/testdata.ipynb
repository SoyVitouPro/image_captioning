{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fcabda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from datasets) (2.2.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "Using cached multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Using cached propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "Using cached yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 datasets-3.5.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2024.12.0 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aefcfebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 10656.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the full dataset\n",
    "dataset = load_dataset(\"jpawan33/fkr30k-image-captioning-dataset\", split=\"train\")\n",
    "\n",
    "# Select only the first 5k examples\n",
    "dataset_small = dataset.select(range(1000))\n",
    "\n",
    "print(len(dataset_small))  # Should print 5000\n",
    "dataset_small.save_to_disk(\"fkr5k-dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f14d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting sniffio (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17452 sha256=74479fb0e2cafad68a03bee11726c1888419a8475c0df00c2c08a4177bc5a672\n",
      "  Stored in directory: /home/vitoupro/.cache/pip/wheels/95/0f/04/b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, sniffio, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0 sniffio-1.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310fb4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming translation from index 51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  37%|███▋      | 373/1000 [09:55<20:04,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'Men with reflective safety jackets on are working on a street intersection with many orange reflective cones .': The read operation timed out\n",
      "Skipping index 373 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  55%|█████▍    | 549/1000 [15:33<14:51,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'A man leans against a rock pillar with his back to an ongoing parade decorated with white and pink balloon arch .': the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Skipping index 549 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset: 100%|██████████| 1000/1000 [30:02<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All translations saved successfully to translated_dataset.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Use Google Translate\n",
    "from googletrans import Translator\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(\"fkr5k-dataset\")\n",
    "\n",
    "# Prepare output folders\n",
    "os.makedirs(\"dataset_images\", exist_ok=True)\n",
    "\n",
    "# Translator setup\n",
    "translator = Translator()\n",
    "\n",
    "# Progress files\n",
    "progress_file = \"progress.json\"\n",
    "output_txt_file = \"translated_dataset.txt\"\n",
    "\n",
    "# Load progress if exists\n",
    "translated_lines = []\n",
    "start_idx = 0\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        progress = json.load(f)\n",
    "        translated_lines = progress.get(\"translated_lines\", [])\n",
    "        start_idx = len(translated_lines)\n",
    "    print(f\"Resuming translation from index {start_idx}...\")\n",
    "else:\n",
    "    print(\"Starting new translation...\")\n",
    "\n",
    "# Start translating\n",
    "for idx, example in enumerate(tqdm(dataset, desc=\"Translating dataset\")):\n",
    "    if idx < start_idx:\n",
    "        continue  # Already translated\n",
    "\n",
    "    text = example[\"text\"]\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    try:\n",
    "        translated_text = translator.translate(text, src='en', dest='km').text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating '{text}': {e}\")\n",
    "        translated_text = None\n",
    "\n",
    "    if translated_text is not None:\n",
    "        # Save image\n",
    "        image_path = f\"dataset_images/image_{idx+1}.jpg\"\n",
    "        image.save(image_path)\n",
    "\n",
    "        # Save translation\n",
    "        line = f\"image_{idx+1}.jpg \\\"{translated_text}\\\"\"\n",
    "        translated_lines.append(line)\n",
    "\n",
    "        # Save progress every 50 items\n",
    "        if idx % 50 == 0:\n",
    "            with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"translated_lines\": translated_lines}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping index {idx} due to translation error.\")\n",
    "\n",
    "    # Optional: delay\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Save final text file\n",
    "try:\n",
    "    with open(output_txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in translated_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "    print(f\"✅ All translations saved successfully to {output_txt_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final text file: {e}\")\n",
    "\n",
    "# Clean up progress file\n",
    "if os.path.exists(progress_file):\n",
    "    os.remove(progress_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb341fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully cleaned and fixed filenames in translated_dataset.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# File to clean\n",
    "input_file = \"translated_dataset.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Function to clean text and fix filenames\n",
    "def clean_line(line):\n",
    "    # First, split between image name and caption\n",
    "    parts = line.strip().split(\" \", 1)\n",
    "    \n",
    "    if len(parts) != 2:\n",
    "        return None  # Skip malformed lines\n",
    "\n",
    "    image_name, caption = parts\n",
    "\n",
    "    # Fix the image name: insert underscore and .jpg properly\n",
    "    # Assuming image name format is like 'image1jpg' -> 'image_1.jpg'\n",
    "    if image_name.startswith(\"image\") and image_name.endswith(\"jpg\"):\n",
    "        index = image_name[5:-3]  # Extract number between 'image' and 'jpg'\n",
    "        fixed_image_name = f\"image_{index}.jpg\"\n",
    "    else:\n",
    "        fixed_image_name = image_name  # fallback if strange format\n",
    "\n",
    "    # Clean the caption: remove \" and special characters\n",
    "    caption = caption.replace('\"', '')\n",
    "    caption = re.sub(r\"[!@#$%^&*()_+=\\[\\]{}\\\\|:;'<>,.?/~`]\", '', caption)\n",
    "    caption = caption.strip()\n",
    "\n",
    "    return f\"{fixed_image_name} {caption}\"\n",
    "\n",
    "# Clean all lines\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    cleaned = clean_line(line)\n",
    "    if cleaned:\n",
    "        cleaned_lines.append(cleaned)\n",
    "\n",
    "# Overwrite the original file\n",
    "with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in cleaned_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"✅ Successfully cleaned and fixed filenames in {input_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbfd1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5000/5000 [00:00<00:00, 11071.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the full dataset\n",
    "dataset = load_dataset(\"jpawan33/fkr30k-image-captioning-dataset\", split=\"train\")\n",
    "\n",
    "# Select only the first 5k examples\n",
    "dataset_small = dataset.select(range(5000))\n",
    "\n",
    "print(len(dataset_small))  # Should print 5000\n",
    "dataset_small.save_to_disk(\"fkr5k-dataset-5k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c1e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming translation from index 51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  23%|██▎       | 1151/5000 [25:57<2:12:15,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'A small boy wearing a diaper stands near the door and is covered in marker .': The read operation timed out\n",
      "Skipping index 1151 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  50%|████▉     | 2485/5000 [1:08:20<1:03:54,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'Four dogs play together on a grassy and leafy ground .': the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Skipping index 2485 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  50%|████▉     | 2486/5000 [1:08:21<57:24,  1.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'A Father is observing his son , Kurt to see if his teaching prevents Kurt from cutting himself as he shave with a razor for the first time .': the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Skipping index 2486 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  50%|████▉     | 2487/5000 [1:08:22<52:24,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'The man in the red , hooded sweatshirt looks back and construction is taking place on a shop with Hebrew lettering .': the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Skipping index 2487 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  50%|█████     | 2500/5000 [1:08:49<1:45:11,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'The female with the dark shirt cutting the hair of the female in the red shirt .': the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Skipping index 2500 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  56%|█████▌    | 2811/5000 [1:18:45<1:08:30,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'A man in a blue jersey and orange visor threw a frisbee along a grass hill .': the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Skipping index 2811 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  64%|██████▍   | 3225/5000 [1:32:20<1:01:03,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'Three boys in sports casual clothing are posing in front of a blue building': The read operation timed out\n",
      "Skipping index 3225 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  65%|██████▌   | 3256/5000 [1:33:28<55:50,  1.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'Two women are giving each other a hug while a man holding a glass is looking at the camera .': The read operation timed out\n",
      "Skipping index 3256 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  76%|███████▌  | 3812/5000 [1:51:52<37:50,  1.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'Four workers walking in a field with a desert in the background .': the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Skipping index 3812 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  90%|████████▉ | 4477/5000 [2:13:46<16:03,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'A female police officer , wearing an officer 's hat and sunglasses , stands in uniform in front of a window-lined street block .': the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Skipping index 4477 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset:  92%|█████████▏| 4584/5000 [2:17:09<10:46,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating 'An Asian store with people walking throughout and a big red arrow pointing left .': The read operation timed out\n",
      "Skipping index 4584 due to translation error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating dataset: 100%|██████████| 5000/5000 [2:30:10<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All translations saved successfully to translated_dataset5k.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Use Google Translate\n",
    "from googletrans import Translator\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(\"fkr5k-dataset-5k\")\n",
    "\n",
    "# Prepare output folders\n",
    "os.makedirs(\"dataset_images\", exist_ok=True)\n",
    "\n",
    "# Translator setup\n",
    "translator = Translator()\n",
    "\n",
    "# Progress files\n",
    "progress_file = \"progress5k.json\"\n",
    "output_txt_file = \"translated_dataset5k.txt\"\n",
    "\n",
    "# Load progress if exists\n",
    "translated_lines = []\n",
    "start_idx = 0\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        progress = json.load(f)\n",
    "        translated_lines = progress.get(\"translated_lines\", [])\n",
    "        start_idx = len(translated_lines)\n",
    "    print(f\"Resuming translation from index {start_idx}...\")\n",
    "else:\n",
    "    print(\"Starting new translation...\")\n",
    "\n",
    "# Start translating\n",
    "for idx, example in enumerate(tqdm(dataset, desc=\"Translating dataset\")):\n",
    "    if idx < start_idx:\n",
    "        continue  # Already translated\n",
    "\n",
    "    text = example[\"text\"]\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    try:\n",
    "        translated_text = translator.translate(text, src='en', dest='km').text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating '{text}': {e}\")\n",
    "        translated_text = None\n",
    "\n",
    "    if translated_text is not None:\n",
    "        # Save image\n",
    "        image_path = f\"dataset_images/image_{idx+1}.jpg\"\n",
    "        image.save(image_path)\n",
    "\n",
    "        # Save translation\n",
    "        line = f\"image_{idx+1}.jpg \\\"{translated_text}\\\"\"\n",
    "        translated_lines.append(line)\n",
    "\n",
    "        # Save progress every 50 items\n",
    "        if idx % 50 == 0:\n",
    "            with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"translated_lines\": translated_lines}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping index {idx} due to translation error.\")\n",
    "\n",
    "    # Optional: delay\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Save final text file\n",
    "try:\n",
    "    with open(output_txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in translated_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "    print(f\"✅ All translations saved successfully to {output_txt_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final text file: {e}\")\n",
    "\n",
    "# Clean up progress file\n",
    "if os.path.exists(progress_file):\n",
    "    os.remove(progress_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df65110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully cleaned and fixed filenames in translated_dataset5k.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# File to clean\n",
    "input_file = \"translated_dataset5k.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Function to clean text and fix filenames\n",
    "def clean_line(line):\n",
    "    # First, split between image name and caption\n",
    "    parts = line.strip().split(\" \", 1)\n",
    "    \n",
    "    if len(parts) != 2:\n",
    "        return None  # Skip malformed lines\n",
    "\n",
    "    image_name, caption = parts\n",
    "\n",
    "    # Clean image name: handle formats like 'image___1...jpg'\n",
    "    match = re.search(r'image_+(\\d+)\\.*jpg', image_name)\n",
    "    if match:\n",
    "        index = match.group(1)\n",
    "        fixed_image_name = f\"image_{index}.jpg\"\n",
    "    else:\n",
    "        # Try to fallback to known pattern\n",
    "        match = re.search(r'image(\\d+)\\.*jpg', image_name)\n",
    "        if match:\n",
    "            index = match.group(1)\n",
    "            fixed_image_name = f\"image_{index}.jpg\"\n",
    "        else:\n",
    "            fixed_image_name = image_name  # keep as-is if unrecognized\n",
    "\n",
    "    # Clean the caption: remove \" and special characters (including Khmer period \"។\")\n",
    "    caption = caption.replace('\"', '')\n",
    "    caption = re.sub(r\"[!@#$%^&*()_+=\\[\\]{}\\\\|:;'<>,.?/~`។]\", '', caption)\n",
    "    caption = caption.strip()\n",
    "\n",
    "    return f\"{fixed_image_name} {caption}\"\n",
    "\n",
    "# Clean all lines\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    cleaned = clean_line(line)\n",
    "    if cleaned:\n",
    "        cleaned_lines.append(cleaned)\n",
    "\n",
    "# Overwrite the original file\n",
    "with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in cleaned_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"✅ Successfully cleaned and fixed filenames in {input_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1495fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
