{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "211f5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: nltk in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from rouge-score) (2.2.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=930cfa745d1de3c658913074124c81bac048fc14bbffffc4517551ccfd912c20\n",
      "  Stored in directory: /home/vitoupro/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.2.2 rouge-score-0.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8c28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File updated with image prefixes.\n"
     ]
    }
   ],
   "source": [
    "# Path to your original file\n",
    "file_path = '/home/vitoupro/code/image_captioning/data/raw/imglabel.txt'\n",
    "\n",
    "# Read all lines\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Modify each line with image prefix\n",
    "modified_lines = [f\"image{i+1}.jpg {line.strip()}\\n\" for i, line in enumerate(lines)]\n",
    "\n",
    "# Write the modified lines back to the same file\n",
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(modified_lines)\n",
    "\n",
    "print(\"File updated with image prefixes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2190cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: torch in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from timm) (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from timm) (0.21.0+cu118)\n",
      "Requirement already satisfied: pyyaml in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from timm) (0.30.1)\n",
      "Requirement already satisfied: safetensors in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from huggingface_hub->timm) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from huggingface_hub->timm) (4.13.1)\n",
      "Requirement already satisfied: networkx in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torchvision->timm) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from torchvision->timm) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: timm\n",
      "Successfully installed timm-1.0.15\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8cd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word_level_khmercut_420.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding a list of words (word-level)\n",
    "def encode_khmer_sentence(sentence, word2idx):\n",
    "    words = sentence.strip().split()\n",
    "    indices = []\n",
    "    for word in words:\n",
    "        index = word2idx.get(word)\n",
    "        if index is None:\n",
    "            return None, f\"Word '{word}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "# Decoding a list of indices (word-level)\n",
    "def decode_indices(indices, idx2word):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        word = idx2word.get(str(index))\n",
    "        if word is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        words.append(word)\n",
    "    return ' '.join(words), None\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if 'layer4' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1]) # output shape: (B, 2048, 1, 1)\n",
    "        self.linear = nn.Linear(2048, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.linear(features)\n",
    "        return features.unsqueeze(1)\n",
    "\n",
    "# # Data-efficient Image Transformer (DeiT) Encoder\n",
    "# class DeiTEncoder(nn.Module):\n",
    "#     def __init__(self, embed_size):\n",
    "#         super(DeiTEncoder, self).__init__()\n",
    "#         self.deit = create_model('deit_tiny_patch16_224', pretrained=True)\n",
    "#         self.deit.head = nn.Identity()\n",
    "#         self.linear = nn.Linear(768, embed_size)\n",
    "\n",
    "#     def forward(self, images):\n",
    "#         features = self.deit.forward_features(images)\n",
    "#         features = self.linear(features)\n",
    "#         return features.unsqueeze(1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads=8, num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = self.embedding(captions)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        memory = features.permute(1, 0, 2)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out.permute(1, 0, 2)\n",
    "\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_sentence(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/data/raw/img'\n",
    "image_names, captions = [], []\n",
    "\n",
    "with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            image_name, caption = parts\n",
    "            image_names.append(image_name)\n",
    "            captions.append(caption)\n",
    "\n",
    "all_images = pd.DataFrame({'image': image_names, 'caption': captions})\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(pd.DataFrame({'image': train_images, 'caption': train_captions}), img_dir, word2idx, transform, 20)\n",
    "eval_dataset = ImageCaptionDataset(pd.DataFrame({'image': eval_images, 'caption': eval_captions}), img_dir, word2idx, transform, 20)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embed_size = 256\n",
    "encoder = EncoderCNN(embed_size=embed_size).to(device)\n",
    "decoder = TransformerDecoder(len(word2idx), embed_size, num_heads=8, num_layers=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "smoothing = SmoothingFunction().method1\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, total_bleu1, total_bleu2, total_bleu4, total_rougeL = 0, 0, 0, 0, 0, 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "\n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "\n",
    "                ref_match = re.search(r\"<START>(.*?)<END>\", gt_caption)\n",
    "                pred_match = re.search(r\"^(.*?)<END>\", pred_caption)\n",
    "                reference = ref_match.group(1).strip() if ref_match else \"\"\n",
    "                prediction = pred_match.group(1).strip() if pred_match else \"\"\n",
    "                if not reference or not prediction:\n",
    "                    continue\n",
    "\n",
    "                ref_tokens = reference.split()\n",
    "                pred_tokens = prediction.split()\n",
    "\n",
    "                total_wer += jiwer.wer(reference, prediction)\n",
    "                total_cer += jiwer.cer(reference, prediction)\n",
    "                total_bleu1 += sentence_bleu([ref_tokens], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu2 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu4 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "                total_rougeL += rouge.score(reference, prediction)['rougeL'].fmeasure\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu1 = total_bleu1 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu2 = total_bleu2 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu4 = total_bleu4 / num_samples if num_samples > 0 else 0\n",
    "    avg_rougeL = total_rougeL / num_samples if num_samples > 0 else 0\n",
    "\n",
    "    print(f\"WER: {avg_wer:.2f}, CER: {avg_cer:.2f}, BLEU-1: {avg_bleu1:.2f}, BLEU-2: {avg_bleu2:.2f}, BLEU-4: {avg_bleu4:.2f}, ROUGE-L: {avg_rougeL:.2f}\")\n",
    "    return avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee72427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 4.9101\n",
      "WER: 0.00, CER: 0.00, BLEU-1: 0.00, BLEU-2: 0.00, BLEU-4: 0.00, ROUGE-L: 0.00\n",
      "Epoch 2: Train Loss: 4.5289\n",
      "WER: 0.90, CER: 0.86, BLEU-1: 0.02, BLEU-2: 0.01, BLEU-4: 0.01, ROUGE-L: 0.00\n",
      "Epoch 3: Train Loss: 4.1939\n",
      "WER: 0.83, CER: 0.72, BLEU-1: 0.19, BLEU-2: 0.09, BLEU-4: 0.04, ROUGE-L: 0.00\n",
      "Epoch 4: Train Loss: 3.8968\n",
      "WER: 0.83, CER: 0.75, BLEU-1: 0.20, BLEU-2: 0.09, BLEU-4: 0.04, ROUGE-L: 0.00\n",
      "Epoch 5: Train Loss: 4.0195\n",
      "WER: 0.94, CER: 0.87, BLEU-1: 0.09, BLEU-2: 0.04, BLEU-4: 0.02, ROUGE-L: 0.00\n",
      "Epoch 6: Train Loss: 3.8247\n",
      "WER: 0.82, CER: 0.71, BLEU-1: 0.24, BLEU-2: 0.11, BLEU-4: 0.05, ROUGE-L: 0.00\n",
      "Epoch 7: Train Loss: 3.6439\n",
      "WER: 0.80, CER: 0.68, BLEU-1: 0.25, BLEU-2: 0.13, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 8: Train Loss: 3.7190\n",
      "WER: 0.82, CER: 0.71, BLEU-1: 0.22, BLEU-2: 0.09, BLEU-4: 0.05, ROUGE-L: 0.00\n",
      "Epoch 9: Train Loss: 3.6129\n",
      "WER: 0.79, CER: 0.68, BLEU-1: 0.26, BLEU-2: 0.13, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 10: Train Loss: 3.6554\n",
      "WER: 0.83, CER: 0.72, BLEU-1: 0.25, BLEU-2: 0.14, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 11: Train Loss: 3.1329\n",
      "WER: 0.79, CER: 0.67, BLEU-1: 0.31, BLEU-2: 0.16, BLEU-4: 0.08, ROUGE-L: 0.00\n",
      "Epoch 12: Train Loss: 3.5476\n",
      "WER: 0.76, CER: 0.70, BLEU-1: 0.28, BLEU-2: 0.15, BLEU-4: 0.07, ROUGE-L: 0.00\n",
      "Epoch 13: Train Loss: 3.1910\n",
      "WER: 0.76, CER: 0.65, BLEU-1: 0.30, BLEU-2: 0.17, BLEU-4: 0.08, ROUGE-L: 0.00\n",
      "Epoch 14: Train Loss: 3.3019\n",
      "WER: 0.81, CER: 0.68, BLEU-1: 0.26, BLEU-2: 0.13, BLEU-4: 0.07, ROUGE-L: 0.00\n",
      "Epoch 15: Train Loss: 3.2150\n",
      "WER: 0.80, CER: 0.69, BLEU-1: 0.28, BLEU-2: 0.14, BLEU-4: 0.06, ROUGE-L: 0.00\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "teacher_forcing_ratio = 0.9  # Start with 90% teacher forcing\n",
    "\n",
    "train_losses, wer_scores, cer_scores = [], [], []\n",
    "bleu1_scores, bleu2_scores, bleu4_scores, rougeL_scores = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        \n",
    "        input_tokens = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        if torch.rand(1).item() > teacher_forcing_ratio:\n",
    "            # Scheduled sampling: use own prediction as input\n",
    "            outputs = []\n",
    "            batch_size = images.size(0)\n",
    "            inputs = torch.full((batch_size, 1), word2idx['<START>'], dtype=torch.long, device=device)\n",
    "\n",
    "            for _ in range(input_tokens.size(1)):\n",
    "                output = decoder(features, inputs)\n",
    "                last_output = output[:, -1, :]  # last timestep\n",
    "                predicted = last_output.argmax(-1).unsqueeze(1)\n",
    "                inputs = torch.cat((inputs, predicted), dim=1)\n",
    "                outputs.append(last_output)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1).squeeze(2)  # (B, T, vocab)\n",
    "        else:\n",
    "            # Teacher forcing: normal\n",
    "            outputs = decoder(features, input_tokens)\n",
    "\n",
    "        batch_size, seq_len, vocab_size = outputs.size()\n",
    "        loss = criterion(\n",
    "            outputs.reshape(batch_size * seq_len, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "\n",
    "    wer_scores.append(avg_wer)\n",
    "    cer_scores.append(avg_cer)\n",
    "\n",
    "    # Reduce teacher forcing slowly\n",
    "    teacher_forcing_ratio = max(0.3, teacher_forcing_ratio * 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a9af286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing a prediction after training ===\n",
      "\n",
      "Predicted Caption: ·ûÄ·üÖ·û¢·û∏ ·û¢·ûÑ·üí·ûÇ·ûª·ûô ·ûä·üÇ·ûõ ·ûò·û∂·ûì ·ûê·ûº ·ûö·ûª·ûÄ·üí·ûÅ·ûá·û∂·ûè·û∑ ·ûò·ûΩ·ûô\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # shape (1, 3, H, W)\n",
    "\n",
    "    # Encode image\n",
    "    encoder_out = encoder(image)  # (1, 1, embed_size)\n",
    "\n",
    "    # Start generating\n",
    "    generated_indices = [word2idx['<START>']]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        input_tensor = torch.tensor([generated_indices], dtype=torch.long).to(device)  # (1, T)\n",
    "        \n",
    "        # Predict next token\n",
    "        output = decoder(encoder_out, input_tensor)  # (1, T, vocab_size)\n",
    "        next_token_logits = output[:, -1, :]          # (1, vocab_size)\n",
    "        predicted_index = next_token_logits.argmax(dim=-1).item()\n",
    "\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "        \n",
    "        generated_indices.append(predicted_index)\n",
    "\n",
    "    # Decode indices to words\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in generated_indices[1:]]  # Skip <START>\n",
    "\n",
    "    predicted_caption = ' '.join(predicted_tokens)  # üî• join words with space\n",
    "    return predicted_caption\n",
    "\n",
    "print(\"\\n=== Testing a prediction after training ===\\n\")\n",
    "\n",
    "test_image_path = '/home/vitoupro/code/image_captioning/data/image.png'  # your image\n",
    "\n",
    "predicted_caption = predict_caption(\n",
    "    image_path=test_image_path,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    idx2word=idx2word,\n",
    "    word2idx=word2idx,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "print(\"Predicted Caption:\", predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "267474f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ·ûÄ·üÖ·û¢·û∏·û¢·ûÑ·üí·ûÇ·ûª·ûô·ûä·üÇ·ûõ·ûò·û∂·ûì·ûñ·ûº·ûÄ·ûñ·ûé·üå·ûÅ·üí·ûò·üÖ·ûì·û∑·ûÑ·ûò·û∂·ûì·ûï·üí·ûÄ·û∂·ûò·ûΩ·ûô·ûì·üÖ·ûá·û∑·ûè·ûÄ·üÖ·û¢·û∏\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_caption_beam_search(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20, beam_size=3):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode\n",
    "    encoder_out = encoder(image)  # (1, 1, embed)\n",
    "\n",
    "    sequences = [[list([word2idx['<START>']]), 0.0]]  # (sequence, score)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            tgt = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "            outputs = decoder(encoder_out, tgt)\n",
    "            outputs = outputs[:, -1, :]  # last step\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "            topk_probs, topk_indices = probs.topk(beam_size)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                candidate = seq + [topk_indices[0][i].item()]\n",
    "                candidate_score = score - torch.log(topk_probs[0][i]).item()\n",
    "                all_candidates.append((candidate, candidate_score))\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:beam_size]\n",
    "\n",
    "    best_seq = sequences[0][0]\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in best_seq[1:] if idx != word2idx['<END>']]  # skip <START>\n",
    "    predicted_caption = ''.join(predicted_tokens)\n",
    "    return predicted_caption\n",
    "  \n",
    "  \n",
    "test_image_path = '/home/vitoupro/code/image_captioning/data/image.png'  # your image\n",
    "\n",
    "predicted_caption = predict_caption_beam_search(\n",
    "    image_path=test_image_path,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    idx2word=idx2word,\n",
    "    word2idx=word2idx,\n",
    "    max_length=20,\n",
    "    beam_size=3  # you can test beam_size=3 or 5\n",
    ")\n",
    "\n",
    "print(\"Predicted Caption:\", predicted_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7120901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Save best model checkpoint\n",
    "    if avg_wer < best_wer:\n",
    "        best_wer = avg_wer\n",
    "        torch.save({\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }, 'best_model_checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
