{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from timm import create_model\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word_level_khmercut_420.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding a list of words (word-level)\n",
    "def encode_khmer_sentence(sentence, word2idx):\n",
    "    words = sentence.strip().split()\n",
    "    indices = []\n",
    "    for word in words:\n",
    "        index = word2idx.get(word)\n",
    "        if index is None:\n",
    "            return None, f\"Word '{word}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "# Decoding a list of indices (word-level)\n",
    "def decode_indices(indices, idx2word):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        word = idx2word.get(str(index))\n",
    "        if word is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        words.append(word)\n",
    "    return ' '.join(words), None\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "\n",
    "# Data-efficient Image Transformer (DeiT) Encoder\n",
    "class DeiTEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(DeiTEncoder, self).__init__()\n",
    "        import timm\n",
    "        self.deit = timm.create_model('deit_tiny_patch16_224', pretrained=True)\n",
    "        self.deit.head = nn.Identity()  # Remove classification head\n",
    "        self.linear = nn.Linear(self.deit.num_features, embed_size)  # Project to embed_size\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.deit.forward_features(images)  # (B, seq_len, 192)\n",
    "        cls_token = features[:, 0, :]  # Take only [CLS] token: (B, 192)\n",
    "        out = self.linear(cls_token)  # Project to (B, embed_size)\n",
    "        return out.unsqueeze(1)  # (B, 1, embed_size)\n",
    "       \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads=8, num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = self.embedding(captions)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        memory = features.permute(1, 0, 2)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out.permute(1, 0, 2)\n",
    "\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_sentence(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/data/raw/img'\n",
    "image_names, captions = [], []\n",
    "\n",
    "with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            image_name, caption = parts\n",
    "            image_names.append(image_name)\n",
    "            captions.append(caption)\n",
    "\n",
    "all_images = pd.DataFrame({'image': image_names, 'caption': captions})\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(pd.DataFrame({'image': train_images, 'caption': train_captions}), img_dir, word2idx, transform, 20)\n",
    "eval_dataset = ImageCaptionDataset(pd.DataFrame({'image': eval_images, 'caption': eval_captions}), img_dir, word2idx, transform, 20)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embed_size = 256\n",
    "encoder = DeiTEncoder(embed_size=embed_size).to(device)\n",
    "decoder = TransformerDecoder(len(word2idx), embed_size, num_heads=8, num_layers=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "smoothing = SmoothingFunction().method1\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, total_bleu1, total_bleu2, total_bleu4, total_rougeL = 0, 0, 0, 0, 0, 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "\n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "\n",
    "                ref_match = re.search(r\"<START>(.*?)<END>\", gt_caption)\n",
    "                pred_match = re.search(r\"^(.*?)<END>\", pred_caption)\n",
    "                reference = ref_match.group(1).strip() if ref_match else \"\"\n",
    "                prediction = pred_match.group(1).strip() if pred_match else \"\"\n",
    "                if not reference or not prediction:\n",
    "                    continue\n",
    "\n",
    "                ref_tokens = reference.split()\n",
    "                pred_tokens = prediction.split()\n",
    "\n",
    "                total_wer += jiwer.wer(reference, prediction)\n",
    "                total_cer += jiwer.cer(reference, prediction)\n",
    "                total_bleu1 += sentence_bleu([ref_tokens], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu2 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu4 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "                total_rougeL += rouge.score(reference, prediction)['rougeL'].fmeasure\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu1 = total_bleu1 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu2 = total_bleu2 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu4 = total_bleu4 / num_samples if num_samples > 0 else 0\n",
    "    avg_rougeL = total_rougeL / num_samples if num_samples > 0 else 0\n",
    "\n",
    "    print(f\"WER: {avg_wer:.2f}, CER: {avg_cer:.2f}, BLEU-1: {avg_bleu1:.2f}, BLEU-2: {avg_bleu2:.2f}, BLEU-4: {avg_bleu4:.2f}, ROUGE-L: {avg_rougeL:.2f}\")\n",
    "    return avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d869b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 4.8700\n",
      "WER: 0.91, CER: 0.78, BLEU-1: 0.11, BLEU-2: 0.03, BLEU-4: 0.02, ROUGE-L: 0.00\n",
      "Epoch 2: Train Loss: 3.9889\n",
      "WER: 0.87, CER: 0.73, BLEU-1: 0.24, BLEU-2: 0.12, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 3: Train Loss: 4.3184\n",
      "WER: 0.82, CER: 0.72, BLEU-1: 0.20, BLEU-2: 0.08, BLEU-4: 0.04, ROUGE-L: 0.00\n",
      "Epoch 4: Train Loss: 3.8762\n",
      "WER: 0.82, CER: 0.70, BLEU-1: 0.25, BLEU-2: 0.12, BLEU-4: 0.05, ROUGE-L: 0.00\n",
      "Epoch 5: Train Loss: 3.6982\n",
      "WER: 0.80, CER: 0.67, BLEU-1: 0.25, BLEU-2: 0.13, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 6: Train Loss: 3.6491\n",
      "WER: 0.90, CER: 0.73, BLEU-1: 0.28, BLEU-2: 0.15, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 7: Train Loss: 3.6855\n",
      "WER: 0.89, CER: 0.72, BLEU-1: 0.27, BLEU-2: 0.13, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 8: Train Loss: 3.6568\n",
      "WER: 0.75, CER: 0.61, BLEU-1: 0.33, BLEU-2: 0.18, BLEU-4: 0.08, ROUGE-L: 0.00\n",
      "Epoch 9: Train Loss: 3.5825\n",
      "WER: 0.78, CER: 0.65, BLEU-1: 0.30, BLEU-2: 0.16, BLEU-4: 0.07, ROUGE-L: 0.00\n",
      "Epoch 10: Train Loss: 3.5989\n",
      "WER: 0.94, CER: 0.75, BLEU-1: 0.29, BLEU-2: 0.16, BLEU-4: 0.08, ROUGE-L: 0.00\n",
      "Epoch 11: Train Loss: 3.7227\n",
      "WER: 0.77, CER: 0.65, BLEU-1: 0.33, BLEU-2: 0.19, BLEU-4: 0.08, ROUGE-L: 0.00\n",
      "Epoch 12: Train Loss: 3.3633\n",
      "WER: 0.77, CER: 0.67, BLEU-1: 0.28, BLEU-2: 0.16, BLEU-4: 0.07, ROUGE-L: 0.00\n",
      "Epoch 13: Train Loss: 3.8095\n",
      "WER: 0.76, CER: 0.65, BLEU-1: 0.30, BLEU-2: 0.16, BLEU-4: 0.09, ROUGE-L: 0.00\n",
      "Epoch 14: Train Loss: 3.0843\n",
      "WER: 0.78, CER: 0.67, BLEU-1: 0.30, BLEU-2: 0.17, BLEU-4: 0.10, ROUGE-L: 0.00\n",
      "Epoch 15: Train Loss: 3.2465\n",
      "WER: 0.86, CER: 0.72, BLEU-1: 0.31, BLEU-2: 0.17, BLEU-4: 0.08, ROUGE-L: 0.00\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "teacher_forcing_ratio = 0.9  # Start with 90% teacher forcing\n",
    "\n",
    "train_losses, wer_scores, cer_scores = [], [], []\n",
    "bleu1_scores, bleu2_scores, bleu4_scores, rougeL_scores = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        \n",
    "        input_tokens = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        if torch.rand(1).item() > teacher_forcing_ratio:\n",
    "            # Scheduled sampling: use own prediction as input\n",
    "            outputs = []\n",
    "            batch_size = images.size(0)\n",
    "            inputs = torch.full((batch_size, 1), word2idx['<START>'], dtype=torch.long, device=device)\n",
    "\n",
    "            for _ in range(input_tokens.size(1)):\n",
    "                output = decoder(features, inputs)\n",
    "                last_output = output[:, -1, :]  # last timestep\n",
    "                predicted = last_output.argmax(-1).unsqueeze(1)\n",
    "                inputs = torch.cat((inputs, predicted), dim=1)\n",
    "                outputs.append(last_output)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1).squeeze(2)  # (B, T, vocab)\n",
    "        else:\n",
    "            # Teacher forcing: normal\n",
    "            outputs = decoder(features, input_tokens)\n",
    "\n",
    "        batch_size, seq_len, vocab_size = outputs.size()\n",
    "        loss = criterion(\n",
    "            outputs.reshape(batch_size * seq_len, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "\n",
    "    wer_scores.append(avg_wer)\n",
    "    cer_scores.append(avg_cer)\n",
    "\n",
    "    # Reduce teacher forcing slowly\n",
    "    teacher_forcing_ratio = max(0.3, teacher_forcing_ratio * 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d6f745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing a prediction after training ===\n",
      "\n",
      "Predicted Caption: ·ûò·û∂·ûì ·ûì·üÅ·üá ·ûò·û∂·ûì ·ûò·ûΩ·ûô ·ûò·ûΩ·ûô\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_caption(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # shape (1, 3, H, W)\n",
    "\n",
    "    # Encode image\n",
    "    encoder_out = encoder(image)  # (1, 1, embed_size)\n",
    "\n",
    "    # Start generating\n",
    "    generated_indices = [word2idx['<START>']]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        input_tensor = torch.tensor([generated_indices], dtype=torch.long).to(device)  # (1, T)\n",
    "        \n",
    "        # Predict next token\n",
    "        output = decoder(encoder_out, input_tensor)  # (1, T, vocab_size)\n",
    "        next_token_logits = output[:, -1, :]          # (1, vocab_size)\n",
    "        predicted_index = next_token_logits.argmax(dim=-1).item()\n",
    "\n",
    "        if predicted_index == word2idx['<END>']:\n",
    "            break\n",
    "        \n",
    "        generated_indices.append(predicted_index)\n",
    "\n",
    "    # Decode indices to words\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in generated_indices[1:]]  # Skip <START>\n",
    "\n",
    "    predicted_caption = ' '.join(predicted_tokens)  # üî• join words with space\n",
    "    return predicted_caption\n",
    "\n",
    "print(\"\\n=== Testing a prediction after training ===\\n\")\n",
    "\n",
    "test_image_path = '/home/vitoupro/code/image_captioning/data/9.png'  # your image\n",
    "\n",
    "predicted_caption = predict_caption(\n",
    "    image_path=test_image_path,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    idx2word=idx2word,\n",
    "    word2idx=word2idx,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "print(\"Predicted Caption:\", predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f264761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: ·ûö·ûº·ûî·ûì·üÅ·üá·ûò·û∂·ûì·ûÄ·üí·ûö·û°·ûê·üí·ûò·ûò·ûΩ·ûô·ûì·û∑·ûÑ·ûü·üí·ûõ·û∂·ûî·ûñ·üí·ûö·û∂·ûò·ûΩ·ûô\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_caption_beam_search(image_path, encoder, decoder, transform, device, idx2word, word2idx, max_length=20, beam_size=3):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode\n",
    "    encoder_out = encoder(image)  # (1, 1, embed)\n",
    "\n",
    "    sequences = [[list([word2idx['<START>']]), 0.0]]  # (sequence, score)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            tgt = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "            outputs = decoder(encoder_out, tgt)\n",
    "            outputs = outputs[:, -1, :]  # last step\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "            topk_probs, topk_indices = probs.topk(beam_size)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                candidate = seq + [topk_indices[0][i].item()]\n",
    "                candidate_score = score - torch.log(topk_probs[0][i]).item()\n",
    "                all_candidates.append((candidate, candidate_score))\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:beam_size]\n",
    "\n",
    "    best_seq = sequences[0][0]\n",
    "    predicted_tokens = [idx2word[str(idx)] for idx in best_seq[1:] if idx != word2idx['<END>']]  # skip <START>\n",
    "    predicted_caption = ''.join(predicted_tokens)\n",
    "    return predicted_caption\n",
    "  \n",
    "  \n",
    "test_image_path = '/home/vitoupro/code/image_captioning/data/9.png'  # your image\n",
    "\n",
    "predicted_caption = predict_caption_beam_search(\n",
    "    image_path=test_image_path,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    idx2word=idx2word,\n",
    "    word2idx=word2idx,\n",
    "    max_length=20,\n",
    "    beam_size=3  # you can test beam_size=3 or 5\n",
    ")\n",
    "\n",
    "print(\"Predicted Caption:\", predicted_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11dcadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitoupro/code/image_captioning/image_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jiwer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from timm import create_model\n",
    "\n",
    "# Function to load idx2word and convert it to word2idx\n",
    "def load_vocabulary(path):\n",
    "    with open(path, 'r') as file:\n",
    "        idx2word = json.load(file)\n",
    "    word2idx = {v: int(k) for k, v in idx2word.items()}\n",
    "    return idx2word, word2idx\n",
    "\n",
    "# Load vocabulary\n",
    "idx2word_path = '/home/vitoupro/code/image_captioning/data/processed/idx2word_level_khmercut_420.json'\n",
    "idx2word, word2idx = load_vocabulary(idx2word_path)\n",
    "\n",
    "# Encoding a list of words (word-level)\n",
    "def encode_khmer_sentence(sentence, word2idx):\n",
    "    words = sentence.strip().split()\n",
    "    indices = []\n",
    "    for word in words:\n",
    "        index = word2idx.get(word)\n",
    "        if index is None:\n",
    "            return None, f\"Word '{word}' not found in vocabulary!\"\n",
    "        indices.append(index)\n",
    "    return indices, None\n",
    "\n",
    "# Decoding a list of indices (word-level)\n",
    "def decode_indices(indices, idx2word):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        word = idx2word.get(str(index))\n",
    "        if word is None:\n",
    "            return None, f\"Index '{index}' not found in idx2word!\"\n",
    "        words.append(word)\n",
    "    return ' '.join(words), None\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, hidden):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_out.size(1), 1)\n",
    "        attn_input = torch.cat((encoder_out, hidden), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        alpha = torch.softmax(attention, dim=1)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "\n",
    "# Data-efficient Image Transformer (DeiT) Encoder\n",
    "class DeiTEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(DeiTEncoder, self).__init__()\n",
    "        import timm\n",
    "        self.deit = timm.create_model('deit_small_patch16_224', pretrained=True)\n",
    "        self.deit.head = nn.Identity()  # Remove classification head\n",
    "        self.linear = nn.Linear(self.deit.num_features, embed_size)  # Project to embed_size\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.deit.forward_features(images)  # (B, seq_len, 192)\n",
    "        cls_token = features[:, 0, :]  # Take only [CLS] token: (B, 192)\n",
    "        out = self.linear(cls_token)  # Project to (B, embed_size)\n",
    "        return out.unsqueeze(1)  # (B, 1, embed_size)\n",
    "       \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads=8, num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = self.embedding(captions)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        memory = features.permute(1, 0, 2)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out)\n",
    "        return out.permute(1, 0, 2)\n",
    "\n",
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_labels, img_dir, vocab, transform=None, max_length=50):\n",
    "        self.img_labels = img_labels\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        caption = self.img_labels.iloc[idx, 1]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        indices, error = encode_khmer_sentence(caption, self.vocab)\n",
    "        if error:\n",
    "            print(f\"Error encoding caption: {error}\")\n",
    "            indices = [self.vocab['<UNK>']] * self.max_length\n",
    "        tokens = [self.vocab['<START>']] + indices + [self.vocab['<END>']]\n",
    "        tokens += [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n",
    "        return image, torch.tensor(tokens[:self.max_length])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "annotations_file = '/home/vitoupro/code/image_captioning/data/processed/word_segmented_imglabel_khmercut.txt'\n",
    "img_dir = '/home/vitoupro/code/image_captioning/data/raw/img'\n",
    "image_names, captions = [], []\n",
    "\n",
    "with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            image_name, caption = parts\n",
    "            image_names.append(image_name)\n",
    "            captions.append(caption)\n",
    "\n",
    "all_images = pd.DataFrame({'image': image_names, 'caption': captions})\n",
    "train_images, eval_images, train_captions, eval_captions = train_test_split(\n",
    "    all_images['image'].tolist(), all_images['caption'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ImageCaptionDataset(pd.DataFrame({'image': train_images, 'caption': train_captions}), img_dir, word2idx, transform, 20)\n",
    "eval_dataset = ImageCaptionDataset(pd.DataFrame({'image': eval_images, 'caption': eval_captions}), img_dir, word2idx, transform, 20)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embed_size = 256\n",
    "encoder = DeiTEncoder(embed_size=embed_size).to(device)\n",
    "decoder = TransformerDecoder(len(word2idx), embed_size, num_heads=8, num_layers=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "smoothing = SmoothingFunction().method1\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, device, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_wer, total_cer, total_bleu1, total_bleu2, total_bleu4, total_rougeL = 0, 0, 0, 0, 0, 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            predicted_captions = outputs.argmax(-1)\n",
    "\n",
    "            for i in range(len(captions)):\n",
    "                gt_caption = decode_indices(captions[i].tolist(), idx2word)[0]\n",
    "                pred_caption = decode_indices(predicted_captions[i].tolist(), idx2word)[0]\n",
    "\n",
    "                ref_match = re.search(r\"<START>(.*?)<END>\", gt_caption)\n",
    "                pred_match = re.search(r\"^(.*?)<END>\", pred_caption)\n",
    "                reference = ref_match.group(1).strip() if ref_match else \"\"\n",
    "                prediction = pred_match.group(1).strip() if pred_match else \"\"\n",
    "                if not reference or not prediction:\n",
    "                    continue\n",
    "\n",
    "                ref_tokens = reference.split()\n",
    "                pred_tokens = prediction.split()\n",
    "\n",
    "                total_wer += jiwer.wer(reference, prediction)\n",
    "                total_cer += jiwer.cer(reference, prediction)\n",
    "                total_bleu1 += sentence_bleu([ref_tokens], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu2 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "                total_bleu4 += sentence_bleu([ref_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "                total_rougeL += rouge.score(reference, prediction)['rougeL'].fmeasure\n",
    "                num_samples += 1\n",
    "\n",
    "    avg_wer = total_wer / num_samples if num_samples > 0 else 0\n",
    "    avg_cer = total_cer / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu1 = total_bleu1 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu2 = total_bleu2 / num_samples if num_samples > 0 else 0\n",
    "    avg_bleu4 = total_bleu4 / num_samples if num_samples > 0 else 0\n",
    "    avg_rougeL = total_rougeL / num_samples if num_samples > 0 else 0\n",
    "\n",
    "    print(f\"WER: {avg_wer:.2f}, CER: {avg_cer:.2f}, BLEU-1: {avg_bleu1:.2f}, BLEU-2: {avg_bleu2:.2f}, BLEU-4: {avg_bleu4:.2f}, ROUGE-L: {avg_rougeL:.2f}\")\n",
    "    return avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16968e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 4.7892\n",
      "WER: 0.91, CER: 0.73, BLEU-1: 0.16, BLEU-2: 0.07, BLEU-4: 0.03, ROUGE-L: 0.00\n",
      "Epoch 2: Train Loss: 4.2321\n",
      "WER: 0.87, CER: 0.73, BLEU-1: 0.19, BLEU-2: 0.10, BLEU-4: 0.05, ROUGE-L: 0.00\n",
      "Epoch 3: Train Loss: 4.0388\n",
      "WER: 0.85, CER: 0.71, BLEU-1: 0.24, BLEU-2: 0.11, BLEU-4: 0.05, ROUGE-L: 0.00\n",
      "Epoch 4: Train Loss: 3.8215\n",
      "WER: 0.78, CER: 0.67, BLEU-1: 0.27, BLEU-2: 0.14, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 5: Train Loss: 3.9683\n",
      "WER: 0.78, CER: 0.67, BLEU-1: 0.26, BLEU-2: 0.12, BLEU-4: 0.05, ROUGE-L: 0.00\n",
      "Epoch 6: Train Loss: 4.0067\n",
      "WER: 0.80, CER: 0.70, BLEU-1: 0.21, BLEU-2: 0.12, BLEU-4: 0.05, ROUGE-L: 0.00\n",
      "Epoch 7: Train Loss: 3.5999\n",
      "WER: 0.75, CER: 0.65, BLEU-1: 0.28, BLEU-2: 0.16, BLEU-4: 0.07, ROUGE-L: 0.00\n",
      "Epoch 8: Train Loss: 3.5465\n",
      "WER: 0.75, CER: 0.65, BLEU-1: 0.31, BLEU-2: 0.18, BLEU-4: 0.08, ROUGE-L: 0.00\n",
      "Epoch 9: Train Loss: 3.5066\n",
      "WER: 0.76, CER: 0.64, BLEU-1: 0.28, BLEU-2: 0.15, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 10: Train Loss: 4.0272\n",
      "WER: 0.76, CER: 0.65, BLEU-1: 0.28, BLEU-2: 0.15, BLEU-4: 0.07, ROUGE-L: 0.00\n",
      "Epoch 11: Train Loss: 3.9142\n",
      "WER: 0.79, CER: 0.68, BLEU-1: 0.27, BLEU-2: 0.15, BLEU-4: 0.07, ROUGE-L: 0.00\n",
      "Epoch 12: Train Loss: 3.8621\n",
      "WER: 0.86, CER: 0.71, BLEU-1: 0.26, BLEU-2: 0.15, BLEU-4: 0.06, ROUGE-L: 0.00\n",
      "Epoch 13: Train Loss: 3.2548\n",
      "WER: 0.79, CER: 0.68, BLEU-1: 0.30, BLEU-2: 0.18, BLEU-4: 0.09, ROUGE-L: 0.00\n",
      "Epoch 14: Train Loss: 3.4745\n",
      "WER: 0.80, CER: 0.68, BLEU-1: 0.32, BLEU-2: 0.19, BLEU-4: 0.09, ROUGE-L: 0.00\n",
      "Epoch 15: Train Loss: 4.1044\n",
      "WER: 0.82, CER: 0.69, BLEU-1: 0.29, BLEU-2: 0.16, BLEU-4: 0.06, ROUGE-L: 0.00\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "best_wer = float('inf')\n",
    "teacher_forcing_ratio = 0.9  # Start with 90% teacher forcing\n",
    "\n",
    "train_losses, wer_scores, cer_scores = [], [], []\n",
    "bleu1_scores, bleu2_scores, bleu4_scores, rougeL_scores = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        \n",
    "        input_tokens = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        if torch.rand(1).item() > teacher_forcing_ratio:\n",
    "            # Scheduled sampling: use own prediction as input\n",
    "            outputs = []\n",
    "            batch_size = images.size(0)\n",
    "            inputs = torch.full((batch_size, 1), word2idx['<START>'], dtype=torch.long, device=device)\n",
    "\n",
    "            for _ in range(input_tokens.size(1)):\n",
    "                output = decoder(features, inputs)\n",
    "                last_output = output[:, -1, :]  # last timestep\n",
    "                predicted = last_output.argmax(-1).unsqueeze(1)\n",
    "                inputs = torch.cat((inputs, predicted), dim=1)\n",
    "                outputs.append(last_output)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1).squeeze(2)  # (B, T, vocab)\n",
    "        else:\n",
    "            # Teacher forcing: normal\n",
    "            outputs = decoder(features, input_tokens)\n",
    "\n",
    "        batch_size, seq_len, vocab_size = outputs.size()\n",
    "        loss = criterion(\n",
    "            outputs.reshape(batch_size * seq_len, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_wer, avg_cer, avg_bleu1, avg_bleu2, avg_bleu4, avg_rougeL = evaluate_model(encoder, decoder, eval_loader, device, epoch)\n",
    "\n",
    "    wer_scores.append(avg_wer)\n",
    "    cer_scores.append(avg_cer)\n",
    "\n",
    "    # Reduce teacher forcing slowly\n",
    "    teacher_forcing_ratio = max(0.3, teacher_forcing_ratio * 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0632b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
